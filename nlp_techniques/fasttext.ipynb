{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "FILE = '../data/canada_data_select.csv'\n",
    "df_data = pd.read_csv(FILE)\n",
    "df_data.drop_duplicates(['ad_id', 'locations', 'phone_id', 'title', 'body'], inplace=True)\n",
    "df_data.reset_index(inplace=True)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "content = []\n",
    "for index, row in df_data.iterrows():\n",
    "    c = ''\n",
    "    # if type(row['title']) == type(' '):\n",
    "    # \tc = c + row['title']\n",
    "    if type(row['body']) == type(' '):\n",
    "        c = c + row['body']\n",
    "    c = re.sub(r'\\d+', '', c)\n",
    "    c = re.sub(r'[^\\x00-\\x7F]+','', c)\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    c = re.sub(cleanr, '', c)\n",
    "    c = re.sub(r\"[^a-zA-Z0-9.?]+\", ' ', c)\n",
    "    content.append(c.lower())\n",
    "    if index%10000 == 0:\n",
    "        print (index)\n",
    "\n",
    "df_data = pd.read_csv('../data/ht_unique_unique_ads_cosine.csv')\n",
    "for index, row in df_data.iterrows():\n",
    "    c = ''\n",
    "    # if type(row['title']) == type(' '):\n",
    "    # \tc = c + row['title']\n",
    "    if type(row['body']) == type(' '):\n",
    "        c = c + row['body']\n",
    "    c = re.sub(r'\\d+', '', c)\n",
    "    c = re.sub(r'[^\\x00-\\x7F]+','', c)\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    c = re.sub(cleanr, '', c)\n",
    "    c = re.sub(r\"[^a-zA-Z0-9.?]+\", ' ', c)\n",
    "    content.append(c.lower())\n",
    "    \n",
    "print (\"Writing to file....\")\n",
    "with open('../data/canada/only_text_train1.txt', 'a') as f:\n",
    "    f.writelines(content)\n",
    "\n",
    "print (len(content)) \n",
    "\n",
    "# def filter_words(word):\n",
    "#     word = str(word)\n",
    "#     if any(not x.isalpha() for x in word):\n",
    "#         return True\n",
    "\n",
    "#     elif len(word) < 3:\n",
    "#         return True\n",
    "\n",
    "#     return False\n",
    "\n",
    "# words_total = []\n",
    "# for index, sent in enumerate(content):\n",
    "#     sent_words = []\n",
    "#     try:\n",
    "#         sentences = nltk.sent_tokenize(sent)\n",
    "#     except Exception as e:\n",
    "#         print (e)\n",
    "#         continue\n",
    "    \n",
    "#     for ind, sen in enumerate(sentences):\n",
    "#         words = list(nltk.word_tokenize(sen))\n",
    "\n",
    "#         for w in words:\n",
    "#             w = re.sub('\\W+','', w)\n",
    "#             if filter_words(w):\n",
    "#                 continue\n",
    "#             sent_words.append(w.lower())\n",
    "\n",
    "#     if sent_words:\n",
    "#         with open('../data/canada_select_only_text1.txt', 'a') as f:\n",
    "#             f.write(' '.join(sent_words))\n",
    "#             f.write('\\n')\n",
    "\n",
    "# print (len(all_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.train_unsupervised('../data/canada/only_text_train1.txt', model='skipgram')\n",
    "word_vectors = fasttext_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def filter_words(word):\n",
    "    word = str(word)\n",
    "    if any(not x.isalpha() for x in word):\n",
    "        return True\n",
    "\n",
    "    elif len(word) < 3:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "df_data = pd.read_csv('../data/ht_unique_unique_ads_cosine.csv')\n",
    "content = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for index, row in df_data.iterrows():\n",
    "    c = ''\n",
    "    # if type(row['title']) == type(' '):\n",
    "    # \tc = c + row['title']\n",
    "    if type(row['body']) == type(' '):\n",
    "        c = c + row['body']\n",
    "    # c = row['title'] + ' ' + row['body']\n",
    "    c = re.sub(r'\\d+', '', c)\n",
    "    if type(row['Name']) == type(''):\n",
    "        name = row['Name'].split(';')\n",
    "        for n in name:\n",
    "            name_regex = re.compile(re.escape(n), re.IGNORECASE)\n",
    "            c = name_regex.sub('', c)\n",
    "        c = re.sub(r'[^\\x00-\\x7F]+',' ', c)\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        c = re.sub(cleanr, '', c)\n",
    "    content.append(c)\n",
    "df_data['content_p'] = content\n",
    "\n",
    "filtered_content = []\n",
    "all_words = []\n",
    "\n",
    "for index, sent in enumerate(content):\n",
    "    sent_words = []\n",
    "    \n",
    "    try:\n",
    "        sentences = nltk.sent_tokenize(sent)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        continue\n",
    "    \n",
    "    for ind, sen in enumerate(sentences):\n",
    "        words = list(nltk.word_tokenize(sen))\n",
    "\n",
    "        for w in words:\n",
    "            w = re.sub('\\W+','', w)\n",
    "            if filter_words(w):\n",
    "\n",
    "                continue\n",
    "            sent_words.append(w.lower())\n",
    "\n",
    "    if not sent_words:\n",
    "        df_data.drop(index, inplace=True)\n",
    "    else:\n",
    "        all_words.append(sent_words)\n",
    "        filtered_content.append(' '.join(sent_words))\n",
    "    \n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(1,1), norm='l2', \n",
    "     stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "\n",
    "bigram_matrix = vectorizer.fit_transform(filtered_content)\n",
    "features_col = vectorizer.get_feature_names()\n",
    "bigram_dense_matrix = np.asarray(bigram_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.zeros((len(all_words), 100), dtype=float)\n",
    "total_not_found = 0.0\n",
    "not_found_count = 0.0\n",
    "\n",
    "for ind, word_tokens in enumerate(all_words):\n",
    "    not_found_count = 0\n",
    "    for word in word_tokens:\n",
    "        weight = 0.0\n",
    "        try:\n",
    "            word_index = features_col.index(word)\n",
    "            weight = bigram_dense_matrix[ind][word_index]\n",
    "        except:\n",
    "#             print (word)\n",
    "            not_found_count += 1\n",
    "        if word in word_vectors and weight != 0.0:\n",
    "            vectors[ind] += (weight * 10 * word_vectors[word])\n",
    "\n",
    "    total_not_found += not_found_count\n",
    "    print (\"Not Found : {}\".format(not_found_count/len(word_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df_data['label'].values.tolist()\n",
    "\n",
    "binary_true_labels = [0] * len(true_labels)\n",
    "for ind, label in enumerate(true_labels):\n",
    "    if label >= 4:\n",
    "        binary_true_labels[ind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['binary_label'] = binary_true_labels\n",
    "# df_unique = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['green', 'red', 'blue']\n",
    "color_array = []\n",
    "for ind, l in enumerate(binary_true_labels):\n",
    "    color_array.append(colors[int(l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "X_embedded = umap.UMAP().fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################### HBDSCAN ############################\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, algorithm='best', alpha=1.0)\n",
    "clusterer.fit(vectors)\n",
    "#     print (clusterer.labels_)\n",
    "labels = clusterer.labels_\n",
    "labels_clustering = labels\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "unique_labels = set(labels)\n",
    "probs = clusterer.probabilities_\n",
    "#     colors = [plt.cm.Spectral(each)\n",
    "#               for each in np.linspace(0, 1, len(set(true_labels))]\n",
    "print (\"Number of labels : \" + str(len(list(set(clusterer.labels_)))))\n",
    "#     palette = sns.color_palette()\n",
    "#     cluster_colors = [sns.desaturate(palette[col], sat)\n",
    "#                       if col >= 0 else (0.5, 0.5, 0.5) for col, sat in\n",
    "#                       zip(clusterer.labels_, clusterer.probabilities_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "binary_true_labels = np.asarray(binary_true_labels)\n",
    "# noise_labels = np.where(labels==-1)\n",
    "no_noise_labels = np.where(labels!=-1)\n",
    "tr_labels = np.where(binary_true_labels==1)\n",
    "print (binary_true_labels.shape)\n",
    "print (no_noise_labels[0].shape)\n",
    "print (tr_labels)\n",
    "# lst3 = [value for value in tr_labels if value in noise_labels] \n",
    "common_tp = np.intersect1d(no_noise_labels[0], tr_labels[0])\n",
    "print (common_tp.shape)\n",
    "total_p = len(common_tp)/len((tr_labels[0]))\n",
    "print (total_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.scatter(X_embedded.T[0], X_embedded.T[1], c=color_array, alpha=0.2)\n",
    "# plt.title(\"Clustering accuracy={}, fmeasure_synth={}, number_of_labels={}\".format(clustering_acc, fmeasure, \n",
    "#                                                                                   len(unique_labels)))\n",
    "plt.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "right='off', left='off', labelleft='off')\n",
    "# plt.text(-5, 10, 'FastText', fontsize=24)\n",
    "plt.savefig('../results/embedding_fasttext.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subgraphs(mat_data, cl_ind):\n",
    "    filtered_sub = mat_data[cl_ind,:][:]\n",
    "    bigrams_count = np.count_nonzero(filtered_sub, axis=0)\n",
    "#     print (\"Bigrams count matrix shape : {}\".format(bigrams_count.shape))\n",
    "    zero_count_index = np.where(bigrams_count==0)[1]\n",
    "#     print (zero_count_index)\n",
    "    one_count_index = list(np.where(bigrams_count==1)[1])\n",
    "#     print (\"One count index : {}\".format(one_count_index))\n",
    "#     print (\"Max zero count index : {} \".format(max(zero_count_index)))\n",
    "    core_bigrams_index = list(np.where(bigrams_count>1)[1])\n",
    "#     print (\"Core Bigrams index : {}\".format(len(core_bigrams_index)))\n",
    "    outer_bigrams_index = list(one_count_index + core_bigrams_index)\n",
    "    \n",
    "    bigram_induced_graph = mat_data[:][:,core_bigrams_index]\n",
    "#     print (\"Bigram Induced Graph shape : {}\".format(bigram_induced_graph.shape))\n",
    "    ads_count = np.count_nonzero(bigram_induced_graph, axis=1)\n",
    "    ads_in_shell_index = np.where(ads_count>1)[0]\n",
    "    \n",
    "    not_core_ads = list(set(list(ads_in_shell_index)) - set(cl_ind))\n",
    "    not_core_bigrams = list(set(list(one_count_index)) - set(core_bigrams_index))\n",
    "    mat_copy = np.asarray(mat_data)\n",
    "    for i in not_core_ads:\n",
    "        for j in not_core_bigrams:\n",
    "            mat_copy[i][j] = 0\n",
    "    shell_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "    mat_copy[not_core_ads, :] = 0\n",
    "    outer_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "    mat_copy[:,one_count_index] = 0\n",
    "    core_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "#     print (\"Shell Subgraph shape : {}\". format(shell_subgraph.shape))\n",
    "#     print (\"Outer Subgraph shape : {}\". format(outer_subgraph.shape))\n",
    "#     print (\"Core Subgraph shape : {}\". format(core_subgraph.shape))\n",
    "#     print (zero_count_index.shape)\n",
    "#     print (zero_count_index[1])\n",
    "#     outer_subgraph = np.delete(filtered_sub, zero_count_index, axis=1)\n",
    "#     core_subgraph = outer_subgraph.copy()\n",
    "#     core_subgraph[:,one_count_index] = 0\n",
    "#     core_subgraph = np.delete(filtered_sub, list(set(list(zero_count_index) + list(one_count_index))), axis=1)\n",
    "    shell_subgraph = np.asarray(shell_subgraph)\n",
    "    outer_subgraph = np.asarray(outer_subgraph)\n",
    "    core_subgraph = np.asarray(core_subgraph)\n",
    "#     print (\"subgraph sizes: {}, {}, {}\".format(core_subgraph.shape, outer_subgraph.shape, shell_subgraph.shape))\n",
    "    \n",
    "    return shell_subgraph, outer_subgraph, core_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_matrix = bigram_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_unweighted_density(core_mat):\n",
    "    edge_weight = np.count_nonzero(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "#     print (ads_core_num)\n",
    "#     print (bigrams_core_num)\n",
    "    return edge_weight/(ads_core_num * bigrams_core_num + 1)\n",
    "\n",
    "def calculate_weighted_density(core_mat):\n",
    "    edge_weight = np.sum(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "\n",
    "    return edge_weight/(ads_core_num * bigrams_core_num + 1)\n",
    "\n",
    "def calculate_unweighted_fraudar_score(core_mat):\n",
    "    edge_weight = np.count_nonzero(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return edge_weight/(ads_core_num + bigrams_core_num + 1)\n",
    "\n",
    "def calculate_weighted_fraudar_score(core_mat):\n",
    "    edge_weight = np.sum(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return edge_weight/(ads_core_num + bigrams_core_num + 1)\n",
    "\n",
    "def calculate_unweighted_edge_per_score(core_mat, outer_mat):\n",
    "    core_edges = np.count_nonzero(core_mat)\n",
    "    outer_edges = np.count_nonzero(outer_mat)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return ((core_edges + 1)/(outer_edges +1))*(math.log(bigrams_core_num+1))\n",
    "\n",
    "def calculate_weighted_edge_per_score(core_mat, outer_mat):\n",
    "    core_edges = np.sum(core_mat)\n",
    "    outer_edges = np.sum(outer_mat)\n",
    "    \n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return ((core_edges + 1)/(outer_edges +1))*(math.log(bigrams_core_num+1))\n",
    "\n",
    "\n",
    "def calculate_custom_score(core_mat, outer_mat):\n",
    "    bigram_degrees = np.count_nonzero(np.asarray(core_mat), axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    outer_edges = np.count_nonzero(outer_mat)\n",
    "    bigram_degrees = bigram_degrees/bigrams_core_num\n",
    "    \n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    \n",
    "    return (np.sum(bigram_degrees)/(outer_edges+1))*(math.log(bigrams_core_num+1))*(math.log(ads_core_num+1))\n",
    "# #     print (mat.shape)\n",
    "#     edges_nonzero = np.count_nonzero(mat, axis=0)\n",
    "#     unique, counts = np.unique(edges_nonzero, return_counts=True)\n",
    "#     degree_counts = dict(zip(unique, counts))\n",
    "#     numerator = 0.0\n",
    "#     denominator = 0.0\n",
    "#     half = max(mat.shape[0]/2, 2)\n",
    "#     for k, v in degree_counts.items():\n",
    "#         if k == 0:\n",
    "#             continue\n",
    "#         elif k <= half:\n",
    "#             denominator += k*v\n",
    "#         else:\n",
    "#             denominator += k*v\n",
    "#             numerator += k*v\n",
    "#     if denominator == 0.0:\n",
    "#         return 0.0\n",
    "#     else:\n",
    "#         return numerator/denominator\n",
    "\n",
    "# def calculate_weighted_edge_per_score(mat):\n",
    "#     return 0.0\n",
    "\n",
    "#Should be shell_mat instead of outer_mat, change once you figure out how to get shell subgraph.\n",
    "def calculate_unweighted_modularity_score(core_mat, outer_mat, total_edges):\n",
    "#     total_edges = math.log(total_edges)\n",
    "    ad_degrees = np.count_nonzero(np.asarray(outer_mat), axis=1)\n",
    "#     print (ad_degrees)\n",
    "    bigram_degrees = np.count_nonzero(np.asarray(outer_mat), axis=0)\n",
    "#     print (bigram_degrees)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_index = np.where(ads_count>0)[0]\n",
    "    bigrams_core_index = np.where(bigrams_count>0)[0]\n",
    "#     ads_list_index = [x for x in range(core_mat.shape[0])]\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "    summation = 0.0\n",
    "    for i in ads_core_index:\n",
    "        for j in bigrams_core_index:\n",
    "            if core_mat[i][j] == 0:\n",
    "                adj = 0\n",
    "            else:\n",
    "                adj = 1\n",
    "            if adj == 1:\n",
    "                summation += (adj - (ad_degrees[i] * bigram_degrees[j])/total_edges)\n",
    "\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "#     summation = 0.0\n",
    "#     for k in range(nonzero_edges.shape[0]):\n",
    "#         ad_index = nonzero_edges[k][0]\n",
    "#         big_index = nonzero_edges[k][1]\n",
    "# #         print (ad_index)\n",
    "# #         print (big_index)\n",
    "# #         if core_mat[ad_index][big_index] != 0:\n",
    "# #             adj = 1\n",
    "# #         else:\n",
    "# #             adj = 0\n",
    "        \n",
    "#         summation += (1 - (ad_degrees[ad_index] * bigram_degrees[big_index])/(2*total_edges))\n",
    "    \n",
    "    return (summation/total_edges)*(math.log(len(ads_core_index)+1))\n",
    "\n",
    "def calculate_weighted_modularity_score(core_mat, outer_mat, total_edges):\n",
    "    ad_degrees = np.sum(np.asarray(outer_mat), axis=1)\n",
    "    bigram_degrees = np.sum(np.asarray(outer_mat), axis=0)\n",
    "    \n",
    "    ads_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_index = np.where(ads_count>0)[0]\n",
    "    bigrams_core_index = np.where(bigrams_count>0)[0]\n",
    "\n",
    "    \n",
    "\n",
    "#     ads_list_index = [x for x in range(core_mat.shape[0])]\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "    summation = 0.0\n",
    "    for i in ads_core_index:\n",
    "        for j in bigrams_core_index:\n",
    "            if core_mat[i][j] != 0:\n",
    "                summation += (core_mat[i][j] - (ad_degrees[i] * bigram_degrees[j])/total_edges)\n",
    "#     summation = 0.0\n",
    "#     for k in range(nonzero_edges.shape[0]):\n",
    "#         ad_index = nonzero_edges[k][0]\n",
    "#         big_index = nonzero_edges[k][1]\n",
    "#         summation += (outer_mat[ad_index][big_index] - (ad_degrees[ad_index] * bigram_degrees[big_index])/(2*total_edges))\n",
    "    \n",
    "    return (summation/total_edges)*(math.log(len(ads_core_index)+1))\n",
    "\n",
    "def calculate_pairwise_modularity(mat):\n",
    "    mat = np.asarray(mat.todense())\n",
    "    sim_scores = np.zeros((mat.shape[0], mat.shape[0]))\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(i+1, mat.shape[0]):\n",
    "#             print (\"i : {}, j : {}\".format(i,j))\n",
    "            if i == j:\n",
    "                continue\n",
    "#             print (len(mat[i]))\n",
    "            sim_scores[i][j] = calculate_modularity_score(np.vstack((mat[i], mat[j])))\n",
    "    \n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_all_metrics(bigram_matrix, unique_labels, labels, df_data):\n",
    "    eigen_ratios = []\n",
    "    weighted_cluster_density = []\n",
    "    unweighted_cluster_density = []\n",
    "    unweighted_fraudar_scores = []\n",
    "    weighted_fraudar_scores = []\n",
    "    unweighted_outer_edge_perc_scores = []\n",
    "    weighted_outer_edge_perc_scores = []\n",
    "    unweighted_shell_edge_perc_scores = []\n",
    "    weighted_shell_edge_perc_scores = []\n",
    "    weighted_outer_modularity_scores = []\n",
    "    unweighted_outer_modularity_scores = []\n",
    "    weighted_shell_modularity_scores = []\n",
    "    unweighted_shell_modularity_scores = []\n",
    "    pairwise_similarity = []\n",
    "    custom_score = []\n",
    "    avg_label_scores = []\n",
    "    max_label_scores = []\n",
    "    sum_label_scores = []\n",
    "    avg_binary_scores = []\n",
    "    max_binary_scores = []\n",
    "    sum_binary_scores = []\n",
    "    clusters = []\n",
    "    cluster_counts = []\n",
    "    \n",
    "    total_edges_unweighted = np.count_nonzero(bigram_matrix)\n",
    "    total_edges_weighted = np.sum(bigram_matrix)\n",
    "    for l in unique_labels:\n",
    "#         s = bigram_matrix.sum(axis=1)\n",
    "        if l== -1:\n",
    "            weighted_cluster_density.append(0)\n",
    "            unweighted_cluster_density.append(0)\n",
    "            weighted_fraudar_scores.append(0)\n",
    "            unweighted_fraudar_scores.append(0)\n",
    "            weighted_outer_edge_perc_scores.append(0)\n",
    "            unweighted_outer_edge_perc_scores.append(0)\n",
    "            weighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_outer_modularity_scores.append(0)\n",
    "            weighted_outer_modularity_scores.append(0)\n",
    "            unweighted_shell_modularity_scores.append(0)\n",
    "            weighted_shell_modularity_scores.append(0)\n",
    "            pairwise_similarity.append(0)\n",
    "            custom_score.append(0)\n",
    "            cluster_counts.append(math.log(len(cluster_idx), 10)) \n",
    "            eigen_ratios.append(0)\n",
    "            clusters.append(l)\n",
    "            \n",
    "            max_label_scores.append(0)\n",
    "            avg_label_scores.append(0)\n",
    "            sum_label_scores.append(0)\n",
    "\n",
    "            max_binary_scores.append(0)\n",
    "            avg_binary_scores.append(0)\n",
    "            sum_binary_scores.append(0)\n",
    "            continue\n",
    "#         print (s.shape)\n",
    "#         print (\"bigram matrix sum : {}\".format(bigram_matrix.sum()))\n",
    "#         print (\"Zero elems: {}\".format(len(np.argwhere(s==0))))\n",
    "        cluster_idx = np.argwhere(labels == l).reshape(-1)\n",
    "#         print (l, len(cluster_idx))\n",
    "        \n",
    "        print (cluster_idx)\n",
    "        shell_subgraph, outer_subgraph, core_subgraph = get_all_subgraphs(bigram_matrix.copy(), cluster_idx)\n",
    "#         print (l, len(cluster_idx), core_subgraph.sum(), outer_subgraph.sum(), shell_subgraph.sum())\n",
    "        \n",
    "\n",
    "        df_filt = df_data[df_data['cluster_label']== l]\n",
    "        if len(df_filt) == 0 or core_subgraph.shape[0] == 0 or shell_subgraph.shape[0] == 0 or outer_subgraph.shape[0] == 0:\n",
    "            weighted_cluster_density.append(0)\n",
    "            unweighted_cluster_density.append(0)\n",
    "            weighted_fraudar_scores.append(0)\n",
    "            unweighted_fraudar_scores.append(0)\n",
    "            weighted_outer_edge_perc_scores.append(0)\n",
    "            unweighted_outer_edge_perc_scores.append(0)\n",
    "            weighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_outer_modularity_scores.append(0)\n",
    "            weighted_outer_modularity_scores.append(0)\n",
    "            unweighted_shell_modularity_scores.append(0)\n",
    "            weighted_shell_modularity_scores.append(0)\n",
    "            pairwise_similarity.append(0)\n",
    "            custom_score.append(0)\n",
    "            cluster_counts.append(math.log(len(cluster_idx), 10)) \n",
    "            eigen_ratios.append(0)\n",
    "            clusters.append(l)\n",
    "            \n",
    "            max_label_scores.append(0)\n",
    "            avg_label_scores.append(0)\n",
    "            sum_label_scores.append(0)\n",
    "\n",
    "            max_binary_scores.append(0)\n",
    "            avg_binary_scores.append(0)\n",
    "            sum_binary_scores.append(0)\n",
    "            continue\n",
    "#         elif core_subgraph.shape[0] == 0 or shell_subgraph.shape[0] == 0 or outer_subgraph.shape[0] == 0:\n",
    "#             continue\n",
    "        print (l, df_filt.shape)\n",
    "        max_label_scores.append(max(df_filt['label']))\n",
    "        avg_label_scores.append(sum(df_filt['label'])/len(df_filt['label']))\n",
    "        sum_label_scores.append(sum(df_filt['label']))\n",
    "\n",
    "        max_binary_scores.append(max(df_filt['binary_label']))\n",
    "        avg_binary_scores.append(sum(df_filt['binary_label'])/len(df_filt['binary_label']))\n",
    "        sum_binary_scores.append(sum(df_filt['binary_label']))\n",
    "        \n",
    "        local_content = list(df_filt['content_p'])\n",
    "        count_vectorizer = TfidfVectorizer(ngram_range=(2,2), use_idf=False)\n",
    "        count_data = count_vectorizer.fit_transform(local_content)\n",
    "\n",
    "        svd = TruncatedSVD(n_components=2)\n",
    "        local_vecs = svd.fit_transform(count_data)\n",
    "        w = svd.singular_values_\n",
    "        if len(w) > 1:\n",
    "            eigen_rat = w[1]/w[0]\n",
    "        else:\n",
    "            eigen_rat = 1\n",
    "        eigen_ratios.append(eigen_rat)\n",
    "        \n",
    "        print (outer_subgraph.shape)\n",
    "        print (core_subgraph.shape)\n",
    "        pairwise_sim_mat = cosine_similarity(outer_subgraph, dense_output=True)\n",
    "        pairwise_sim_mat = np.tril(pairwise_sim_mat, -1)\n",
    "#         print (sum(pairwise_sim_mat).shape)\n",
    "        print (pairwise_sim_mat.sum())\n",
    "        an_score = calculate_weighted_edge_per_score(core_subgraph, outer_subgraph)\n",
    "        names = [x.lower() if type(x) == type('') else None for x in df_filt['Name'].unique()]\n",
    "        if math.nan in names:\n",
    "            names.remove(math.nan)\n",
    "        if None in names:\n",
    "            names.remove(None)\n",
    "        names = list(set(names))\n",
    "        c_score = an_score * max(0, len(names)-1)\n",
    "        print (\"Scores : {}, {}, {}\".format(an_score, len(names), c_score))\n",
    "        weighted_cluster_density.append(calculate_weighted_density(core_subgraph))\n",
    "        unweighted_cluster_density.append(calculate_unweighted_density(core_subgraph))\n",
    "        weighted_fraudar_scores.append(calculate_weighted_fraudar_score(core_subgraph))\n",
    "        unweighted_fraudar_scores.append(calculate_unweighted_fraudar_score(core_subgraph))\n",
    "        weighted_outer_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "        unweighted_outer_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "        weighted_shell_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "        unweighted_shell_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "        unweighted_outer_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, outer_subgraph, total_edges_unweighted))\n",
    "        weighted_outer_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, outer_subgraph, total_edges_weighted))\n",
    "        unweighted_shell_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, shell_subgraph, total_edges_unweighted))\n",
    "        weighted_shell_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, shell_subgraph, total_edges_weighted))\n",
    "        custom_score.append(c_score)\n",
    "        pairwise_similarity.append(pairwise_sim_mat.sum()/len(cluster_idx))\n",
    "        cluster_counts.append(math.log(len(cluster_idx), 10)) \n",
    "        clusters.append(l)\n",
    "        \n",
    "        count_data = []\n",
    "        local_content = []\n",
    "        shell_subgraph = []\n",
    "        core_subgraph = []\n",
    "        outer_subgraph = []\n",
    "        if l % 50 == 0:\n",
    "            print (l)\n",
    "            gc.collect()\n",
    "#     original_labels = labels.copy()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['weighted_cluster_density'] = weighted_cluster_density\n",
    "    metrics['unweighted_cluster_density'] = unweighted_cluster_density\n",
    "    metrics['weighted_fraudar_scores'] = weighted_fraudar_scores\n",
    "    metrics['unweighted_fraudar_scores'] = unweighted_fraudar_scores\n",
    "    metrics['weighted_outer_edge_perc_scores'] = weighted_outer_edge_perc_scores\n",
    "    metrics['unweighted_outer_edge_perc_scores'] = unweighted_outer_edge_perc_scores\n",
    "    metrics['weighted_shell_edge_perc_scores'] = weighted_shell_edge_perc_scores\n",
    "    metrics['unweighted_shell_edge_perc_scores'] = unweighted_shell_edge_perc_scores\n",
    "    metrics['unweighted_outer_modularity_scores'] = unweighted_outer_modularity_scores\n",
    "    metrics['weighted_outer_modularity_scores'] = weighted_outer_modularity_scores\n",
    "    metrics['unweighted_shell_modularity_scores'] = unweighted_shell_modularity_scores\n",
    "    metrics['weighted_shell_modularity_scores'] = weighted_shell_modularity_scores\n",
    "    metrics['pairwise_similarity'] = pairwise_similarity\n",
    "    metrics['custom_score'] = custom_score\n",
    "    metrics['avg_label_scores'] = avg_label_scores\n",
    "    metrics['sum_label_scores'] = sum_label_scores\n",
    "    metrics['max_label_scores'] = max_label_scores\n",
    "    metrics['avg_binary_scores'] = avg_binary_scores\n",
    "    metrics['max_binary_scores'] = max_binary_scores\n",
    "    metrics['sum_binary_scores'] = sum_binary_scores\n",
    "    metrics['eigen_ratios'] = eigen_ratios\n",
    "    metrics['clusters'] = clusters\n",
    "    metrics['labels'] = labels.copy()\n",
    "    metrics['cluster_counts'] = cluster_counts\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_merging_metrics = get_all_metrics(bigram_matrix, unique_labels, labels, df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores = pre_merging_metrics['custom_score']\n",
    "text_similarity = pre_merging_metrics['pairwise_similarity']\n",
    "clusters = pre_merging_metrics['clusters']\n",
    "avg_label_scores = pre_merging_metrics['avg_label_scores']\n",
    "avg_binary_scores = pre_merging_metrics['avg_binary_scores']\n",
    "w_density = pre_merging_metrics['weighted_cluster_density']\n",
    "uw_density = pre_merging_metrics['unweighted_cluster_density']\n",
    "w_fraudar = pre_merging_metrics['weighted_fraudar_scores']\n",
    "uw_fraudar = pre_merging_metrics['unweighted_fraudar_scores']\n",
    "w_outer_edge = pre_merging_metrics['weighted_outer_edge_perc_scores']\n",
    "uw_outer_edge = pre_merging_metrics['unweighted_outer_edge_perc_scores']\n",
    "w_shell_edge = pre_merging_metrics['weighted_shell_edge_perc_scores']\n",
    "uw_shell_edge = pre_merging_metrics['unweighted_shell_edge_perc_scores']\n",
    "w_outer_mod = pre_merging_metrics['weighted_outer_modularity_scores']\n",
    "uw_outer_mod = pre_merging_metrics['unweighted_outer_modularity_scores']\n",
    "w_shell_mod = pre_merging_metrics['weighted_shell_modularity_scores']\n",
    "uw_shell_mod = pre_merging_metrics['unweighted_shell_modularity_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=avg_label_scores, y=suspicious_scores, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='anomaly score')\n",
    "plt.show()\n",
    "\n",
    "ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "ax = sns.regplot(x=text_similarity, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Text Similarity', ylabel='Weighted outer edge percentage')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=avg_label_scores, y=text_similarity, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Text Similarity')\n",
    "plt.show()\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.scatter(avg_label_scores, w_outer_edge, alpha=0.2)\n",
    "# plt.title('Avg label score vs weighted outer edge percentage')\n",
    "# plt.xlabel('avg label score')\n",
    "# plt.ylabel('weighted outer edge percentage')\n",
    "# # w = np.linalg.lstsq(avg_label_scores, w_outer_edge)[0]\n",
    "# # yh = np.dot(avg_label_scores,w)\n",
    "# # plt.plot(avg_label_scores, yh, 'r-')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(avg_label_scores, w_shell_edge, alpha=0.2)\n",
    "# plt.title('Avg label score vs weighted shell edge percentage')\n",
    "# plt.xlabel('avg label score')\n",
    "# plt.ylabel('weighted shell edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(text_similarity, w_outer_edge, alpha=0.2)\n",
    "# plt.title('Avg text similarity vs weighted outer edge percentage')\n",
    "# plt.xlabel('avg text similarity')\n",
    "# plt.ylabel('weighted outer edge percentage')\n",
    "\n",
    "# # plt.scatter(avg_label_scores, w_shell_edge, alpha=0.2)\n",
    "# # plt.title('Avg label score vs weighted shell edge percentage')\n",
    "# # plt.xlabel('avg label score')\n",
    "# # plt.ylabel('weighted shell edge percentage')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_name(clusters, df_data):\n",
    "    filtered_clusters = []\n",
    "    for cl in clusters:\n",
    "        df_fil = df_data[df_data['cluster_label']==cl]\n",
    "        names = [x.lower() if type(x) == type('') else None for x in df_fil['Name'].unique()]\n",
    "        if math.nan in names:\n",
    "            names.remove(math.nan)\n",
    "        if None in names:\n",
    "            names.remove(None)\n",
    "        names = list(set(names))\n",
    "        if len(names)>1:\n",
    "#             print (names)\n",
    "            filtered_clusters.append(cl)\n",
    "#             filtered_cluster_metric.append(anomaly_cluster_metric[ind])\n",
    "#         else:\n",
    "#             filtered_clusters.append(c)\n",
    "#             filtered_cluster_metric.append(anomaly_cluster_metric[ind])\n",
    "    return filtered_clusters\n",
    "\n",
    "def filter_by_threshold(clusters, metric, threshold=0.0):\n",
    "    filtered_clusters = []\n",
    "    index_list = [i for i, e in enumerate(metric) if e > threshold]\n",
    "    filtered_clusters = [clusters[i] for i in index_list] \n",
    "    \n",
    "    return filtered_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_clusters = filter_by_name(clusters, df_data.copy())\n",
    "print (filtered_clusters)\n",
    "filtered_clusters = set(filtered_clusters)\n",
    "index_list = [i for i, e in enumerate(clusters) if e in filtered_clusters]\n",
    "filtered_avg_label_scores = [avg_label_scores[i] for i in index_list] \n",
    "filtered_text_similarity = [text_similarity[i] for i in index_list] \n",
    "filtered_suspicious_scores = [suspicious_scores[i] for i in index_list] \n",
    "filtered_avg_binary_scores = [avg_binary_scores[i] for i in index_list] \n",
    "filtered_w_density = [w_density[i] for i in index_list] \n",
    "filtered_uw_density = [uw_density[i] for i in index_list] \n",
    "filtered_w_fraudar = [w_fraudar[i] for i in index_list] \n",
    "filtered_uw_fraudar = [uw_fraudar[i] for i in index_list] \n",
    "filtered_w_outer_edge = [w_outer_edge[i] for i in index_list] \n",
    "filtered_uw_outer_edge = [uw_outer_edge[i] for i in index_list] \n",
    "filtered_w_shell_edge = [w_shell_edge[i] for i in index_list] \n",
    "filtered_uw_shell_edge = [uw_shell_edge[i] for i in index_list] \n",
    "filtered_w_outer_mod = [w_outer_mod[i] for i in index_list] \n",
    "filtered_uw_outer_mod = [uw_outer_mod[i] for i in index_list] \n",
    "filtered_w_shell_mod = [w_shell_mod[i] for i in index_list] \n",
    "filtered_uw_shell_mod = [uw_shell_mod[i] for i in index_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import plotly.express as px\n",
    "\n",
    "filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \\\n",
    "filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, filtered_w_outer_edge, \\\n",
    "filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  filtered_w_outer_mod, \\\n",
    "filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod = \\\n",
    "zip(*sorted(zip(filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \n",
    "                filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, \n",
    "                filtered_w_outer_edge, filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  \n",
    "                filtered_w_outer_mod, filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod)))\n",
    "filtered_suspicious_scores = list(reversed(filtered_suspicious_scores))\n",
    "filtered_clusters = list(reversed(filtered_clusters))\n",
    "filtered_avg_label_scores = list(reversed(filtered_avg_label_scores))\n",
    "filtered_text_similarity = list(reversed(filtered_text_similarity))\n",
    "filtered_avg_binary_scores = list(reversed(filtered_avg_binary_scores))\n",
    "filtered_w_density = list(reversed(filtered_w_density))\n",
    "filtered_uw_density = list(reversed(filtered_uw_density))\n",
    "filtered_w_fraudar = list(reversed(filtered_w_fraudar))  \n",
    "filtered_uw_fraudar = list(reversed(filtered_uw_fraudar))\n",
    "filtered_w_outer_edge = list(reversed(filtered_w_outer_edge)) \n",
    "filtered_uw_outer_edge = list(reversed(filtered_w_density)) \n",
    "filtered_w_shell_edge = list(reversed(filtered_w_shell_edge))\n",
    "filtered_uw_shell_edge = list(reversed(filtered_uw_shell_edge))  \n",
    "filtered_w_outer_mod = list(reversed(filtered_w_outer_mod)) \n",
    "filtered_uw_outer_mod = list(reversed(filtered_uw_outer_mod)) \n",
    "filtered_w_shell_mod = list(reversed(filtered_w_shell_mod)) \n",
    "filtered_uw_shell_mod = list(reversed(filtered_uw_shell_mod))\n",
    "top_k = [10, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "rcParams['figure.figsize'] = 4,4\n",
    "for k in top_k:\n",
    "    print ('==================================k = {}========================================'.format(k) )\n",
    "    plt.hist(filtered_avg_label_scores[:k], bins=7)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(filtered_avg_binary_scores[:k], bins=3)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(filtered_text_similarity[:k], marker='o', linestyle='--')\n",
    "    plt.xlabel(' Clusters')\n",
    "    plt.ylabel('avg text similarity')\n",
    "    plt.show()\n",
    "    \n",
    "    print (\"Average of average label scores : {}\".format(sum(filtered_avg_label_scores[:k])/k))\n",
    "    print (\"Average text similarity : {}\".format(sum(filtered_text_similarity[:k])/k))\n",
    "    print ('\\n')\n",
    "    eig_ts_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_text_similarity[:k])[0]\n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print (\"Spearman Correlation | Average Label Score |          Text Similarity          | {0:.2f} \".format(eig_ts_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "    \n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print ('\\n')\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "print (len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10,10\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_suspicious_scores, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='anomaly score')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_w_outer_edge, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "ax = sns.regplot(x=filtered_text_similarity, y=filtered_w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Text Similarity', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_text_similarity, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Text Similarity')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores = pre_merging_metrics['weighted_outer_edge_perc_scores']\n",
    "text_similarity = pre_merging_metrics['pairwise_similarity']\n",
    "clusters = pre_merging_metrics['clusters']\n",
    "avg_label_scores = pre_merging_metrics['avg_label_scores']\n",
    "avg_binary_scores = pre_merging_metrics['avg_binary_scores']\n",
    "w_density = pre_merging_metrics['weighted_cluster_density']\n",
    "uw_density = pre_merging_metrics['unweighted_cluster_density']\n",
    "w_fraudar = pre_merging_metrics['weighted_fraudar_scores']\n",
    "uw_fraudar = pre_merging_metrics['unweighted_fraudar_scores']\n",
    "w_outer_edge = pre_merging_metrics['weighted_outer_edge_perc_scores']\n",
    "uw_outer_edge = pre_merging_metrics['unweighted_outer_edge_perc_scores']\n",
    "w_shell_edge = pre_merging_metrics['weighted_shell_edge_perc_scores']\n",
    "uw_shell_edge = pre_merging_metrics['unweighted_shell_edge_perc_scores']\n",
    "w_outer_mod = pre_merging_metrics['weighted_outer_modularity_scores']\n",
    "uw_outer_mod = pre_merging_metrics['unweighted_outer_modularity_scores']\n",
    "w_shell_mod = pre_merging_metrics['weighted_shell_modularity_scores']\n",
    "uw_shell_mod = pre_merging_metrics['unweighted_shell_modularity_scores']\n",
    "cluster_counts = pre_merging_metrics['cluster_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores, clusters, avg_label_scores, text_similarity, \\\n",
    "w_density, uw_density, w_fraudar,  uw_fraudar, w_outer_edge, \\\n",
    "uw_outer_edge, w_shell_edge, uw_shell_edge,  w_outer_mod, \\\n",
    "uw_outer_mod, w_shell_mod, uw_shell_mod, cluster_counts = \\\n",
    "zip(*sorted(zip(suspicious_scores, clusters, avg_label_scores, text_similarity, \n",
    "                w_density, uw_density, w_fraudar,  uw_fraudar, w_outer_edge, \n",
    "                uw_outer_edge, w_shell_edge, uw_shell_edge,  w_outer_mod, \n",
    "                uw_outer_mod, w_shell_mod, uw_shell_mod, cluster_counts)))\n",
    "suspicious_scores = list(reversed(suspicious_scores))\n",
    "clusters = list(reversed(clusters))\n",
    "avg_label_scores = list(reversed(avg_label_scores))\n",
    "text_similarity = list(reversed(text_similarity))\n",
    "# filtered_avg_binary_scores = list(reversed(filtered_avg_binary_scores))\n",
    "w_density = list(reversed(w_density))\n",
    "uw_density = list(reversed(uw_density))\n",
    "w_fraudar = list(reversed(w_fraudar))  \n",
    "uw_fraudar = list(reversed(uw_fraudar))\n",
    "w_outer_edge = list(reversed(w_outer_edge)) \n",
    "uw_outer_edge = list(reversed(uw_outer_edge)) \n",
    "w_shell_edge = list(reversed(w_shell_edge))\n",
    "uw_shell_edge = list(reversed(uw_shell_edge))  \n",
    "w_outer_mod = list(reversed(w_outer_mod)) \n",
    "uw_outer_mod = list(reversed(uw_outer_mod)) \n",
    "w_shell_mod = list(reversed(w_shell_mod)) \n",
    "uw_shell_mod = list(reversed(uw_shell_mod))\n",
    "cluster_counts = list(reversed(cluster_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = list(df_data['content_p'])\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2, 3), norm='l2', \n",
    "    smooth_idf=True, stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "# where_are_nans = np.isnan(content)\n",
    "# content[where_are_nans] = 0\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "\n",
    "print (bigram_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['cluster_label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = range(len(df_data))\n",
    "df_data = df_data.reindex(new_index)\n",
    "# print (df_da.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "sort_ind = []\n",
    "for ind, cl in enumerate(clusters[:k]):\n",
    "    if cl==-1 or cl==-2:\n",
    "        continue\n",
    "    else:\n",
    "        df_f = df_data[df_data['cluster_label'] == cl]\n",
    "        print (df_f.shape)\n",
    "#         if df_f.shape[0] < 20:\n",
    "#             continue\n",
    "        sort_ind += list(df_f.index)\n",
    "print (sort_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_matrix = cosine_similarity(bigram_matrix)\n",
    "print (sim_matrix.shape)\n",
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.tick_params(\n",
    "#     axis='both',          # changes apply to the x-axis\n",
    "#     which='both',      # both major and minor ticks are affected\n",
    "#     bottom=False,      # ticks along the bottom edge are off\n",
    "#     top=False,         # ticks along the top edge are off\n",
    "#     labelbottom=False,\n",
    "# right='off', left='off', labelleft='off')\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "ax = sns.heatmap(sim_matrix[sort_ind, :][:,sort_ind], cmap=cmap, xticklabels=False, yticklabels=False, cbar=False)\n",
    "# ax.set(xlabel='cosine similarity', ylabel='cosine_similarity')\n",
    "plt.savefig('../results/sim_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_index = clusters.index(-1)\n",
    "del suspicious_scores[noise_index]\n",
    "del text_similarity[noise_index]\n",
    "del clusters[noise_index]\n",
    "del avg_label_scores[noise_index]\n",
    "del avg_binary_scores[noise_index]\n",
    "del w_density[noise_index]\n",
    "del uw_density[noise_index]\n",
    "del w_fraudar[noise_index]\n",
    "del uw_fraudar[noise_index]\n",
    "del w_outer_edge[noise_index]\n",
    "del uw_outer_edge[noise_index]\n",
    "del w_shell_edge[noise_index]\n",
    "del uw_shell_edge[noise_index]\n",
    "del w_outer_mod[noise_index]\n",
    "del uw_outer_mod[noise_index]\n",
    "del w_shell_mod[noise_index]\n",
    "del uw_shell_mod[noise_index]\n",
    "del cluster_counts[noise_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = [10**x for x in cluster_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ads_in_clusters = sum(cluster_counts)\n",
    "avg_of_avg_label_scores = sum(avg_label_scores[:k])/k\n",
    "avg_of_anomaly_scores = sum(suspicious_scores[:k])/k\n",
    "avg_text_similarity = sum(text_similarity[:k])/k\n",
    "avg_cluster_size = sum(cluster_counts[:k])/k\n",
    "\n",
    "print (\"Total ads in clusters: {}, Average of label scores per cluster: {}, \\\n",
    "average of anomaly scores per cluster: {}, Average text sim: {}, avg cluster size: {}\".format(\n",
    "total_ads_in_clusters, avg_of_avg_label_scores, avg_of_anomaly_scores, avg_text_similarity, avg_cluster_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "b =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
