{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_orig - 10000 ads\n",
    "#df_uniq - around 9000 ads\n",
    "#df_non_noisy - around 5500 ads\n",
    "#Append the rows that need to be deleted at the end of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "FILE = '../data/ht_unique_jun.csv'\n",
    "OUTPUT_FILE = '../data/svd_results/results_temp8.csv'\n",
    "OUTPUT_PROP_FILE = '../data/svd_results/cluster_properties_temp8.json'\n",
    "\n",
    "df_orig = pd.read_csv(FILE)\n",
    "\n",
    "content = []\n",
    "for index, row in df_orig.iterrows():\n",
    "    c = ''\n",
    "\n",
    "    if type(row['body']) == type(' '):\n",
    "        c = c + row['body']\n",
    "    # c = row['title'] + ' ' + row['body']\n",
    "    c = re.sub(r'\\d+', '', c)\n",
    "#     if type(row['Name']) == type(''):\n",
    "#         name = row['Name'].split(';')\n",
    "#         for n in name:\n",
    "#             name_regex = re.compile(re.escape(n), re.IGNORECASE)\n",
    "#             c = name_regex.sub('', c)\n",
    "    c = re.sub(r'[^\\x00-\\x7F]+',' ', c)\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    c = re.sub(cleanr, '', c)\n",
    "#     df_data.at[index, 'body'] = c\n",
    "    content.append(c)\n",
    "df_orig['content_p'] = content\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2,3), norm='l2', \n",
    "    smooth_idf=True, stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "print(bigram_matrix.shape)\n",
    "# print (bigram_matrix[0])\n",
    "\n",
    "# svd = TruncatedSVD(n_components=3)\n",
    "# svd.fit_transform(bigram_matrix)\n",
    "# np.save('../data/modalities_data/tf_idf_bigrams.npy', bigram_matrix)\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.drop_duplicates(['title', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = df_orig['label'].value_counts()\n",
    "ordered_counts = list(map(lambda x: counts[x], range(0, 7)))\n",
    "c = ['green', 'green', 'green', 'grey', 'red', 'red', 'red']\n",
    "t_l = ['0-Strongly likely Not Trafficking', '1-Likely Not Trafficking', '2-Weakly likely Not Trafficking', '3-Unsure',\n",
    "      '4-Weakly likely Trafficking', '5-Likely Trafficking', '6-Strongly Likely Trafficking']\n",
    "plt.barh(range(0,7), width=ordered_counts, color=c, alpha=0.4)\n",
    "plt.yticks(range(0,7), t_l)\n",
    "plt.xlabel('Ad Count')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig('../results/ads_count_dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (ordered_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df_orig['label'].values.tolist()\n",
    "\n",
    "binary_true_labels = [0] * len(true_labels)\n",
    "for ind, label in enumerate(true_labels):\n",
    "    if label >= 4:\n",
    "        binary_true_labels[ind] = 1\n",
    "\n",
    "df_orig['binary_label'] = binary_true_labels\n",
    "# df_unique = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=20)\n",
    "encoded_vecs = svd.fit_transform(bigram_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, algorithm='best', alpha=1.0)\n",
    "clusterer.fit(encoded_vecs)\n",
    "#     print (clusterer.labels_)\n",
    "labels = clusterer.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "unique_labels = set(labels)\n",
    "probs = clusterer.probabilities_\n",
    "#     colors = [plt.cm.Spectral(each)\n",
    "#               for each in np.linspace(0, 1, len(set(true_labels))]\n",
    "print (\"Number of labels : \" + str(len(list(set(clusterer.labels_)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig['cluster_label'] = labels\n",
    "df_orig['sim_check'] = False\n",
    "df_orig['sim_index'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairwise_similarities(mat):\n",
    "    mat = np.asarray(mat.todense())\n",
    "    sim_scores = np.zeros((mat.shape[0], mat.shape[0]))\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(i+1, mat.shape[0]):\n",
    "#             print (\"i : {}, j : {}\".format(i,j))\n",
    "            if i == j:\n",
    "                continue\n",
    "#             print (len(mat[i]))\n",
    "            sim_scores[i][j] = (mat[i]==mat[j]).all()\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "filtered_df = pd.DataFrame(columns=df_orig.columns)\n",
    "# unique_labels = list(unique_labels) + [-1]\n",
    "blacklisted_global_indices = []\n",
    "for l in unique_labels:\n",
    "    df_fil = df_orig[df_orig['cluster_label']==l].copy()\n",
    "    if l == -1:\n",
    "        filtered_df = pd.concat((filtered_df, df_fil), axis=0)\n",
    "        continue\n",
    "    indices = list(df_fil.index)\n",
    "    df_fil.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    ads = list(df_fil['content_p'])\n",
    "#     print (ads)\n",
    "    count_vectorizer = CountVectorizer(lowercase=True, ngram_range=(2,3), stop_words=stop_words)\n",
    "    ads_vectors = count_vectorizer.fit_transform(ads)\n",
    "#     sim_scores = calculate_pairwise_similarities(ads_vectors)\n",
    "    sim_scores = cosine_similarity(ads_vectors, dense_output=True)\n",
    "    sim_scores *= np.tri(*sim_scores.shape)\n",
    "    np.fill_diagonal(sim_scores, 0.0)\n",
    "\n",
    "    indices_similar = np.where(sim_scores>0.998)\n",
    "    cluster_tuples = zip(indices_similar[0], indices_similar[1])\n",
    "#     print (cluster_tuples)\n",
    "#     indices_similar = np.where(sim_scores == 1)\n",
    "#     x = indices_similar[0]\n",
    "#     y = indices_similar[1]\n",
    "#     print (sim_scores)\n",
    "#     print (y)\n",
    "    blacklisted_indices = []\n",
    "    \n",
    "    for tup in cluster_tuples:\n",
    "#         print (tup)\n",
    "        if tup[0] > tup[1]:\n",
    "            blacklisted_indices.append(indices[tup[1]])\n",
    "            blacklisted_global_indices.append(indices[tup[1]])\n",
    "#     print (df_orig.loc[blacklisted_global_indices, 'body'])\n",
    "#     break\n",
    "#     blacklisted_indices = list(set(blacklisted_indices))\n",
    "    print (indices, set(blacklisted_indices))\n",
    "    remain_ind = [x for x in indices if x not in blacklisted_indices]\n",
    "    df_orig.loc[blacklisted_indices, 'sim_index'] = remain_ind[0] if len(remain_ind) >0 else -1\n",
    "#     if len(df_fil) < 10:\n",
    "#         print (df_fil['body'])\n",
    "#         print (sim_scores)\n",
    "#         print (blacklisted_indices)\n",
    "#     df_fil.drop(blacklisted_indices, inplace=True)\n",
    "#     filtered_df = pd.concat((filtered_df, df_fil), axis=0)\n",
    "blacklisted_global_indices = list(set(blacklisted_global_indices))\n",
    "df_orig.loc[blacklisted_global_indices, 'sim_check'] = True\n",
    "\n",
    "#     break\n",
    "print (df_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(blacklisted_global_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_orig.at[720, 'body'])\n",
    "print (df_orig.at[3916, 'body'])\n",
    "print (df_orig.at[1319, 'body'])\n",
    "print (df_orig.at[4352, 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(set(blacklisted_global_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_df = df_orig[df_orig['sim_check'] == False].copy()\n",
    "print (filtered_df.shape)\n",
    "content = []\n",
    "# content = df_data['body']\n",
    "# content = content.replace(np.nan, '', regex=True)\n",
    "for index, row in filtered_df.iterrows():\n",
    "    c = ''\n",
    "\n",
    "    if type(row['body']) == type(' '):\n",
    "        c = c + row['body']\n",
    "    # c = row['title'] + ' ' + row['body']\n",
    "    c = re.sub(r'\\d+', '', c)\n",
    "    if type(row['Name']) == type(''):\n",
    "        name = row['Name'].split(';')\n",
    "        for n in name:\n",
    "            name_regex = re.compile(re.escape(n), re.IGNORECASE)\n",
    "            c = name_regex.sub('', c)\n",
    "        c = re.sub(r'[^\\x00-\\x7F]+',' ', c)\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        c = re.sub(cleanr, '', c)\n",
    "#     df_data.at[index, 'body'] = c\n",
    "    content.append(c)\n",
    "filtered_df['content_p'] = content\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2,4), norm='l2', \n",
    "     stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "features_col = vectorizer.get_feature_names()\n",
    "print(bigram_matrix.shape)\n",
    "# print (bigram_matrix[0])\n",
    "\n",
    "# svd = TruncatedSVD(n_components=3)\n",
    "# svd.fit_transform(bigram_matrix)\n",
    "# np.save('../data/modalities_data/tf_idf_bigrams.npy', bigram_matrix)\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bigram_matrix.sum(axis=1)\n",
    "print (s.shape)\n",
    "print (len(np.argwhere(s==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=64)\n",
    "encoded_vecs = svd.fit_transform(bigram_matrix)\n",
    "# print (a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (encoded_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "# rcParams['figure.figsize'] = 10,100\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.imshow(encoded_vecs)\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "origin = np.zeros(64)\n",
    "dist = np.zeros(len(encoded_vecs), dtype=float)\n",
    "for i in range(len(encoded_vecs)):\n",
    "    dist[i] = distance.euclidean(origin, encoded_vecs[i])\n",
    "    \n",
    "noisy_set = np.argwhere(dist < 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bigram_matrix))\n",
    "print (bigram_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bigram_matrix.sum(axis=1)\n",
    "print (s.shape)\n",
    "print (len(np.argwhere(s==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['noise'] = False\n",
    "filtered_df['index1'] = filtered_df.index\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "print (max(filtered_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(bigram_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_matrix = bigram_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(bigram_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "# spatial.distance.euclidean(origin, encoded_vecs[0])\n",
    "noisy_list = []\n",
    "# filtered_df.reset_index(drop=True, inplace=True)\n",
    "noisy_list=list(map(lambda x : x[0], noisy_set))\n",
    "print(len(noisy_list))\n",
    "filtered_df.loc[noisy_list, 'noise'] = True\n",
    "\n",
    "# noisy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.set_index('index1', inplace=True)\n",
    "df_orig = df_orig.join(filtered_df['noise'], how='outer')\n",
    "filtered_df['index1'] = filtered_df.index\n",
    "filtered_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonoise = filtered_df[filtered_df['noise'] == False].copy()\n",
    "df_nonoise.reset_index(drop=True, inplace=True)\n",
    "bigram_matrix = np.delete(bigram_matrix, noisy_list, axis=0)\n",
    "encoded_vecs = np.delete(encoded_vecs, noisy_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (encoded_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_set = noisy_set.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['green', 'red', 'blue']\n",
    "binary_true_labels = list(df_nonoise['binary_label'])\n",
    "color_array = []\n",
    "for ind, l in enumerate(binary_true_labels):\n",
    "    color_array.append(colors[int(l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(color_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "X_embedded = umap.UMAP().fit_transform(encoded_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################### HBDSCAN ############################\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, algorithm='best', alpha=1.0)\n",
    "clusterer.fit(encoded_vecs)\n",
    "#     print (clusterer.labels_)\n",
    "labels = clusterer.labels_\n",
    "labels_clustering = labels\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "unique_labels = set(labels)\n",
    "probs = clusterer.probabilities_\n",
    "#     colors = [plt.cm.Spectral(each)\n",
    "#               for each in np.linspace(0, 1, len(set(true_labels))]\n",
    "print (\"Number of labels : \" + str(len(list(set(clusterer.labels_)))))\n",
    "#     palette = sns.color_palette()\n",
    "#     cluster_colors = [sns.desaturate(palette[col], sat)\n",
    "#                       if col >= 0 else (0.5, 0.5, 0.5) for col, sat in\n",
    "#                       zip(clusterer.labels_, clusterer.probabilities_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_true_labels = np.asarray(binary_true_labels)\n",
    "# noise_labels = np.where(labels==-1)\n",
    "no_noise_labels = np.where(labels!=-1)\n",
    "tr_labels = np.where(binary_true_labels==1)\n",
    "print (binary_true_labels.shape)\n",
    "print (no_noise_labels[0].shape)\n",
    "print (tr_labels)\n",
    "# lst3 = [value for value in tr_labels if value in noise_labels] \n",
    "common_tp = np.intersect1d(no_noise_labels[0], tr_labels[0])\n",
    "print (common_tp.shape)\n",
    "total_p = len(common_tp)/len((tr_labels[0]))\n",
    "print (total_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without_threshold_labels = [0]*len(binary_true_labels)\n",
    "# for i in common_tp:\n",
    "#     without_threshold_labels[i] = 1\n",
    "\n",
    "# pscore = classification_report(binary_true_labels, without_threshold_labels)\n",
    "# print (pscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "noise_labels = np.where(labels==-1)\n",
    "print (len(noise_labels[0]))\n",
    "noise_vecs = encoded_vecs[noise_labels[0],:][:]\n",
    "noise_dist = np.zeros(len(noise_vecs), dtype=float)\n",
    "for i in range(len(noise_dist)):\n",
    "    noise_dist[i] = distance.euclidean(origin, encoded_vecs[i])\n",
    "print (noise_dist)\n",
    "noise_dist = np.sort(noise_dist)\n",
    "plt.plot(noise_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_indices = []\n",
    "rgba_colors = np.zeros((len(binary_true_labels),4))\n",
    "for ind, cl in enumerate(labels):\n",
    "    if cl == -1:\n",
    "        continue\n",
    "    cluster_idx = np.argwhere(labels == cl).reshape(-1)\n",
    "    anomaly_indices += list(cluster_idx)\n",
    "rgba_colors[:, 0] = 0\n",
    "rgba_colors[:, 3] = 0.01\n",
    "# print (anomaly_indices)\n",
    "for ind in anomaly_indices:\n",
    "    rgba_colors[ind, 0] = 1\n",
    "    rgba_colors[ind, 3] = 0.1\n",
    "    \n",
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.scatter(X_embedded.T[0], X_embedded.T[1], color=rgba_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.scatter(X_embedded.T[0], X_embedded.T[1], c=color_array, alpha=0.1)\n",
    "# plt.title(\"Clustering accuracy={}, fmeasure_synth={}, number_of_labels={}\".format(clustering_acc, fmeasure, \n",
    "#                                                                                   len(unique_labels)))\n",
    "plt.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "right='off', left='off', labelleft='off')\n",
    "plt.text(15, 15, 'TLDetect', fontsize=24)\n",
    "plt.savefig('../results/embedding_scatter_trafficking10k.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10,2\n",
    "\n",
    "\n",
    "a = [0,1,2,3,4,5]\n",
    "# b = [0,1,2,3,4,5]\n",
    "\n",
    "from itertools import combinations \n",
    "tuples = list(combinations(a, 2))\n",
    "count = 0\n",
    "# for i in range(5):\n",
    "#     if i%5 == 0:\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         continue\n",
    "#     ax1 = plt.subplot(1,5,1)\n",
    "#     ax2 = plt.subplot(1,5,2)\n",
    "#     ax3 = plt.subplot(1,5,3)\n",
    "#     ax4 = plt.subplot(1,5,4)\n",
    "#     ax5 = plt.subplot(1,5,5)\n",
    "    \n",
    "#     ax1.scatter(encoded_vecs.T[tuples[i][0]], encoded_vecs.T[tuples[i][1]], c=color_array, alpha=0.2)\n",
    "#     ax1.title.set_text(\"{},{}\".format(tuples[i][0], tuples[i][1]))\n",
    "    \n",
    "#     ax2.scatter(encoded_vecs.T[tuples[i+1][0]], encoded_vecs.T[tuples[i+1][1]], c=color_array, alpha=0.2)\n",
    "#     ax2.title.set_text(\"{},{}\".format(tuples[i+1][0], tuples[i+1][1]))\n",
    "    \n",
    "#     ax3.scatter(encoded_vecs.T[tuples[i+2][0]], encoded_vecs.T[tuples[i+2][1]], c=color_array, alpha=0.2)\n",
    "#     ax3.title.set_text(\"{},{}\".format(tuples[i+2][0], tuples[i+2][1]))\n",
    "    \n",
    "#     ax4.scatter(encoded_vecs.T[tuples[i+3][0]], encoded_vecs.T[tuples[i+3][1]], c=color_array, alpha=0.2)\n",
    "#     ax4.title.set_text(\"{},{}\".format(tuples[i+3][0], tuples[i+3][1]))\n",
    "    \n",
    "#     ax5.scatter(encoded_vecs.T[tuples[i+4][0]], encoded_vecs.T[tuples[i+4][1]], c=color_array, alpha=0.2)\n",
    "#     ax5.title.set_text(\"{},{}\".format(tuples[i+4][0], tuples[i+4][1]))\n",
    "    \n",
    "    \n",
    "#     ax2.scatter(encoded_vecs.T[0], encoded_vecs.T[2], c=color_array, alpha=0.2)\n",
    "#     ax2.title.set_text(\"{},{}\".format(tup[i][0], tup[i][1]))\n",
    "#     plt.tight_layout()\n",
    "#     # plt.savefig('../results/svd_components_0.png')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    ax1 = plt.subplot(1,5,1)\n",
    "    ax2 = plt.subplot(1,5,2)\n",
    "    ax3 = plt.subplot(1,5,3)\n",
    "    ax4 = plt.subplot(1,5,4)\n",
    "    ax5 = plt.subplot(1,5,5)\n",
    "    \n",
    "    ax1.scatter(encoded_vecs.T[i], encoded_vecs.T[i+1], c=color_array, alpha=0.1)\n",
    "    ax1.title.set_text(\"{},{}\".format(i, i+1))\n",
    "    \n",
    "    ax2.scatter(encoded_vecs.T[i], encoded_vecs.T[i+2], c=color_array, alpha=0.1)\n",
    "    ax2.title.set_text(\"{},{}\".format(i, i+2))\n",
    "    \n",
    "    ax3.scatter(encoded_vecs.T[i], encoded_vecs.T[i+3], c=color_array, alpha=0.1)\n",
    "    ax3.title.set_text(\"{},{}\".format(i, i+3))\n",
    "    \n",
    "    ax4.scatter(encoded_vecs.T[i], encoded_vecs.T[i+4], c=color_array, alpha=0.1)\n",
    "    ax4.title.set_text(\"{},{}\".format(i, i+4))\n",
    "    \n",
    "    ax5.scatter(encoded_vecs.T[i], encoded_vecs.T[i+5], c=color_array, alpha=0.1)\n",
    "    ax5.title.set_text(\"{},{}\".format(i, i+5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/svd_components_{}.png'.format(i))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcParams['figure.figsize'] = 6,2\n",
    "# fig, axs = plt.subplots(1, 3)\n",
    "# # ax1 = plt.subplot(1,4,1)\n",
    "# # ax2 = plt.subplot(1,4,2)\n",
    "# # ax3 = plt.subplot(1,4,3)\n",
    "# # ax4 = plt.subplot(1,4,4)\n",
    "\n",
    "# axs[0, 0].scatter(encoded_vecs.T[0], encoded_vecs.T[3], c=color_array, alpha=0.2)\n",
    "# axs[0, 0].text(0.5, 0.3, \"{},{}\".format(0, 3))\n",
    "# # ax1.title.set_text(\"{},{}\".format(i, i+1))\n",
    "\n",
    "# axs[0, 1].scatter(encoded_vecs.T[2], encoded_vecs.T[3], c=color_array, alpha=0.2)\n",
    "# axs[0, 1].text(-0.05, 0.3,\"{},{}\".format(2, 3))\n",
    "\n",
    "# axs[0, 2].scatter(encoded_vecs.T[7], encoded_vecs.T[10], c=color_array, alpha=0.2)\n",
    "# axs[0, 2].text(0.4, 0.4,\"{},{}\".format(1, 5))\n",
    "\n",
    "# axs[1, 0].scatter(encoded_vecs.T[4], encoded_vecs.T[9], c=color_array, alpha=0.2)\n",
    "# axs[1, 0].text(0.6, 0.5,\"{},{}\".format(4, 9))\n",
    "\n",
    "# axs[1, 1].scatter(encoded_vecs.T[1], encoded_vecs.T[5], c=color_array, alpha=0.2)\n",
    "# axs[1, 1].text(0.5, 0.6,\"{},{}\".format(1, 5))\n",
    "\n",
    "# axs[1, 2].scatter(encoded_vecs.T[5], encoded_vecs.T[9], c=color_array, alpha=0.2)\n",
    "# axs[1, 2].text(0.5, 0.5,\"{},{}\".format(1, 5))\n",
    "\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../results/svd_components_trafficking10k.png'.format(i))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subgraphs(mat_data, cl_ind):\n",
    "    filtered_sub = mat_data[cl_ind,:][:]\n",
    "    bigrams_count = np.count_nonzero(filtered_sub, axis=0)\n",
    "#     print (\"Bigrams count matrix shape : {}\".format(bigrams_count.shape))\n",
    "    zero_count_index = np.where(bigrams_count==0)[1]\n",
    "#     print (zero_count_index)\n",
    "    one_count_index = list(np.where(bigrams_count==1)[1])\n",
    "#     print (\"One count index : {}\".format(one_count_index))\n",
    "#     print (\"Max zero count index : {} \".format(max(zero_count_index)))\n",
    "    core_bigrams_index = list(np.where(bigrams_count>1)[1])\n",
    "#     print (\"Core Bigrams index : {}\".format(len(core_bigrams_index)))\n",
    "    outer_bigrams_index = list(one_count_index + core_bigrams_index)\n",
    "    \n",
    "    bigram_induced_graph = mat_data[:][:,core_bigrams_index]\n",
    "#     print (\"Bigram Induced Graph shape : {}\".format(bigram_induced_graph.shape))\n",
    "    ads_count = np.count_nonzero(bigram_induced_graph, axis=1)\n",
    "    ads_in_shell_index = np.where(ads_count>1)[0]\n",
    "    \n",
    "    not_core_ads = list(set(list(ads_in_shell_index)) - set(cl_ind))\n",
    "    not_core_bigrams = list(set(list(one_count_index)) - set(core_bigrams_index))\n",
    "    mat_copy = np.asarray(mat_data)\n",
    "    for i in not_core_ads:\n",
    "        for j in not_core_bigrams:\n",
    "            mat_copy[i][j] = 0\n",
    "    shell_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "    mat_copy[not_core_ads, :] = 0\n",
    "    outer_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "    mat_copy[:,one_count_index] = 0\n",
    "    core_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "#     print (\"Shell Subgraph shape : {}\". format(shell_subgraph.shape))\n",
    "#     print (\"Outer Subgraph shape : {}\". format(outer_subgraph.shape))\n",
    "#     print (\"Core Subgraph shape : {}\". format(core_subgraph.shape))\n",
    "#     print (zero_count_index.shape)\n",
    "#     print (zero_count_index[1])\n",
    "#     outer_subgraph = np.delete(filtered_sub, zero_count_index, axis=1)\n",
    "#     core_subgraph = outer_subgraph.copy()\n",
    "#     core_subgraph[:,one_count_index] = 0\n",
    "#     core_subgraph = np.delete(filtered_sub, list(set(list(zero_count_index) + list(one_count_index))), axis=1)\n",
    "    shell_subgraph = np.asarray(shell_subgraph)\n",
    "    outer_subgraph = np.asarray(outer_subgraph)\n",
    "    core_subgraph = np.asarray(core_subgraph)\n",
    "#     print (\"subgraph sizes: {}, {}, {}\".format(core_subgraph.shape, outer_subgraph.shape, shell_subgraph.shape))\n",
    "    \n",
    "    return shell_subgraph, outer_subgraph, core_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.scatter(encoded_vecs.T[2], encoded_vecs.T[3], c=color_array, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores = []\n",
    "df_nonoise['cluster_label'] = labels\n",
    "df_nonoise['probabilities'] = probs\n",
    "for c in unique_labels:\n",
    "    df_fil = df_nonoise[df_nonoise['cluster_label']==c]\n",
    "    score = sum(df_fil['label'])/len(df_fil['label'])\n",
    "    avg_scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_unweighted_density(core_mat):\n",
    "    edge_weight = np.count_nonzero(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "#     print (ads_core_num)\n",
    "#     print (bigrams_core_num)\n",
    "    return edge_weight/(ads_core_num * bigrams_core_num + 1)\n",
    "\n",
    "def calculate_weighted_density(core_mat):\n",
    "    edge_weight = np.sum(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "\n",
    "    return edge_weight/(ads_core_num * bigrams_core_num + 1)\n",
    "\n",
    "def calculate_unweighted_fraudar_score(core_mat):\n",
    "    edge_weight = np.count_nonzero(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return edge_weight/(ads_core_num + bigrams_core_num + 1)\n",
    "\n",
    "def calculate_weighted_fraudar_score(core_mat):\n",
    "    edge_weight = np.sum(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return edge_weight/(ads_core_num + bigrams_core_num + 1)\n",
    "\n",
    "def calculate_unweighted_edge_per_score(core_mat, outer_mat):\n",
    "    core_edges = np.count_nonzero(core_mat)\n",
    "    outer_edges = np.count_nonzero(outer_mat)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return ((core_edges + 1)/(outer_edges +1))*(math.log(bigrams_core_num+1))\n",
    "\n",
    "def calculate_weighted_edge_per_score(core_mat, outer_mat):\n",
    "    core_edges = np.sum(core_mat)\n",
    "    outer_edges = np.sum(outer_mat)\n",
    "    \n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return ((core_edges + 1)/(outer_edges +1))*(math.log(bigrams_core_num+1))\n",
    "\n",
    "\n",
    "def calculate_custom_score(core_mat, outer_mat):\n",
    "    bigram_degrees = np.count_nonzero(np.asarray(core_mat), axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    outer_edges = np.count_nonzero(outer_mat)\n",
    "    bigram_degrees = bigram_degrees/bigrams_core_num\n",
    "    \n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    \n",
    "    return (np.sum(bigram_degrees)/(outer_edges+1))*(math.log(bigrams_core_num+1))*(math.log(ads_core_num+1))\n",
    "# #     print (mat.shape)\n",
    "#     edges_nonzero = np.count_nonzero(mat, axis=0)\n",
    "#     unique, counts = np.unique(edges_nonzero, return_counts=True)\n",
    "#     degree_counts = dict(zip(unique, counts))\n",
    "#     numerator = 0.0\n",
    "#     denominator = 0.0\n",
    "#     half = max(mat.shape[0]/2, 2)\n",
    "#     for k, v in degree_counts.items():\n",
    "#         if k == 0:\n",
    "#             continue\n",
    "#         elif k <= half:\n",
    "#             denominator += k*v\n",
    "#         else:\n",
    "#             denominator += k*v\n",
    "#             numerator += k*v\n",
    "#     if denominator == 0.0:\n",
    "#         return 0.0\n",
    "#     else:\n",
    "#         return numerator/denominator\n",
    "\n",
    "# def calculate_weighted_edge_per_score(mat):\n",
    "#     return 0.0\n",
    "\n",
    "#Should be shell_mat instead of outer_mat, change once you figure out how to get shell subgraph.\n",
    "def calculate_unweighted_modularity_score(core_mat, outer_mat, total_edges):\n",
    "#     total_edges = math.log(total_edges)\n",
    "    ad_degrees = np.count_nonzero(np.asarray(outer_mat), axis=1)\n",
    "#     print (ad_degrees)\n",
    "    bigram_degrees = np.count_nonzero(np.asarray(outer_mat), axis=0)\n",
    "#     print (bigram_degrees)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_index = np.where(ads_count>0)[0]\n",
    "    bigrams_core_index = np.where(bigrams_count>0)[0]\n",
    "#     ads_list_index = [x for x in range(core_mat.shape[0])]\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "    summation = 0.0\n",
    "    for i in ads_core_index:\n",
    "        for j in bigrams_core_index:\n",
    "            if core_mat[i][j] == 0:\n",
    "                adj = 0\n",
    "            else:\n",
    "                adj = 1\n",
    "            if adj == 1:\n",
    "                summation += (adj - (ad_degrees[i] * bigram_degrees[j])/total_edges)\n",
    "\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "#     summation = 0.0\n",
    "#     for k in range(nonzero_edges.shape[0]):\n",
    "#         ad_index = nonzero_edges[k][0]\n",
    "#         big_index = nonzero_edges[k][1]\n",
    "# #         print (ad_index)\n",
    "# #         print (big_index)\n",
    "# #         if core_mat[ad_index][big_index] != 0:\n",
    "# #             adj = 1\n",
    "# #         else:\n",
    "# #             adj = 0\n",
    "        \n",
    "#         summation += (1 - (ad_degrees[ad_index] * bigram_degrees[big_index])/(2*total_edges))\n",
    "    \n",
    "    return (summation/total_edges)*(math.log(len(ads_core_index)+1))\n",
    "\n",
    "def calculate_weighted_modularity_score(core_mat, outer_mat, total_edges):\n",
    "    ad_degrees = np.sum(np.asarray(outer_mat), axis=1)\n",
    "    bigram_degrees = np.sum(np.asarray(outer_mat), axis=0)\n",
    "    \n",
    "    ads_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_index = np.where(ads_count>0)[0]\n",
    "    bigrams_core_index = np.where(bigrams_count>0)[0]\n",
    "\n",
    "    \n",
    "\n",
    "#     ads_list_index = [x for x in range(core_mat.shape[0])]\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "    summation = 0.0\n",
    "    for i in ads_core_index:\n",
    "        for j in bigrams_core_index:\n",
    "            if core_mat[i][j] != 0:\n",
    "                summation += (core_mat[i][j] - (ad_degrees[i] * bigram_degrees[j])/total_edges)\n",
    "#     summation = 0.0\n",
    "#     for k in range(nonzero_edges.shape[0]):\n",
    "#         ad_index = nonzero_edges[k][0]\n",
    "#         big_index = nonzero_edges[k][1]\n",
    "#         summation += (outer_mat[ad_index][big_index] - (ad_degrees[ad_index] * bigram_degrees[big_index])/(2*total_edges))\n",
    "    \n",
    "    return (summation/total_edges)*(math.log(len(ads_core_index)+1))\n",
    "\n",
    "def calculate_pairwise_modularity(mat):\n",
    "    mat = np.asarray(mat.todense())\n",
    "    sim_scores = np.zeros((mat.shape[0], mat.shape[0]))\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(i+1, mat.shape[0]):\n",
    "#             print (\"i : {}, j : {}\".format(i,j))\n",
    "            if i == j:\n",
    "                continue\n",
    "#             print (len(mat[i]))\n",
    "            sim_scores[i][j] = calculate_modularity_score(np.vstack((mat[i], mat[j])))\n",
    "    \n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bigram_matrix.sum(axis=1)\n",
    "print (s.shape)\n",
    "print (len(np.argwhere(s==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_all_metrics(bigram_matrix, unique_labels, labels, df_data):\n",
    "    eigen_ratios = []\n",
    "    weighted_cluster_density = []\n",
    "    unweighted_cluster_density = []\n",
    "    unweighted_fraudar_scores = []\n",
    "    weighted_fraudar_scores = []\n",
    "    unweighted_outer_edge_perc_scores = []\n",
    "    weighted_outer_edge_perc_scores = []\n",
    "    unweighted_shell_edge_perc_scores = []\n",
    "    weighted_shell_edge_perc_scores = []\n",
    "    weighted_outer_modularity_scores = []\n",
    "    unweighted_outer_modularity_scores = []\n",
    "    weighted_shell_modularity_scores = []\n",
    "    unweighted_shell_modularity_scores = []\n",
    "    pairwise_similarity = []\n",
    "    custom_score = []\n",
    "    avg_label_scores = []\n",
    "    max_label_scores = []\n",
    "    sum_label_scores = []\n",
    "    avg_binary_scores = []\n",
    "    max_binary_scores = []\n",
    "    sum_binary_scores = []\n",
    "    clusters = []\n",
    "    cluster_counts = []\n",
    "    \n",
    "    total_edges_unweighted = np.count_nonzero(bigram_matrix)\n",
    "    total_edges_weighted = np.sum(bigram_matrix)\n",
    "    for l in unique_labels:\n",
    "#         s = bigram_matrix.sum(axis=1)\n",
    "        if l== -1:\n",
    "            weighted_cluster_density.append(0)\n",
    "            unweighted_cluster_density.append(0)\n",
    "            weighted_fraudar_scores.append(0)\n",
    "            unweighted_fraudar_scores.append(0)\n",
    "            weighted_outer_edge_perc_scores.append(0)\n",
    "            unweighted_outer_edge_perc_scores.append(0)\n",
    "            weighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_outer_modularity_scores.append(0)\n",
    "            weighted_outer_modularity_scores.append(0)\n",
    "            unweighted_shell_modularity_scores.append(0)\n",
    "            weighted_shell_modularity_scores.append(0)\n",
    "            pairwise_similarity.append(0)\n",
    "            custom_score.append(0)\n",
    "            cluster_counts.append(len(cluster_idx))\n",
    "            eigen_ratios.append(0)\n",
    "            clusters.append(l)\n",
    "            \n",
    "            max_label_scores.append(0)\n",
    "            avg_label_scores.append(0)\n",
    "            sum_label_scores.append(0)\n",
    "\n",
    "            max_binary_scores.append(0)\n",
    "            avg_binary_scores.append(0)\n",
    "            sum_binary_scores.append(0)\n",
    "            continue\n",
    "#         print (s.shape)\n",
    "#         print (\"bigram matrix sum : {}\".format(bigram_matrix.sum()))\n",
    "#         print (\"Zero elems: {}\".format(len(np.argwhere(s==0))))\n",
    "        cluster_idx = np.argwhere(labels == l).reshape(-1)\n",
    "#         print (l, len(cluster_idx))\n",
    "        \n",
    "        print (cluster_idx)\n",
    "        shell_subgraph, outer_subgraph, core_subgraph = get_all_subgraphs(bigram_matrix.copy(), cluster_idx)\n",
    "#         print (l, len(cluster_idx), core_subgraph.sum(), outer_subgraph.sum(), shell_subgraph.sum())\n",
    "        \n",
    "\n",
    "        df_filt = df_data[df_data['cluster_label']== l]\n",
    "        if len(df_filt) == 0 or core_subgraph.shape[0] == 0 or shell_subgraph.shape[0] == 0 or outer_subgraph.shape[0] == 0:\n",
    "            weighted_cluster_density.append(0)\n",
    "            unweighted_cluster_density.append(0)\n",
    "            weighted_fraudar_scores.append(0)\n",
    "            unweighted_fraudar_scores.append(0)\n",
    "            weighted_outer_edge_perc_scores.append(0)\n",
    "            unweighted_outer_edge_perc_scores.append(0)\n",
    "            weighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_outer_modularity_scores.append(0)\n",
    "            weighted_outer_modularity_scores.append(0)\n",
    "            unweighted_shell_modularity_scores.append(0)\n",
    "            weighted_shell_modularity_scores.append(0)\n",
    "            pairwise_similarity.append(0)\n",
    "            custom_score.append(0)\n",
    "            cluster_counts.append(len(cluster_idx))\n",
    "            eigen_ratios.append(0)\n",
    "            clusters.append(l)\n",
    "            \n",
    "            max_label_scores.append(0)\n",
    "            avg_label_scores.append(0)\n",
    "            sum_label_scores.append(0)\n",
    "\n",
    "            max_binary_scores.append(0)\n",
    "            avg_binary_scores.append(0)\n",
    "            sum_binary_scores.append(0)\n",
    "            continue\n",
    "#         elif core_subgraph.shape[0] == 0 or shell_subgraph.shape[0] == 0 or outer_subgraph.shape[0] == 0:\n",
    "#             continue\n",
    "        print (l, df_filt.shape)\n",
    "        max_label_scores.append(max(df_filt['label']))\n",
    "        avg_label_scores.append(sum(df_filt['label'])/len(df_filt['label']))\n",
    "        sum_label_scores.append(sum(df_filt['label']))\n",
    "\n",
    "        max_binary_scores.append(max(df_filt['binary_label']))\n",
    "        avg_binary_scores.append(sum(df_filt['binary_label'])/len(df_filt['binary_label']))\n",
    "        sum_binary_scores.append(sum(df_filt['binary_label']))\n",
    "        \n",
    "        local_content = list(df_filt['content_p'])\n",
    "        count_vectorizer = TfidfVectorizer(ngram_range=(2,2), use_idf=False)\n",
    "        count_data = count_vectorizer.fit_transform(local_content)\n",
    "\n",
    "        svd = TruncatedSVD(n_components=2)\n",
    "        local_vecs = svd.fit_transform(count_data)\n",
    "        w = svd.singular_values_\n",
    "        eigen_rat = w[1]/w[0]\n",
    "        eigen_ratios.append(eigen_rat)\n",
    "        \n",
    "        print (outer_subgraph.shape)\n",
    "        print (core_subgraph.shape)\n",
    "        pairwise_sim_mat = cosine_similarity(outer_subgraph, dense_output=True)\n",
    "        pairwise_sim_mat = np.tril(pairwise_sim_mat, -1)\n",
    "#         print (sum(pairwise_sim_mat).shape)\n",
    "        print (pairwise_sim_mat.sum())\n",
    "        an_score = calculate_weighted_edge_per_score(core_subgraph, outer_subgraph)\n",
    "        names = [x.lower() if type(x) == type('') else None for x in df_filt['Name'].unique()]\n",
    "        if math.nan in names:\n",
    "            names.remove(math.nan)\n",
    "        if None in names:\n",
    "            names.remove(None)\n",
    "        names = list(set(names))\n",
    "        c_score = an_score * max(0, len(names)-1)\n",
    "        print (\"Scores : {}, {}, {}\".format(an_score, len(names), c_score))\n",
    "        weighted_cluster_density.append(calculate_weighted_density(core_subgraph))\n",
    "        unweighted_cluster_density.append(calculate_unweighted_density(core_subgraph))\n",
    "        weighted_fraudar_scores.append(calculate_weighted_fraudar_score(core_subgraph))\n",
    "        unweighted_fraudar_scores.append(calculate_unweighted_fraudar_score(core_subgraph))\n",
    "        weighted_outer_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "        unweighted_outer_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "        weighted_shell_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "        unweighted_shell_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "        unweighted_outer_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, outer_subgraph, total_edges_unweighted))\n",
    "        weighted_outer_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, outer_subgraph, total_edges_weighted))\n",
    "        unweighted_shell_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, shell_subgraph, total_edges_unweighted))\n",
    "        weighted_shell_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, shell_subgraph, total_edges_weighted))\n",
    "        custom_score.append(c_score)\n",
    "        pairwise_similarity.append(pairwise_sim_mat.sum()/len(cluster_idx))\n",
    "        cluster_counts.append(len(cluster_idx))\n",
    "        clusters.append(l)\n",
    "        \n",
    "        count_data = []\n",
    "        local_content = []\n",
    "        shell_subgraph = []\n",
    "        core_subgraph = []\n",
    "        outer_subgraph = []\n",
    "        if l % 50 == 0:\n",
    "            print (l)\n",
    "            gc.collect()\n",
    "#     original_labels = labels.copy()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['weighted_cluster_density'] = weighted_cluster_density\n",
    "    metrics['unweighted_cluster_density'] = unweighted_cluster_density\n",
    "    metrics['weighted_fraudar_scores'] = weighted_fraudar_scores\n",
    "    metrics['unweighted_fraudar_scores'] = unweighted_fraudar_scores\n",
    "    metrics['weighted_outer_edge_perc_scores'] = weighted_outer_edge_perc_scores\n",
    "    metrics['unweighted_outer_edge_perc_scores'] = unweighted_outer_edge_perc_scores\n",
    "    metrics['weighted_shell_edge_perc_scores'] = weighted_shell_edge_perc_scores\n",
    "    metrics['unweighted_shell_edge_perc_scores'] = unweighted_shell_edge_perc_scores\n",
    "    metrics['unweighted_outer_modularity_scores'] = unweighted_outer_modularity_scores\n",
    "    metrics['weighted_outer_modularity_scores'] = weighted_outer_modularity_scores\n",
    "    metrics['unweighted_shell_modularity_scores'] = unweighted_shell_modularity_scores\n",
    "    metrics['weighted_shell_modularity_scores'] = weighted_shell_modularity_scores\n",
    "    metrics['pairwise_similarity'] = pairwise_similarity\n",
    "    metrics['custom_score'] = custom_score\n",
    "    metrics['avg_label_scores'] = avg_label_scores\n",
    "    metrics['sum_label_scores'] = sum_label_scores\n",
    "    metrics['max_label_scores'] = max_label_scores\n",
    "    metrics['avg_binary_scores'] = avg_binary_scores\n",
    "    metrics['max_binary_scores'] = max_binary_scores\n",
    "    metrics['sum_binary_scores'] = sum_binary_scores\n",
    "    metrics['eigen_ratios'] = eigen_ratios\n",
    "    metrics['clusters'] = clusters\n",
    "    metrics['labels'] = labels.copy()\n",
    "    metrics['cluster_counts'] = cluster_counts\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bigram_matrix.sum(axis=1)\n",
    "print (s.shape)\n",
    "print (len(np.argwhere(s==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_merging_metrics = get_all_metrics(bigram_matrix, unique_labels, labels, df_nonoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (max(pre_merging_metrics['custom_score']))\n",
    "print (pre_merging_metrics['clusters'][pre_merging_metrics['custom_score'].index(max(pre_merging_metrics['custom_score']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filt = df_nonoise[df_nonoise['cluster_label'] == 375]\n",
    "names = [x.lower() if type(x) == type('') else None for x in df_filt['Name'].unique()]\n",
    "if math.nan in names:\n",
    "    names.remove(math.nan)\n",
    "if None in names:\n",
    "    names.remove(None)\n",
    "names = list(set(names))\n",
    "print (len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_idx = np.argwhere(labels == 9).reshape(-1)\n",
    "print (cluster_idx)\n",
    "bigram_matrix[cluster_idx,:][:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (max(pre_merging_metrics['weighted_outer_edge_perc_scores']))\n",
    "plt.scatter(pre_merging_metrics['avg_label_scores'], pre_merging_metrics['weighted_outer_edge_perc_scores'], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 5,5\n",
    "clusters = pre_merging_metrics['clusters']\n",
    "pre_merging_metrics['eigen_ratios'][pre_merging_metrics['clusters'].index(-1)] = 1\n",
    "\n",
    "# plt.plot(pre_merging_metrics['eigen_ratios'])\n",
    "# plt.show()\n",
    "\n",
    "plt.scatter(pre_merging_metrics['avg_label_scores'], pre_merging_metrics['eigen_ratios'], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_ratios = pre_merging_metrics['eigen_ratios']\n",
    "eigen_np = np.array(eigen_ratios)\n",
    "clusters_nonhomogenous_index = np.where(eigen_np > 0.8)[0]\n",
    "clusters_nonhomogenous = [clusters[i] for i in clusters_nonhomogenous_index]\n",
    "# print (clusters_nonhomogenous)\n",
    "non_noisy_clusters = [x for x in clusters if x not in clusters_nonhomogenous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (bigram_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_cluster_idx = []\n",
    "original_labels = pre_merging_metrics['labels'].copy()\n",
    "for l in clusters_nonhomogenous:\n",
    "    cluster_idx = np.argwhere(original_labels == l).reshape(-1)\n",
    "    rerun_cluster_idx += list(cluster_idx)\n",
    "#     print (total_cluster_idx)\n",
    "print (len(rerun_cluster_idx))\n",
    "rerun_bigram_data = bigram_matrix[rerun_cluster_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=64)\n",
    "encoded_vecs_rerun = svd.fit_transform(rerun_bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "X_embedded_rerun = umap.UMAP().fit_transform(encoded_vecs_rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.scatter(X_embedded_rerun.T[0], X_embedded_rerun.T[1], alpha=0.1)\n",
    "# plt.title(\"Clustering accuracy={}, fmeasure_synth={}, number_of_labels={}\".format(clustering_acc, fmeasure, \n",
    "#                                                                                   len(unique_labels)))\n",
    "plt.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "right='off', left='off', labelleft='off')\n",
    "plt.text(15, 15, 'TLDetect', fontsize=24)\n",
    "# plt.savefig('../results/embedding_scatter_trafficking10k.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.fit(encoded_vecs_rerun)\n",
    "#     print (clusterer.labels_)\n",
    "rerun_clustering_labels = clusterer.labels_\n",
    "rerun_labels_clustering = labels\n",
    "rerun_n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_rerun_ = list(labels).count(-1)\n",
    "rerun_unique_labels = set(labels)\n",
    "rerun_probs = clusterer.probabilities_\n",
    "#     colors = [plt.cm.Spectral(each)\n",
    "#               for each in np.linspace(0, 1, len(set(true_labels))]\n",
    "print (\"Number of labels : \" + str(len(list(set(clusterer.labels_)))))\n",
    "#     palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_indices = []\n",
    "rgba_colors = np.zeros((len(binary_true_labels),4))\n",
    "for ind, cl in enumerate(labels):\n",
    "    if cl == -1:\n",
    "        continue\n",
    "    cluster_idx = np.argwhere(labels == cl).reshape(-1)\n",
    "    anomaly_indices += list(cluster_idx)\n",
    "rgba_colors[:, 0] = 0\n",
    "rgba_colors[:, 3] = 0.01\n",
    "# print (anomaly_indices)\n",
    "for ind in anomaly_indices:\n",
    "    rgba_colors[ind, 0] = 1\n",
    "    rgba_colors[ind, 3] = 0.1\n",
    "    \n",
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.scatter(X_embedded.T[0], X_embedded.T[1], color=rgba_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clusters = max(pre_merging_metrics['clusters'])\n",
    "# labels = pre_merging_metrics['labels']\n",
    "rerun_clusters = []\n",
    "for ind, ad in enumerate(rerun_cluster_idx):\n",
    "    if rerun_clustering_labels[ind] != -1:\n",
    "        original_labels[ad] = rerun_clustering_labels[ind] + max_clusters + 1\n",
    "        rerun_clusters.append(rerun_clustering_labels[ind] + max_clusters + 1)\n",
    "        probs[ad] = rerun_probs[ind]\n",
    "    else:\n",
    "        original_labels[ad] = -1\n",
    "        probs[ad] = rerun_probs[ind]\n",
    "        rerun_clusters.append(-1)\n",
    "rerun_clusters = list(set(rerun_clusters))\n",
    "clusters = rerun_clusters + non_noisy_clusters\n",
    "print (clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonoise['cluster_label'] = original_labels\n",
    "df_nonoise['probabilities'] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_splitting_metrics = get_all_metrics(bigram_matrix, rerun_clusters, original_labels, df_nonoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (max(post_splitting_metrics['custom_score']))\n",
    "print (post_splitting_metrics['clusters'][post_splitting_metrics['custom_score'].index(max(post_splitting_metrics['custom_score']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filt = df_nonoise[df_nonoise['cluster_label'] == 485]\n",
    "names = [x.lower() if type(x) == type('') else None for x in df_filt['Name'].unique()]\n",
    "if math.nan in names:\n",
    "    names.remove(math.nan)\n",
    "if None in names:\n",
    "    names.remove(None)\n",
    "names = list(set(names))\n",
    "print (len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_ratios = []\n",
    "weighted_cluster_density = []\n",
    "unweighted_cluster_density = []\n",
    "unweighted_fraudar_scores = []\n",
    "weighted_fraudar_scores = []\n",
    "unweighted_outer_edge_perc_scores = []\n",
    "weighted_outer_edge_perc_scores = []\n",
    "unweighted_shell_edge_perc_scores = []\n",
    "weighted_shell_edge_perc_scores = []\n",
    "weighted_outer_modularity_scores = []\n",
    "unweighted_outer_modularity_scores = []\n",
    "weighted_shell_modularity_scores = []\n",
    "unweighted_shell_modularity_scores = []\n",
    "pairwise_similarity = []\n",
    "custom_score = []\n",
    "avg_label_scores = []\n",
    "max_label_scores = []\n",
    "sum_label_scores = []\n",
    "avg_binary_scores = []\n",
    "max_binary_scores = []\n",
    "sum_binary_scores = []\n",
    "cluster_counts = []\n",
    "print (len(rerun_clusters))\n",
    "print (len(post_splitting_metrics['eigen_ratios']))\n",
    "for i in clusters:\n",
    "    if i in rerun_clusters:\n",
    "        ind = rerun_clusters.index(i)\n",
    "        eigen_ratios.append(post_splitting_metrics['eigen_ratios'][ind])\n",
    "        weighted_cluster_density.append(post_splitting_metrics['weighted_cluster_density'][ind])\n",
    "        unweighted_cluster_density.append(post_splitting_metrics['unweighted_cluster_density'][ind])\n",
    "        unweighted_fraudar_scores.append(post_splitting_metrics['unweighted_fraudar_scores'][ind])\n",
    "        weighted_fraudar_scores.append(post_splitting_metrics['weighted_fraudar_scores'][ind])\n",
    "        unweighted_outer_edge_perc_scores.append(post_splitting_metrics['unweighted_outer_edge_perc_scores'][ind])\n",
    "        weighted_outer_edge_perc_scores.append(post_splitting_metrics['weighted_outer_edge_perc_scores'][ind])\n",
    "        unweighted_shell_edge_perc_scores.append(post_splitting_metrics['unweighted_shell_edge_perc_scores'][ind])\n",
    "        weighted_shell_edge_perc_scores.append(post_splitting_metrics['weighted_shell_edge_perc_scores'][ind])\n",
    "        weighted_outer_modularity_scores.append(post_splitting_metrics['weighted_outer_modularity_scores'][ind])\n",
    "        unweighted_outer_modularity_scores.append(post_splitting_metrics['unweighted_outer_modularity_scores'][ind])\n",
    "        weighted_shell_modularity_scores.append(post_splitting_metrics['weighted_shell_modularity_scores'][ind])\n",
    "        unweighted_shell_modularity_scores.append(post_splitting_metrics['unweighted_shell_modularity_scores'][ind])\n",
    "        pairwise_similarity.append(post_splitting_metrics['pairwise_similarity'][ind])\n",
    "        custom_score.append(post_splitting_metrics['custom_score'][ind])\n",
    "        avg_label_scores.append(post_splitting_metrics['avg_label_scores'][ind])\n",
    "        max_label_scores.append(post_splitting_metrics['max_label_scores'][ind])\n",
    "        sum_label_scores.append(post_splitting_metrics['sum_label_scores'][ind])\n",
    "        avg_binary_scores.append(post_splitting_metrics['avg_binary_scores'][ind])\n",
    "        max_binary_scores.append(post_splitting_metrics['max_binary_scores'][ind])\n",
    "        sum_binary_scores.append(post_splitting_metrics['sum_binary_scores'][ind])\n",
    "        cluster_counts.append(post_splitting_metrics['cluster_counts'][ind])\n",
    "        \n",
    "    elif i in non_noisy_clusters:\n",
    "        ind = pre_merging_metrics['clusters'].index(i)\n",
    "        eigen_ratios.append(pre_merging_metrics['eigen_ratios'][ind])\n",
    "        weighted_cluster_density.append(pre_merging_metrics['weighted_cluster_density'][ind])\n",
    "        unweighted_cluster_density.append(pre_merging_metrics['unweighted_cluster_density'][ind])\n",
    "        unweighted_fraudar_scores.append(pre_merging_metrics['unweighted_fraudar_scores'][ind])\n",
    "        weighted_fraudar_scores.append(pre_merging_metrics['weighted_fraudar_scores'][ind])\n",
    "        unweighted_outer_edge_perc_scores.append(pre_merging_metrics['unweighted_outer_edge_perc_scores'][ind])\n",
    "        weighted_outer_edge_perc_scores.append(pre_merging_metrics['weighted_outer_edge_perc_scores'][ind])\n",
    "        unweighted_shell_edge_perc_scores.append(pre_merging_metrics['unweighted_shell_edge_perc_scores'][ind])\n",
    "        weighted_shell_edge_perc_scores.append(pre_merging_metrics['weighted_shell_edge_perc_scores'][ind])\n",
    "        weighted_outer_modularity_scores.append(pre_merging_metrics['weighted_outer_modularity_scores'][ind])\n",
    "        unweighted_outer_modularity_scores.append(pre_merging_metrics['unweighted_outer_modularity_scores'][ind])\n",
    "        weighted_shell_modularity_scores.append(pre_merging_metrics['weighted_shell_modularity_scores'][ind])\n",
    "        unweighted_shell_modularity_scores.append(pre_merging_metrics['unweighted_shell_modularity_scores'][ind])\n",
    "        pairwise_similarity.append(pre_merging_metrics['pairwise_similarity'][ind])\n",
    "        custom_score.append(pre_merging_metrics['custom_score'][ind])\n",
    "        avg_label_scores.append(pre_merging_metrics['avg_label_scores'][ind])\n",
    "        max_label_scores.append(pre_merging_metrics['max_label_scores'][ind])\n",
    "        sum_label_scores.append(pre_merging_metrics['sum_label_scores'][ind])\n",
    "        avg_binary_scores.append(pre_merging_metrics['avg_binary_scores'][ind])\n",
    "        max_binary_scores.append(pre_merging_metrics['max_binary_scores'][ind])\n",
    "        sum_binary_scores.append(pre_merging_metrics['sum_binary_scores'][ind])\n",
    "        cluster_counts.append(pre_merging_metrics['cluster_counts'][i])\n",
    "\n",
    "metrics = {}\n",
    "metrics['weighted_cluster_density'] = weighted_cluster_density\n",
    "metrics['unweighted_cluster_density'] = unweighted_cluster_density\n",
    "metrics['weighted_fraudar_scores'] = weighted_fraudar_scores\n",
    "metrics['unweighted_fraudar_scores'] = unweighted_fraudar_scores\n",
    "metrics['weighted_outer_edge_perc_scores'] = weighted_outer_edge_perc_scores\n",
    "metrics['unweighted_outer_edge_perc_scores'] = unweighted_outer_edge_perc_scores\n",
    "metrics['weighted_shell_edge_perc_scores'] = weighted_shell_edge_perc_scores\n",
    "metrics['unweighted_shell_edge_perc_scores'] = unweighted_shell_edge_perc_scores\n",
    "metrics['unweighted_outer_modularity_scores'] = unweighted_outer_modularity_scores\n",
    "metrics['weighted_outer_modularity_scores'] = weighted_outer_modularity_scores\n",
    "metrics['unweighted_shell_modularity_scores'] = unweighted_shell_modularity_scores\n",
    "metrics['weighted_shell_modularity_scores'] = weighted_shell_modularity_scores\n",
    "metrics['pairwise_similarity'] = pairwise_similarity\n",
    "metrics['custom_score'] = custom_score\n",
    "metrics['avg_label_scores'] = avg_label_scores\n",
    "metrics['sum_label_scores'] = sum_label_scores\n",
    "metrics['max_label_scores'] = max_label_scores\n",
    "metrics['avg_binary_scores'] = avg_binary_scores\n",
    "metrics['max_binary_scores'] = max_binary_scores\n",
    "metrics['sum_binary_scores'] = sum_binary_scores\n",
    "metrics['eigen_ratios'] = eigen_ratios\n",
    "metrics['clusters'] = clusters\n",
    "metrics['cluster_label'] = labels.copy()\n",
    "metrics['cluster_counts'] = cluster_counts \n",
    "\n",
    "post_noisy_split_metrics = metrics\n",
    "print (len(post_noisy_split_metrics['eigen_ratios']))\n",
    "print (len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (post_noisy_split_metrics['cluster_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(1,2,1)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "ax1.scatter(pre_merging_metrics['avg_label_scores'], pre_merging_metrics['eigen_ratios'], alpha=0.2)\n",
    "ax1.title.set_text(\"Before Rerun - Avg score vs Eigen Ratios\")\n",
    "ax2.scatter(post_noisy_split_metrics['avg_label_scores'], post_noisy_split_metrics['eigen_ratios'], alpha=0.2)\n",
    "ax2.title.set_text(\"After Rerun - Avg score vs Eigen Ratios\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "############################OUTER EDGE PERCENTAGE#############################\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "ax1.scatter(pre_merging_metrics['avg_label_scores'], pre_merging_metrics['weighted_outer_edge_perc_scores'], alpha=0.2)\n",
    "ax1.title.set_text(\"Before Rerun - Avg score vs Weighted Outer Edge Percentage\")\n",
    "ax2.scatter(post_noisy_split_metrics['avg_label_scores'], post_noisy_split_metrics['weighted_outer_edge_perc_scores'], alpha=0.2)\n",
    "ax2.title.set_text(\"After Rerun - Avg score vs Weighted Outer Edge Percentage\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "ax1.scatter(pre_merging_metrics['avg_label_scores'], pre_merging_metrics['weighted_fraudar_scores'], alpha=0.2)\n",
    "ax1.title.set_text(\"Before Rerun - Avg score vs Weighted Fraudar Score\")\n",
    "ax2.scatter(post_noisy_split_metrics['avg_label_scores'], post_noisy_split_metrics['weighted_fraudar_scores'], alpha=0.2)\n",
    "ax2.title.set_text(\"After Rerun - Avg score vs Weighted Fraudar Score\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "ax1.scatter(pre_merging_metrics['avg_label_scores'], pre_merging_metrics['weighted_outer_modularity_scores'], alpha=0.2)\n",
    "ax1.title.set_text(\"Before Rerun - Avg score vs Weighted outer Modularity\")\n",
    "ax2.scatter(post_noisy_split_metrics['avg_label_scores'], post_noisy_split_metrics['weighted_outer_modularity_scores'], alpha=0.2)\n",
    "ax2.title.set_text(\"After Rerun - Avg score vs Weighted Outer modularity\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cluster_labels_results = df_nonoise['cluster_label']\n",
    "cluster_vectors = np.zeros((len(clusters), bigram_matrix.shape[1]))\n",
    "for ind, cl in enumerate(clusters):\n",
    "    cluster_idx = np.argwhere(cluster_labels_results == cl).reshape(-1)\n",
    "    tf_cluster_mat = bigram_matrix[cluster_idx,:][:]\n",
    "    tf_cluster_mat_flat = np.mean(tf_cluster_mat, axis=0)\n",
    "    print (tf_cluster_mat_flat.shape)\n",
    "    cluster_vectors[ind] = tf_cluster_mat_flat\n",
    "    \n",
    "pairwise_sim_mat = cosine_similarity(cluster_vectors, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_sim_mat *= np.tri(*pairwise_sim_mat.shape)\n",
    "np.fill_diagonal(pairwise_sim_mat, 0.0)\n",
    "\n",
    "sim_clusters = np.where(pairwise_sim_mat>0.8)\n",
    "cluster_tuples = zip(sim_clusters[0], sim_clusters[1])\n",
    "\n",
    "clusters_to_be_merged = []\n",
    "\n",
    "for tup in cluster_tuples:\n",
    "    clusters_to_be_merged.append((clusters[tup[0]], clusters[tup[1]]))\n",
    "print (clusters_to_be_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_clusters = np.array(cluster_labels_results)\n",
    "print (bigram_matrix.shape)\n",
    "for cl in clusters_to_be_merged:\n",
    "    np_clusters = np.array(cluster_labels_results)\n",
    "#     adj_matrix_copy = bigram_matrix.copy()\n",
    "    replace_index = np.where(np_clusters==cl[1])\n",
    "    for i in replace_index:\n",
    "        np_clusters[i] = cl[0]\n",
    "    pre_cl_ind1 = np.argwhere(pre_clusters==cl[0]).reshape(-1)\n",
    "    pre_cl_ind2 = np.argwhere(pre_clusters==cl[1]).reshape(-1)\n",
    "    print (pre_cl_ind1)\n",
    "    pre_shell1, pre_outer1, pre_core1 = get_all_subgraphs(bigram_matrix.copy(), pre_cl_ind1)\n",
    "    pre_shell2, pre_outer2, pre_core2 = get_all_subgraphs(bigram_matrix.copy(), pre_cl_ind2)\n",
    "    total_edges = np.sum(bigram_matrix)\n",
    "    mod1 = calculate_weighted_edge_per_score(pre_core1, pre_outer1)\n",
    "    mod2 = calculate_weighted_edge_per_score(pre_core2, pre_outer2)\n",
    "    print (np.sum(pre_shell1))\n",
    "    post_cl_ind = np.argwhere(np_clusters==cl[0]).reshape(-1)\n",
    "    post_shell, post_outer, post_core = get_all_subgraphs(bigram_matrix.copy(), post_cl_ind)\n",
    "    post_mod = calculate_weighted_edge_per_score(post_core, post_outer)\n",
    "    \n",
    "    print (\"Cluster1: {}, Cluster2: {}, Pre mod1: {}, Pre Mod2: {}, Post Mod: {}\".format(cl[0], cl[1], mod1, mod2, post_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_ratios = post_noisy_split_metrics['eigen_ratios']\n",
    "weighted_cluster_density = post_noisy_split_metrics['weighted_cluster_density']\n",
    "unweighted_cluster_density = post_noisy_split_metrics['unweighted_cluster_density']\n",
    "unweighted_fraudar_scores = post_noisy_split_metrics['unweighted_fraudar_scores']\n",
    "weighted_fraudar_scores = post_noisy_split_metrics['weighted_fraudar_scores']\n",
    "unweighted_outer_edge_perc_scores = post_noisy_split_metrics['unweighted_outer_edge_perc_scores']\n",
    "weighted_outer_edge_perc_scores = post_noisy_split_metrics['weighted_outer_edge_perc_scores']\n",
    "unweighted_shell_edge_perc_scores = post_noisy_split_metrics['unweighted_shell_edge_perc_scores']\n",
    "weighted_shell_edge_perc_scores = post_noisy_split_metrics['weighted_shell_edge_perc_scores']\n",
    "weighted_outer_modularity_scores = post_noisy_split_metrics['weighted_outer_modularity_scores']\n",
    "unweighted_outer_modularity_scores = post_noisy_split_metrics['unweighted_outer_modularity_scores']\n",
    "weighted_shell_modularity_scores = post_noisy_split_metrics['weighted_shell_modularity_scores']\n",
    "unweighted_shell_modularity_scores = post_noisy_split_metrics['unweighted_shell_modularity_scores']\n",
    "pairwise_similarity = post_noisy_split_metrics['pairwise_similarity']\n",
    "custom_score = post_noisy_split_metrics['custom_score']\n",
    "avg_label_scores = post_noisy_split_metrics['avg_label_scores']\n",
    "max_label_scores = post_noisy_split_metrics['max_label_scores']\n",
    "sum_label_scores = post_noisy_split_metrics['sum_label_scores']\n",
    "avg_binary_scores = post_noisy_split_metrics['avg_binary_scores']\n",
    "max_binary_scores = post_noisy_split_metrics['max_binary_scores']\n",
    "sum_binary_scores = post_noisy_split_metrics['sum_binary_scores']\n",
    "cluster_counts = post_noisy_split_metrics['cluster_counts']\n",
    "labels = df_nonoise['cluster_label']\n",
    "df_a = df_nonoise.copy()\n",
    "# labels = df_data['cluster_label']\n",
    "to_calculate_clusters = []\n",
    "for tup in clusters_to_be_merged:\n",
    "    if tup[1] in clusters:\n",
    "        df_nonoise['cluster_label'].replace(tup[1], tup[0], inplace=True)\n",
    "    # Handle case when 3 clusters are similar to each other eg. (a,b) (c,b)\n",
    "#         print (df_a['cluster_label'].unique())\n",
    "\n",
    "        ind = clusters.index(tup[1])\n",
    "        del eigen_ratios[ind]\n",
    "        del weighted_cluster_density[ind]\n",
    "        del unweighted_cluster_density[ind]\n",
    "        del unweighted_fraudar_scores[ind]\n",
    "        del weighted_fraudar_scores[ind]\n",
    "        del unweighted_outer_edge_perc_scores[ind]\n",
    "        del weighted_outer_edge_perc_scores[ind]\n",
    "        del unweighted_shell_edge_perc_scores[ind]\n",
    "        del weighted_shell_edge_perc_scores[ind]\n",
    "        del weighted_outer_modularity_scores[ind]\n",
    "        del unweighted_outer_modularity_scores[ind]\n",
    "        del weighted_shell_modularity_scores[ind]\n",
    "        del unweighted_shell_modularity_scores[ind]\n",
    "        del pairwise_similarity[ind]\n",
    "        del custom_score[ind]\n",
    "        del avg_label_scores[ind]\n",
    "        del max_label_scores[ind]\n",
    "        del sum_label_scores[ind]\n",
    "        del avg_binary_scores[ind]\n",
    "        del max_binary_scores[ind]\n",
    "        del sum_binary_scores[ind]\n",
    "        del cluster_counts[ind]\n",
    "        del clusters[ind]\n",
    "        if tup[1] in to_calculate_clusters:\n",
    "            to_calculate_clusters.remove(tup[1])\n",
    "        to_calculate_clusters.append(tup[0])\n",
    "\n",
    "metrics = get_all_metrics(bigram_matrix, to_calculate_clusters, df_nonoise['cluster_label'], df_nonoise)\n",
    "for i, cl in enumerate(to_calculate_clusters):\n",
    "    ind = clusters.index(cl)\n",
    "    eigen_ratios[ind] = metrics['eigen_ratios'][i]\n",
    "    weighted_cluster_density[ind] = post_noisy_split_metrics['weighted_cluster_density'][i]\n",
    "    unweighted_cluster_density[ind] = post_noisy_split_metrics['unweighted_cluster_density'][i]\n",
    "    unweighted_fraudar_scores[ind] = post_noisy_split_metrics['unweighted_fraudar_scores'][i]\n",
    "    weighted_fraudar_scores[ind] = post_noisy_split_metrics['weighted_fraudar_scores'][i]\n",
    "    unweighted_outer_edge_perc_scores[ind] = post_noisy_split_metrics['unweighted_outer_edge_perc_scores'][i]\n",
    "    weighted_outer_edge_perc_scores[ind] = post_noisy_split_metrics['weighted_outer_edge_perc_scores'][i]\n",
    "    unweighted_shell_edge_perc_scores[ind] = post_noisy_split_metrics['unweighted_shell_edge_perc_scores'][i]\n",
    "    weighted_shell_edge_perc_scores[ind] = post_noisy_split_metrics['weighted_shell_edge_perc_scores'][i]\n",
    "    weighted_outer_modularity_scores[ind] = post_noisy_split_metrics['weighted_outer_modularity_scores'][i]\n",
    "    unweighted_outer_modularity_scores[ind] = post_noisy_split_metrics['unweighted_outer_modularity_scores'][i]\n",
    "    weighted_shell_modularity_scores[ind] = post_noisy_split_metrics['weighted_shell_modularity_scores'][i]\n",
    "    unweighted_shell_modularity_scores[ind] = post_noisy_split_metrics['unweighted_shell_modularity_scores'][i]\n",
    "    pairwise_similarity[ind] = post_noisy_split_metrics['pairwise_similarity'][i]\n",
    "    custom_score[ind] = post_noisy_split_metrics['custom_score'][i]\n",
    "    avg_label_scores[ind] = post_noisy_split_metrics['avg_label_scores'][i]\n",
    "    max_label_scores[ind] = post_noisy_split_metrics['max_label_scores'][i]\n",
    "    sum_label_scores[ind] = post_noisy_split_metrics['sum_label_scores'][i]\n",
    "    avg_binary_scores[ind] = post_noisy_split_metrics['avg_binary_scores'][i]\n",
    "    max_binary_scores[ind] = post_noisy_split_metrics['max_binary_scores'][i]\n",
    "    sum_binary_scores[ind] = post_noisy_split_metrics['sum_binary_scores'][i]\n",
    "    cluster_counts[ind] = post_noisy_split_metrics['cluster_counts'][i]\n",
    "\n",
    "post_merging_metric = {}\n",
    "post_merging_metric['weighted_cluster_density'] = weighted_cluster_density\n",
    "post_merging_metric['unweighted_cluster_density'] = unweighted_cluster_density\n",
    "post_merging_metric['weighted_fraudar_scores'] = weighted_fraudar_scores\n",
    "post_merging_metric['unweighted_fraudar_scores'] = unweighted_fraudar_scores\n",
    "post_merging_metric['weighted_outer_edge_perc_scores'] = weighted_outer_edge_perc_scores\n",
    "post_merging_metric['unweighted_outer_edge_perc_scores'] = unweighted_outer_edge_perc_scores\n",
    "post_merging_metric['weighted_shell_edge_perc_scores'] = weighted_shell_edge_perc_scores\n",
    "post_merging_metric['unweighted_shell_edge_perc_scores'] = unweighted_shell_edge_perc_scores\n",
    "post_merging_metric['unweighted_outer_modularity_scores'] = unweighted_outer_modularity_scores\n",
    "post_merging_metric['weighted_outer_modularity_scores'] = weighted_outer_modularity_scores\n",
    "post_merging_metric['unweighted_shell_modularity_scores'] = unweighted_shell_modularity_scores\n",
    "post_merging_metric['weighted_shell_modularity_scores'] = weighted_shell_modularity_scores\n",
    "post_merging_metric['pairwise_similarity'] = pairwise_similarity\n",
    "post_merging_metric['custom_score'] = custom_score\n",
    "post_merging_metric['avg_label_scores'] = avg_label_scores\n",
    "post_merging_metric['sum_label_scores'] = sum_label_scores\n",
    "post_merging_metric['max_label_scores'] = max_label_scores\n",
    "post_merging_metric['avg_binary_scores'] = avg_binary_scores\n",
    "post_merging_metric['max_binary_scores'] = max_binary_scores\n",
    "post_merging_metric['sum_binary_scores'] = sum_binary_scores\n",
    "post_merging_metric['eigen_ratios'] = eigen_ratios\n",
    "post_merging_metric['clusters'] = clusters\n",
    "post_merging_metric['cluster_label'] = df_nonoise['cluster_label'].copy()\n",
    "post_merging_metric['cluster_counts'] = cluster_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(post_merging_metric['clusters']), df_nonoise['cluster_label'].nunique(), len(post_merging_metric['eigen_ratios']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonoise.set_index('index1', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_a = df_orig.copy()\n",
    "df_nonoise['final_label'] = df_nonoise['cluster_label']\n",
    "df_orig = df_orig.join(df_nonoise['final_label'], how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig['final_label'].isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_orig[df_orig['noise'] == True].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in df_orig.iterrows():\n",
    "    if row['noise'] == True:\n",
    "        df_orig.at[ind, 'final_label'] = -2\n",
    "    elif row['sim_check'] == True:\n",
    "        df_orig.at[ind, 'final_label'] = df_orig.at[row['sim_index'], 'final_label']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in df_orig.iterrows():\n",
    "    if row['noise'] == True:\n",
    "        df_orig.at[ind, 'final_label'] = -2\n",
    "    elif row['sim_check'] == True:\n",
    "        df_orig.at[ind, 'final_label'] = df_orig.at[row['sim_index'], 'final_label']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in clusters:\n",
    "    df_filt = df_orig[df_orig['final_label'] == cl]\n",
    "    names = [x.lower() if type(x) == type('') else None for x in df_filt['Name'].unique()]\n",
    "    if math.nan in names:\n",
    "        names.remove(math.nan)\n",
    "    if None in names:\n",
    "        names.remove(None)\n",
    "    names = list(set(names))\n",
    "    print (cl, len(names))\n",
    "# print (post_merging_metric['cluster_counts'][post_merging_metric['custom_score'].index(max(post_merging_metric['custom_score']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (post_merging_metric['clusters'][post_merging_metric['custom_score'].index(max(post_merging_metric['custom_score']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = df_orig[df_orig['final_label'] == 375]\n",
    "\n",
    "names = [x.lower() if type(x) == type('') else None for x in df_filt['Name'].unique()]\n",
    "if math.nan in names:\n",
    "    names.remove(math.nan)\n",
    "if None in names:\n",
    "    names.remove(None)\n",
    "names = list(set(names))\n",
    "print (names)\n",
    "for ind, row in df_filt.iterrows():\n",
    "    print ('------------------------------------------------------------')\n",
    "    print (row['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_score  = post_merging_metric['custom_score']\n",
    "# multiplier = (max(custom_score) - min(custom_score))\n",
    "# custom_score = list(map(lambda x: x/multiplier, custom_score))\n",
    "# # custom_score = list(map(lambda x: x*2, custom_score))\n",
    "# post_merging_metric['c_score'] = custom_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "text_similarity = post_merging_metric['pairwise_similarity']\n",
    "clusters = post_merging_metric['clusters']\n",
    "avg_label_scores = post_merging_metric['avg_label_scores']\n",
    "avg_binary_scores = post_merging_metric['avg_binary_scores']\n",
    "w_density = post_merging_metric['weighted_cluster_density']\n",
    "uw_density = post_merging_metric['unweighted_cluster_density']\n",
    "w_fraudar = post_merging_metric['weighted_fraudar_scores']\n",
    "uw_fraudar = post_merging_metric['unweighted_fraudar_scores']\n",
    "w_outer_edge = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "uw_outer_edge = post_merging_metric['unweighted_outer_edge_perc_scores']\n",
    "w_shell_edge = post_merging_metric['weighted_shell_edge_perc_scores']\n",
    "uw_shell_edge = post_merging_metric['unweighted_shell_edge_perc_scores']\n",
    "w_outer_mod = post_merging_metric['weighted_outer_modularity_scores']\n",
    "uw_outer_mod = post_merging_metric['unweighted_outer_modularity_scores']\n",
    "w_shell_mod = post_merging_metric['weighted_shell_modularity_scores']\n",
    "uw_shell_mod = post_merging_metric['unweighted_shell_modularity_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (suspicious_scores[0], clusters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filt = df_orig[df_orig['cluster_label'] == 322]\n",
    "\n",
    "# names = [x.lower() if type(x) == type('') else None for x in df_filt['Name'].unique()]\n",
    "# if math.nan in names:\n",
    "#     names.remove(math.nan)\n",
    "# if None in names:\n",
    "#     names.remove(None)\n",
    "# names = list(set(names))\n",
    "# print (names)\n",
    "# for ind, row in df_filt.iterrows():\n",
    "#     print ('------------------------------------------------------------')\n",
    "#     print (row['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=avg_label_scores, y=suspicious_scores, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='anomaly score')\n",
    "plt.show()\n",
    "\n",
    "ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "ax = sns.regplot(x=text_similarity, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Text Similarity', ylabel='Weighted outer edge percentage')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=avg_label_scores, y=text_similarity, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Text Similarity')\n",
    "plt.show()\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.scatter(avg_label_scores, w_outer_edge, alpha=0.2)\n",
    "# plt.title('Avg label score vs weighted outer edge percentage')\n",
    "# plt.xlabel('avg label score')\n",
    "# plt.ylabel('weighted outer edge percentage')\n",
    "# # w = np.linalg.lstsq(avg_label_scores, w_outer_edge)[0]\n",
    "# # yh = np.dot(avg_label_scores,w)\n",
    "# # plt.plot(avg_label_scores, yh, 'r-')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(avg_label_scores, w_shell_edge, alpha=0.2)\n",
    "# plt.title('Avg label score vs weighted shell edge percentage')\n",
    "# plt.xlabel('avg label score')\n",
    "# plt.ylabel('weighted shell edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(text_similarity, w_outer_edge, alpha=0.2)\n",
    "# plt.title('Avg text similarity vs weighted outer edge percentage')\n",
    "# plt.xlabel('avg text similarity')\n",
    "# plt.ylabel('weighted outer edge percentage')\n",
    "\n",
    "# # plt.scatter(avg_label_scores, w_shell_edge, alpha=0.2)\n",
    "# # plt.title('Avg label score vs weighted shell edge percentage')\n",
    "# # plt.xlabel('avg label score')\n",
    "# # plt.ylabel('weighted shell edge percentage')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_name(clusters, df_data):\n",
    "    filtered_clusters = []\n",
    "    for cl in clusters:\n",
    "        df_fil = df_data[df_data['cluster_label']==cl]\n",
    "        names = [x.lower() if type(x) == type('') else None for x in df_fil['Name'].unique()]\n",
    "        if math.nan in names:\n",
    "            names.remove(math.nan)\n",
    "        if None in names:\n",
    "            names.remove(None)\n",
    "        names = list(set(names))\n",
    "        if len(names)>1:\n",
    "#             print (names)\n",
    "            filtered_clusters.append(cl)\n",
    "#             filtered_cluster_metric.append(anomaly_cluster_metric[ind])\n",
    "#         else:\n",
    "#             filtered_clusters.append(c)\n",
    "#             filtered_cluster_metric.append(anomaly_cluster_metric[ind])\n",
    "    return filtered_clusters\n",
    "\n",
    "def filter_by_threshold(clusters, metric, threshold=0.0):\n",
    "    filtered_clusters = []\n",
    "    index_list = [i for i, e in enumerate(metric) if e > threshold]\n",
    "    filtered_clusters = [clusters[i] for i in index_list] \n",
    "    \n",
    "    return filtered_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suspicious_scores = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "# text_similarity = post_merging_metric['pairwise_similarity']\n",
    "# clusters = post_merging_metric['clusters']\n",
    "# avg_label_scores = post_merging_metric['avg_label_scores']\n",
    "# avg_binary_scores = post_merging_metric['avg_binary_scores']\n",
    "# w_density = post_merging_metric['weighted_cluster_density']\n",
    "# uw_density = post_merging_metric['unweighted_cluster_density']\n",
    "# w_fraudar = post_merging_metric['weighted_fraudar_scores']\n",
    "# uw_fraudar = post_merging_metric['unweighted_fraudar_scores']\n",
    "# w_outer_edge = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "# uw_outer_edge = post_merging_metric['unweighted_outer_edge_perc_scores']\n",
    "# w_shell_edge = post_merging_metric['weighted_shell_edge_perc_scores']\n",
    "# uw_shell_edge = post_merging_metric['unweighted_shell_edge_perc_scores']\n",
    "# w_outer_mod = post_merging_metric['weighted_outer_modularity_scores']\n",
    "# uw_outer_mod = post_merging_metric['unweighted_outer_modularity_scores']\n",
    "# w_shell_mod = post_merging_metric['weighted_shell_modularity_scores']\n",
    "# uw_shell_mod = post_merging_metric['unweighted_shell_modularity_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_clusters = filter_by_name(clusters, df_orig.copy())\n",
    "print (filtered_clusters)\n",
    "filtered_clusters = set(filtered_clusters)\n",
    "index_list = [i for i, e in enumerate(clusters) if e in filtered_clusters]\n",
    "filtered_avg_label_scores = [avg_label_scores[i] for i in index_list] \n",
    "filtered_text_similarity = [text_similarity[i] for i in index_list] \n",
    "filtered_suspicious_scores = [suspicious_scores[i] for i in index_list] \n",
    "filtered_avg_binary_scores = [avg_binary_scores[i] for i in index_list] \n",
    "filtered_w_density = [w_density[i] for i in index_list] \n",
    "filtered_uw_density = [uw_density[i] for i in index_list] \n",
    "filtered_w_fraudar = [w_fraudar[i] for i in index_list] \n",
    "filtered_uw_fraudar = [uw_fraudar[i] for i in index_list] \n",
    "filtered_w_outer_edge = [w_outer_edge[i] for i in index_list] \n",
    "filtered_uw_outer_edge = [uw_outer_edge[i] for i in index_list] \n",
    "filtered_w_shell_edge = [w_shell_edge[i] for i in index_list] \n",
    "filtered_uw_shell_edge = [uw_shell_edge[i] for i in index_list] \n",
    "filtered_w_outer_mod = [w_outer_mod[i] for i in index_list] \n",
    "filtered_uw_outer_mod = [uw_outer_mod[i] for i in index_list] \n",
    "filtered_w_shell_mod = [w_shell_mod[i] for i in index_list] \n",
    "filtered_uw_shell_mod = [uw_shell_mod[i] for i in index_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import plotly.express as px\n",
    "\n",
    "filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \\\n",
    "filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, filtered_w_outer_edge, \\\n",
    "filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  filtered_w_outer_mod, \\\n",
    "filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod = \\\n",
    "zip(*sorted(zip(filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \n",
    "                filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, \n",
    "                filtered_w_outer_edge, filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  \n",
    "                filtered_w_outer_mod, filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod)))\n",
    "filtered_suspicious_scores = list(reversed(filtered_suspicious_scores))\n",
    "filtered_clusters = list(reversed(filtered_clusters))\n",
    "filtered_avg_label_scores = list(reversed(filtered_avg_label_scores))\n",
    "filtered_text_similarity = list(reversed(filtered_text_similarity))\n",
    "filtered_avg_binary_scores = list(reversed(filtered_avg_binary_scores))\n",
    "filtered_w_density = list(reversed(filtered_w_density))\n",
    "filtered_uw_density = list(reversed(filtered_uw_density))\n",
    "filtered_w_fraudar = list(reversed(filtered_w_fraudar))  \n",
    "filtered_uw_fraudar = list(reversed(filtered_uw_fraudar))\n",
    "filtered_w_outer_edge = list(reversed(filtered_w_outer_edge)) \n",
    "filtered_uw_outer_edge = list(reversed(filtered_w_density)) \n",
    "filtered_w_shell_edge = list(reversed(filtered_w_shell_edge))\n",
    "filtered_uw_shell_edge = list(reversed(filtered_uw_shell_edge))  \n",
    "filtered_w_outer_mod = list(reversed(filtered_w_outer_mod)) \n",
    "filtered_uw_outer_mod = list(reversed(filtered_uw_outer_mod)) \n",
    "filtered_w_shell_mod = list(reversed(filtered_w_shell_mod)) \n",
    "filtered_uw_shell_mod = list(reversed(filtered_uw_shell_mod))\n",
    "top_k = [10, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "rcParams['figure.figsize'] = 4,4\n",
    "for k in top_k:\n",
    "    print ('==================================k = {}========================================'.format(k) )\n",
    "    plt.hist(filtered_avg_label_scores[:k], bins=7)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(filtered_avg_binary_scores[:k], bins=3)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(filtered_text_similarity[:k], marker='o', linestyle='--')\n",
    "    plt.xlabel(' Clusters')\n",
    "    plt.ylabel('avg text similarity')\n",
    "    plt.show()\n",
    "    \n",
    "    print (\"Average of average label scores : {}\".format(sum(filtered_avg_label_scores[:k])/k))\n",
    "    print (\"Average text similarity : {}\".format(sum(filtered_text_similarity[:k])/k))\n",
    "    print ('\\n')\n",
    "    eig_ts_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_text_similarity[:k])[0]\n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print (\"Spearman Correlation | Average Label Score |          Text Similarity          | {0:.2f} \".format(eig_ts_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "    \n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print ('\\n')\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "print (len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10,10\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_suspicious_scores, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='anomaly score')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_w_outer_edge, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "ax = sns.regplot(x=filtered_text_similarity, y=filtered_w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Text Similarity', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_text_similarity, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Text Similarity')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_clusters = filter_by_threshold(filtered_clusters, filtered_suspicious_scores, threshold=0.00000001)\n",
    "print (filtered_clusters)\n",
    "f2_clusters = set(f2_clusters)\n",
    "index_list = [i for i, e in enumerate(filtered_clusters) if e in f2_clusters]\n",
    "filtered_avg_label_scores = [filtered_avg_label_scores[i] for i in index_list] \n",
    "filtered_text_similarity = [filtered_text_similarity[i] for i in index_list] \n",
    "filtered_suspicious_scores = [filtered_suspicious_scores[i] for i in index_list] \n",
    "filtered_avg_binary_scores = [filtered_avg_binary_scores[i] for i in index_list] \n",
    "filtered_w_density = [filtered_w_density[i] for i in index_list] \n",
    "filtered_uw_density = [filtered_uw_density[i] for i in index_list] \n",
    "filtered_w_fraudar = [filtered_w_fraudar[i] for i in index_list] \n",
    "filtered_uw_fraudar = [filtered_uw_fraudar[i] for i in index_list] \n",
    "filtered_w_outer_edge = [filtered_w_outer_edge[i] for i in index_list] \n",
    "filtered_uw_outer_edge = [filtered_uw_outer_edge[i] for i in index_list] \n",
    "filtered_w_shell_edge = [filtered_w_shell_edge[i] for i in index_list] \n",
    "filtered_uw_shell_edge = [filtered_uw_shell_edge[i] for i in index_list] \n",
    "filtered_w_outer_mod = [filtered_w_outer_mod[i] for i in index_list] \n",
    "filtered_uw_outer_mod = [filtered_uw_outer_mod[i] for i in index_list] \n",
    "filtered_w_shell_mod = [filtered_w_shell_mod[i] for i in index_list] \n",
    "filtered_uw_shell_mod = [filtered_uw_shell_mod[i] for i in index_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import plotly.express as px\n",
    "\n",
    "filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \\\n",
    "filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, filtered_w_outer_edge, \\\n",
    "filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  filtered_w_outer_mod, \\\n",
    "filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod = \\\n",
    "zip(*sorted(zip(filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \n",
    "                filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, \n",
    "                filtered_w_outer_edge, filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  \n",
    "                filtered_w_outer_mod, filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod)))\n",
    "filtered_suspicious_scores = list(reversed(filtered_suspicious_scores))\n",
    "filtered_clusters = list(reversed(filtered_clusters))\n",
    "filtered_avg_label_scores = list(reversed(filtered_avg_label_scores))\n",
    "filtered_text_similarity = list(reversed(filtered_text_similarity))\n",
    "filtered_avg_binary_scores = list(reversed(filtered_avg_binary_scores))\n",
    "filtered_w_density = list(reversed(filtered_w_density))\n",
    "filtered_uw_density = list(reversed(filtered_uw_density))\n",
    "filtered_w_fraudar = list(reversed(filtered_w_fraudar))  \n",
    "filtered_uw_fraudar = list(reversed(filtered_uw_fraudar))\n",
    "filtered_w_outer_edge = list(reversed(filtered_w_outer_edge)) \n",
    "filtered_uw_outer_edge = list(reversed(filtered_w_density)) \n",
    "filtered_w_shell_edge = list(reversed(filtered_w_shell_edge))\n",
    "filtered_uw_shell_edge = list(reversed(filtered_uw_shell_edge))  \n",
    "filtered_w_outer_mod = list(reversed(filtered_w_outer_mod)) \n",
    "filtered_uw_outer_mod = list(reversed(filtered_uw_outer_mod)) \n",
    "filtered_w_shell_mod = list(reversed(filtered_w_shell_mod)) \n",
    "filtered_uw_shell_mod = list(reversed(filtered_uw_shell_mod))\n",
    "top_k = [10, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "rcParams['figure.figsize'] = 4,4\n",
    "for k in top_k:\n",
    "    print ('==================================k = {}========================================'.format(k) )\n",
    "    plt.hist(filtered_avg_label_scores[:k], bins=7)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(filtered_avg_binary_scores[:k], bins=3)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(filtered_text_similarity[:k], marker='o', linestyle='--')\n",
    "    plt.xlabel(' Clusters')\n",
    "    plt.ylabel('avg text similarity')\n",
    "    plt.show()\n",
    "    \n",
    "    print (\"Average of average label scores : {}\".format(sum(filtered_avg_label_scores[:k])/k))\n",
    "    print (\"Average text similarity : {}\".format(sum(filtered_text_similarity[:k])/k))\n",
    "    print ('\\n')\n",
    "    eig_ts_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_text_similarity[:k])[0]\n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print (\"Spearman Correlation | Average Label Score |          Text Similarity          | {0:.2f} \".format(eig_ts_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "    \n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print ('\\n')\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "print (len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_ts_pscore = spearmanr(filtered_avg_label_scores[:10], filtered_suspicious_scores[:10])[0]\n",
    "print (eig_ts_pscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rcParams['figure.figsize'] = 10,10\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_suspicious_scores, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='anomaly score')\n",
    "plt.title('After filtering out clusters with only one individual and with score = 0')\n",
    "plt.show()\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_w_outer_edge, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual and above threshold-0.5')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "ax = sns.regplot(x=filtered_text_similarity, y=filtered_w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Text Similarity', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual and above threshold-0.5')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_text_similarity, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Text Similarity')\n",
    "plt.title('After filtering out clusters with only one individual and above threshold-0.5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "text_similarity = post_merging_metric['pairwise_similarity']\n",
    "clusters = post_merging_metric['clusters']\n",
    "avg_label_scores = post_merging_metric['avg_label_scores']\n",
    "avg_binary_scores = post_merging_metric['avg_binary_scores']\n",
    "w_density = post_merging_metric['weighted_cluster_density']\n",
    "uw_density = post_merging_metric['unweighted_cluster_density']\n",
    "w_fraudar = post_merging_metric['weighted_fraudar_scores']\n",
    "uw_fraudar = post_merging_metric['unweighted_fraudar_scores']\n",
    "w_outer_edge = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "uw_outer_edge = post_merging_metric['unweighted_outer_edge_perc_scores']\n",
    "w_shell_edge = post_merging_metric['weighted_shell_edge_perc_scores']\n",
    "uw_shell_edge = post_merging_metric['unweighted_shell_edge_perc_scores']\n",
    "w_outer_mod = post_merging_metric['weighted_outer_modularity_scores']\n",
    "uw_outer_mod = post_merging_metric['unweighted_outer_modularity_scores']\n",
    "w_shell_mod = post_merging_metric['weighted_shell_modularity_scores']\n",
    "uw_shell_mod = post_merging_metric['unweighted_shell_modularity_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_clusters = filter_by_name(clusters, df_orig.copy())\n",
    "print (filtered_clusters)\n",
    "filtered_clusters = set(filtered_clusters)\n",
    "index_list = [i for i, e in enumerate(clusters) if e in filtered_clusters]\n",
    "filtered_avg_label_scores = [avg_label_scores[i] for i in index_list] \n",
    "filtered_text_similarity = [text_similarity[i] for i in index_list] \n",
    "filtered_suspicious_scores = [suspicious_scores[i] for i in index_list] \n",
    "filtered_avg_binary_scores = [avg_binary_scores[i] for i in index_list] \n",
    "filtered_w_density = [w_density[i] for i in index_list] \n",
    "filtered_uw_density = [uw_density[i] for i in index_list] \n",
    "filtered_w_fraudar = [w_fraudar[i] for i in index_list] \n",
    "filtered_uw_fraudar = [uw_fraudar[i] for i in index_list] \n",
    "filtered_w_outer_edge = [w_outer_edge[i] for i in index_list] \n",
    "filtered_uw_outer_edge = [uw_outer_edge[i] for i in index_list] \n",
    "filtered_w_shell_edge = [w_shell_edge[i] for i in index_list] \n",
    "filtered_uw_shell_edge = [uw_shell_edge[i] for i in index_list] \n",
    "filtered_w_outer_mod = [w_outer_mod[i] for i in index_list] \n",
    "filtered_uw_outer_mod = [uw_outer_mod[i] for i in index_list] \n",
    "filtered_w_shell_mod = [w_shell_mod[i] for i in index_list] \n",
    "filtered_uw_shell_mod = [uw_shell_mod[i] for i in index_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (filtered_suspicious_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import plotly.express as px\n",
    "\n",
    "filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \\\n",
    "filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, filtered_w_outer_edge, \\\n",
    "filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  filtered_w_outer_mod, \\\n",
    "filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod = \\\n",
    "zip(*sorted(zip(filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \n",
    "                filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, \n",
    "                filtered_w_outer_edge, filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  \n",
    "                filtered_w_outer_mod, filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod)))\n",
    "filtered_suspicious_scores = list(reversed(filtered_suspicious_scores))\n",
    "filtered_clusters = list(reversed(filtered_clusters))\n",
    "filtered_avg_label_scores = list(reversed(filtered_avg_label_scores))\n",
    "filtered_text_similarity = list(reversed(filtered_text_similarity))\n",
    "filtered_avg_binary_scores = list(reversed(filtered_avg_binary_scores))\n",
    "filtered_w_density = list(reversed(filtered_w_density))\n",
    "filtered_uw_density = list(reversed(filtered_uw_density))\n",
    "filtered_w_fraudar = list(reversed(filtered_w_fraudar))  \n",
    "filtered_uw_fraudar = list(reversed(filtered_uw_fraudar))\n",
    "filtered_w_outer_edge = list(reversed(filtered_w_outer_edge)) \n",
    "filtered_uw_outer_edge = list(reversed(filtered_w_density)) \n",
    "filtered_w_shell_edge = list(reversed(filtered_w_shell_edge))\n",
    "filtered_uw_shell_edge = list(reversed(filtered_uw_shell_edge))  \n",
    "filtered_w_outer_mod = list(reversed(filtered_w_outer_mod)) \n",
    "filtered_uw_outer_mod = list(reversed(filtered_uw_outer_mod)) \n",
    "filtered_w_shell_mod = list(reversed(filtered_w_shell_mod)) \n",
    "filtered_uw_shell_mod = list(reversed(filtered_uw_shell_mod))\n",
    "top_k = [10, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "rcParams['figure.figsize'] = 4,4\n",
    "for k in top_k:\n",
    "    print ('==================================k = {}========================================'.format(k) )\n",
    "    plt.hist(filtered_avg_label_scores[:k], bins=7)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(filtered_avg_binary_scores[:k], bins=3)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(filtered_text_similarity[:k], marker='o', linestyle='--')\n",
    "    plt.xlabel(' Clusters')\n",
    "    plt.ylabel('avg text similarity')\n",
    "    plt.show()\n",
    "    \n",
    "    print (\"Average of average label scores : {}\".format(sum(filtered_avg_label_scores[:k])/k))\n",
    "    print (\"Average text similarity : {}\".format(sum(filtered_text_similarity[:k])/k))\n",
    "    print ('\\n')\n",
    "    eig_ts_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_text_similarity[:k])[0]\n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print (\"Spearman Correlation | Average Label Score |          Text Similarity          | {0:.2f} \".format(eig_ts_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "    \n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print ('\\n')\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "print (len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_w_outer_edge, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "ax = sns.regplot(x=filtered_text_similarity, y=filtered_w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Text Similarity', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_text_similarity, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Text Similarity')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "text_similarity = post_merging_metric['pairwise_similarity']\n",
    "clusters = post_merging_metric['clusters']\n",
    "avg_label_scores = post_merging_metric['avg_label_scores']\n",
    "avg_binary_scores = post_merging_metric['avg_binary_scores']\n",
    "w_density = post_merging_metric['weighted_cluster_density']\n",
    "uw_density = post_merging_metric['unweighted_cluster_density']\n",
    "w_fraudar = post_merging_metric['weighted_fraudar_scores']\n",
    "uw_fraudar = post_merging_metric['unweighted_fraudar_scores']\n",
    "w_outer_edge = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "uw_outer_edge = post_merging_metric['unweighted_outer_edge_perc_scores']\n",
    "w_shell_edge = post_merging_metric['weighted_shell_edge_perc_scores']\n",
    "uw_shell_edge = post_merging_metric['unweighted_shell_edge_perc_scores']\n",
    "w_outer_mod = post_merging_metric['weighted_outer_modularity_scores']\n",
    "uw_outer_mod = post_merging_metric['unweighted_outer_modularity_scores']\n",
    "w_shell_mod = post_merging_metric['weighted_shell_modularity_scores']\n",
    "uw_shell_mod = post_merging_metric['unweighted_shell_modularity_scores']\n",
    "cluster_counts = post_merging_metric['cluster_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores, clusters, avg_label_scores, text_similarity, \\\n",
    "w_density, uw_density, w_fraudar,  uw_fraudar, w_outer_edge, \\\n",
    "uw_outer_edge, w_shell_edge, uw_shell_edge,  w_outer_mod, \\\n",
    "uw_outer_mod, w_shell_mod, uw_shell_mod, cluster_counts = \\\n",
    "zip(*sorted(zip(suspicious_scores, clusters, avg_label_scores, text_similarity, \n",
    "                w_density, uw_density, w_fraudar,  uw_fraudar, w_outer_edge, \n",
    "                uw_outer_edge, w_shell_edge, uw_shell_edge,  w_outer_mod, \n",
    "                uw_outer_mod, w_shell_mod, uw_shell_mod, cluster_counts)))\n",
    "suspicious_scores = list(reversed(suspicious_scores))\n",
    "clusters = list(reversed(clusters))\n",
    "avg_label_scores = list(reversed(avg_label_scores))\n",
    "text_similarity = list(reversed(text_similarity))\n",
    "# filtered_avg_binary_scores = list(reversed(filtered_avg_binary_scores))\n",
    "w_density = list(reversed(w_density))\n",
    "uw_density = list(reversed(uw_density))\n",
    "w_fraudar = list(reversed(w_fraudar))  \n",
    "uw_fraudar = list(reversed(uw_fraudar))\n",
    "w_outer_edge = list(reversed(w_outer_edge)) \n",
    "uw_outer_edge = list(reversed(uw_outer_edge)) \n",
    "w_shell_edge = list(reversed(w_shell_edge))\n",
    "uw_shell_edge = list(reversed(uw_shell_edge))  \n",
    "w_outer_mod = list(reversed(w_outer_mod)) \n",
    "uw_outer_mod = list(reversed(uw_outer_mod)) \n",
    "w_shell_mod = list(reversed(w_shell_mod)) \n",
    "uw_shell_mod = list(reversed(uw_shell_mod))\n",
    "cluster_counts = list(reversed(cluster_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = list(df_orig['content_p'])\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2, 3), norm='l2', \n",
    "    smooth_idf=True, stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "\n",
    "print (bigram_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data['cluster_label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_index = clusters.index(-1)\n",
    "del suspicious_scores[noise_index]\n",
    "del text_similarity[noise_index]\n",
    "del clusters[noise_index]\n",
    "del avg_label_scores[noise_index]\n",
    "del avg_binary_scores[noise_index]\n",
    "del w_density[noise_index]\n",
    "del uw_density[noise_index]\n",
    "del w_fraudar[noise_index]\n",
    "del uw_fraudar[noise_index]\n",
    "del w_outer_edge[noise_index]\n",
    "del uw_outer_edge[noise_index]\n",
    "del w_shell_edge[noise_index]\n",
    "del uw_shell_edge[noise_index]\n",
    "del w_outer_mod[noise_index]\n",
    "del uw_outer_mod[noise_index]\n",
    "del w_shell_mod[noise_index]\n",
    "del uw_shell_mod[noise_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = range(len(df_orig))\n",
    "df_orig = df_orig.reindex(new_index)\n",
    "# print (df_da.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "sort_ind = []\n",
    "for ind, cl in enumerate(clusters[:k]):\n",
    "    if cl==-1 or cl==-2:\n",
    "        continue\n",
    "    else:\n",
    "        df_f = df_orig[df_orig['final_label'] == cl]\n",
    "        print (df_f.shape)\n",
    "#         if df_f.shape[0] < 20:\n",
    "#             continue\n",
    "        sort_ind += list(df_f.index)\n",
    "print (sort_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sim_matrix = cosine_similarity(bigram_matrix)\n",
    "print (sim_matrix.shape)\n",
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.tick_params(\n",
    "#     axis='both',          # changes apply to the x-axis\n",
    "#     which='both',      # both major and minor ticks are affected\n",
    "#     bottom=False,      # ticks along the bottom edge are off\n",
    "#     top=False,         # ticks along the top edge are off\n",
    "#     labelbottom=False,\n",
    "# right='off', left='off', labelleft='off')\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "ax = sns.heatmap(sim_matrix[sort_ind, :][:,sort_ind], cmap=cmap, xticklabels=False, yticklabels=False, cbar=False)\n",
    "# ax.set(xlabel='cosine similarity', ylabel='cosine_similarity')\n",
    "plt.savefig('../results/sim_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "total_ads_in_clusters = sum(cluster_counts)\n",
    "avg_of_avg_label_scores = sum(avg_label_scores[:k])/k\n",
    "avg_of_anomaly_scores = sum(suspicious_scores[:k])/k\n",
    "avg_text_similarity = sum(text_similarity[:k])/k\n",
    "avg_cluster_size = sum(cluster_counts[:k])/k\n",
    "\n",
    "print (\"Total ads in clusters: {}, Average of label scores per cluster: {}, \\\n",
    "average of anomaly scores per cluster: {}, Average text sim: {}, avg cluster size: {}\".format(\n",
    "total_ads_in_clusters, avg_of_avg_label_scores, avg_of_anomaly_scores, avg_text_similarity, avg_cluster_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "class1_clusters = []\n",
    "class2_clusters = []\n",
    "class3_clusters = []\n",
    "clusters = post_merging_metric['clusters']\n",
    "scores = post_merging_metric['avg_label_scores']\n",
    "# print (scores)\n",
    "df_plot = df_orig[df_orig['sim_check'] == False]\n",
    "for ind, cl in enumerate(clusters[:458]):\n",
    "    if cl == -1:\n",
    "        continue\n",
    "    elif scores[ind] > 3.5:\n",
    "        class1_clusters.append(cl)\n",
    "    elif scores[ind] <=3.5 and scores[ind]> 2:\n",
    "        class2_clusters.append(cl)\n",
    "    else:\n",
    "        class3_clusters.append(cl)\n",
    "\n",
    "purity = np.zeros((3,3))\n",
    "classes = [{'clusters' : class1_clusters,'purity':[]}, \n",
    "           {'clusters' : class2_clusters,'purity':[]}, \n",
    "           {'clusters' : class3_clusters,'purity':[]}]\n",
    "for ind, cls in enumerate(classes):\n",
    "    t_p = [0,0,0]\n",
    "    cls_clusters = cls['clusters']\n",
    "    for cl in cls_clusters:\n",
    "        df_f = df_plot[df_plot['final_label']==cl]\n",
    "        p = [len(df_f[df_f['label'] < 3]),\n",
    "                 len(df_f[df_f['label']==3]),\n",
    "                 len(df_f[df_f['label']>3]),\n",
    "                 len(df_f[df_f['label']==3])]\n",
    "        \n",
    "        t_p = [sum(x) for x in zip(t_p, p)]\n",
    "#     print (t_p)\n",
    "    purity[ind] = np.array(t_p)\n",
    "    cls['purity'] = t_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 6,6\n",
    "xaxis = range(3)\n",
    "print (xaxis)\n",
    "colors = ['Non-trafficking', 'Unsure', 'Trafficking']\n",
    "# width = 0.35  \n",
    "p0 = plt.bar(xaxis, purity.T[0], color='green', alpha=0.5, edgecolor='black', width=0.4)\n",
    "# p1 = plt.bar(xaxis, purity.T[1], width, bottom=purity.T[0], color='green', alpha=0.5 )\n",
    "# gap_1 = [sum(x) for x in zip(purity.T[0], purity.T[1])]\n",
    "p2 = plt.bar(xaxis, purity.T[1], bottom=purity.T[0], color='grey', alpha=0.5, edgecolor='black', width=0.4)\n",
    "gap_2 = [sum(x) for x in zip(purity.T[0], purity.T[1])]\n",
    "p3 = plt.bar(xaxis, purity.T[2], bottom=gap_2, color='red', alpha=0.5, edgecolor='black', width=0.4)\n",
    "# gap_3 = [sum(x) for x in zip(gap_2, purity.T[3])]\n",
    "# p4 = plt.bar(xaxis, purity.T[4], width, bottom=gap_3, color='red', alpha=0.5, edgecolor='black')\n",
    "# gap_4 = [sum(x) for x in zip(gap_3, purity.T[4])]\n",
    "# p5 = plt.bar(xaxis, purity.T[5], width,bottom=gap_4, color='red', alpha=0.5, edgecolor='black')\n",
    "# gap_5 = [sum(x) for x in zip(gap_4, purity.T[5])]\n",
    "# p6 = plt.bar(xaxis, purity.T[6], width,bottom=gap_5,color='red', alpha=0.5, edgecolor='black')\n",
    "plt.legend(colors,loc=2, fontsize='large')\n",
    "plt.xticks(range(3), ['Corroboration', 'Scooping', 'New Attack'], rotation=0, fontsize='xx-large')\n",
    "plt.ylabel('Escort Ads', fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(class1_clusters), len(class2_clusters), len(class3_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = range(3)\n",
    "# width = 0.35  \n",
    "p0 = plt.bar(xaxis, purity.T[0], width, color='green', alpha=0.6)\n",
    "p1 = plt.bar(xaxis, purity.T[1], width, bottom=purity.T[0], color='green', alpha=0.5 )\n",
    "gap_1 = [sum(x) for x in zip(purity.T[0], purity.T[1])]\n",
    "p2 = plt.bar(xaxis, purity.T[2], width, bottom=gap_1, color='green', alpha=0.4)\n",
    "gap_2 = [sum(x) for x in zip(gap_1, purity.T[2])]\n",
    "p3 = plt.bar(xaxis, purity.T[3], width, bottom=gap_2, color='grey', alpha=0.4 )\n",
    "gap_3 = [sum(x) for x in zip(gap_2, purity.T[3])]\n",
    "p4 = plt.bar(xaxis, purity.T[4], width, bottom=gap_3, color='red', alpha=0.5 )\n",
    "gap_4 = [sum(x) for x in zip(gap_3, purity.T[4])]\n",
    "p5 = plt.bar(xaxis, purity.T[5], width,bottom=gap_4, color='red', alpha=0.6 )\n",
    "gap_5 = [sum(x) for x in zip(gap_4, purity.T[5])]\n",
    "p6 = plt.bar(xaxis, purity.T[6], width,bottom=gap_5,color='red', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores = pre_merging_metrics['weighted_outer_edge_perc_scores']\n",
    "text_similarity = pre_merging_metrics['pairwise_similarity']\n",
    "clusters = pre_merging_metrics['clusters']\n",
    "avg_label_scores = pre_merging_metrics['avg_label_scores']\n",
    "avg_binary_scores = pre_merging_metrics['avg_binary_scores']\n",
    "w_density = pre_merging_metrics['weighted_cluster_density']\n",
    "uw_density = pre_merging_metrics['unweighted_cluster_density']\n",
    "w_fraudar = pre_merging_metrics['weighted_fraudar_scores']\n",
    "uw_fraudar = pre_merging_metrics['unweighted_fraudar_scores']\n",
    "w_outer_edge = pre_merging_metrics['weighted_outer_edge_perc_scores']\n",
    "uw_outer_edge = pre_merging_metrics['unweighted_outer_edge_perc_scores']\n",
    "w_shell_edge = pre_merging_metrics['weighted_shell_edge_perc_scores']\n",
    "uw_shell_edge = pre_merging_metrics['unweighted_shell_edge_perc_scores']\n",
    "w_outer_mod = pre_merging_metrics['weighted_outer_modularity_scores']\n",
    "uw_outer_mod = pre_merging_metrics['unweighted_outer_modularity_scores']\n",
    "w_shell_mod = pre_merging_metrics['weighted_shell_modularity_scores']\n",
    "uw_shell_mod = pre_merging_metrics['unweighted_shell_modularity_scores']\n",
    "\n",
    "suspicious_scores, clusters, avg_label_scores, text_similarity, \\\n",
    "w_density, uw_density, w_fraudar,  uw_fraudar, w_outer_edge, \\\n",
    "uw_outer_edge, w_shell_edge, uw_shell_edge,  w_outer_mod, \\\n",
    "uw_outer_mod, w_shell_mod, uw_shell_mod = \\\n",
    "zip(*sorted(zip(suspicious_scores, clusters, avg_label_scores, text_similarity, \n",
    "                w_density, uw_density, w_fraudar,  uw_fraudar, w_outer_edge, \n",
    "                uw_outer_edge, w_shell_edge, uw_shell_edge,  w_outer_mod, \n",
    "                uw_outer_mod, w_shell_mod, uw_shell_mod)))\n",
    "suspicious_scores = list(reversed(suspicious_scores))\n",
    "clusters = list(reversed(clusters))\n",
    "avg_label_scores = list(reversed(avg_label_scores))\n",
    "text_similarity = list(reversed(text_similarity))\n",
    "# filtered_avg_binary_scores = list(reversed(filtered_avg_binary_scores))\n",
    "w_density = list(reversed(w_density))\n",
    "uw_density = list(reversed(uw_density))\n",
    "w_fraudar = list(reversed(w_fraudar))  \n",
    "uw_fraudar = list(reversed(uw_fraudar))\n",
    "w_outer_edge = list(reversed(w_outer_edge)) \n",
    "uw_outer_edge = list(reversed(uw_outer_edge)) \n",
    "w_shell_edge = list(reversed(w_shell_edge))\n",
    "uw_shell_edge = list(reversed(uw_shell_edge))  \n",
    "w_outer_mod = list(reversed(w_outer_mod)) \n",
    "uw_outer_mod = list(reversed(uw_outer_mod)) \n",
    "w_shell_mod = list(reversed(w_shell_mod)) \n",
    "uw_shell_mod = list(reversed(uw_shell_mod))\n",
    "\n",
    "content = list(df_data['content_p'])\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2, 3), norm='l2', \n",
    "    smooth_idf=True, stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "\n",
    "print (bigram_matrix.shape)\n",
    "\n",
    "df_data['cluster_label'] = labels\n",
    "\n",
    "new_index = range(len(df_data))\n",
    "df_data = df_data.reindex(new_index)\n",
    "# print (df_da.index)\n",
    "\n",
    "k=20\n",
    "sort_ind = []\n",
    "for ind, cl in enumerate(clusters[:50]):\n",
    "    if cl==-1 or cl==-2:\n",
    "        continue\n",
    "    else:\n",
    "        df_f = df_data[df_data['cluster_label'] == cl]\n",
    "        print (df_f.shape)\n",
    "#         if df_f.shape[0] < 20:\n",
    "#             continue\n",
    "        sort_ind += list(df_f.index)\n",
    "print (sort_ind)\n",
    "\n",
    "\n",
    "sim_matrix = cosine_similarity(bigram_matrix)\n",
    "print (sim_matrix.shape)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "right='off', left='off', labelleft='off')\n",
    "plt.imshow(sim_matrix[sort_ind, :][:,sort_ind])\n",
    "plt.savefig('../results/sim_matrix.png')\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "fraudar_threshold = 1\n",
    "density_threshold = 0.05\n",
    "modularity_threshold = 0.3\n",
    "\n",
    "def execute_filtering(metric, cutoff_metric, cluster_counts, clusters, binary_true_labels, name_filter=True, threshold_filter=True, filter_type='FRAUDAR'):\n",
    "    labels = df_data['cluster_label']\n",
    "#     if filter_type == 'FRAUDAR':\n",
    "#         anomaly_metric,cluster_counts, clusters = zip(*sorted(zip(anomaly_metric, cluster_counts, clusters)))\n",
    "#         metric = fraudar_scores\n",
    "#         thresh = fraudar_threshold\n",
    "#     elif filter_type == 'DENSITY':\n",
    "#         cluster_density, fraudar_scores, cluster_counts, clusters = zip(*sorted(zip(cluster_density, fraudar_scores, cluster_counts, clusters)))\n",
    "#         metric = cluster_density\n",
    "#         thresh = density_threshold\n",
    "#     elif filter_type == 'MODULARITY':\n",
    "#         modularity_scores, cluster_density, fraudar_scores, cluster_counts, clusters = zip(*sorted(zip(modularity_scores, cluster_density, fraudar_scores, cluster_counts, clusters)))\n",
    "#         metric = modularity_scores\n",
    "#         thresh = modularity_threshold\n",
    "    metric,cluster_counts, clusters = zip(*sorted(zip(metric, cluster_counts, clusters)))\n",
    "    if threshold_filter:\n",
    "        max_index = None\n",
    "        for index, item in enumerate(metric):\n",
    "            if item > cutoff_metric:\n",
    "                max_index = index\n",
    "                break\n",
    "        if not max_index:\n",
    "            max_index = len(metric) - 1\n",
    "    else:\n",
    "        max_index = 0\n",
    "\n",
    "\n",
    "    input_size = bigram_matrix.shape[0]\n",
    "    binary_pred_labels = [0] * input_size\n",
    "    #     print (\"NUmber of anomalous clusters : {}\".format(len(clusters[max_index:])))\n",
    "    for c in clusters[max_index:]:\n",
    "        cluster_idx = np.argwhere(labels == l).reshape(-1)\n",
    "        for i in cluster_idx:\n",
    "            binary_pred_labels[i] = 1\n",
    "\n",
    "    anomaly_clusters = clusters[max_index:]\n",
    "    anomaly_cluster_metric = metric[max_index:]\n",
    "\n",
    "    #     print (\"Number of dense clusters : {}\".format(sum(cluster_counts[max_index:])))\n",
    "    filtered_clusters = []\n",
    "    filtered_cluster_metric = []\n",
    "    df_orig = df_data.copy()\n",
    "#     df_orig['cluster_label'] = labels\n",
    "\n",
    "    for ind, c in enumerate(anomaly_clusters):\n",
    "        if anomaly_cluster_metric[ind] == 1.0:\n",
    "            continue\n",
    "            \n",
    "        suspicious_scores = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "text_similarity = post_merging_metric['pairwise_similarity']\n",
    "clusters = post_merging_metric['clusters']\n",
    "avg_label_scores = post_merging_metric['avg_label_scores']\n",
    "avg_binary_scores = post_merging_metric['avg_binary_scores']\n",
    "w_density = post_merging_metric['weighted_cluster_density']\n",
    "uw_density = post_merging_metric['unweighted_cluster_density']\n",
    "w_fraudar = post_merging_metric['weighted_fraudar_scores']\n",
    "uw_fraudar = post_merging_metric['unweighted_fraudar_scores']\n",
    "w_outer_edge = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "uw_outer_edge = post_merging_metric['unweighted_outer_edge_perc_scores']\n",
    "w_shell_edge = post_merging_metric['weighted_shell_edge_perc_scores']\n",
    "uw_shell_edge = post_merging_metric['unweighted_shell_edge_perc_scores']\n",
    "w_outer_mod = post_merging_metric['weighted_outer_modularity_scores']\n",
    "uw_outer_mod = post_merging_metric['unweighted_outer_modularity_scores']\n",
    "w_shell_mod = post_merging_metric['weighted_shell_modularity_scores']\n",
    "uw_shell_mod = post_merging_metric['unweighted_shell_modularity_scores']\n",
    "\n",
    "filtered_clusters = filter_by_name(clusters, df_orig.copy())\n",
    "print (filtered_clusters)\n",
    "filtered_clusters = set(filtered_clusters)\n",
    "index_list = [i for i, e in enumerate(clusters) if e in filtered_clusters]\n",
    "filtered_avg_label_scores = [avg_label_scores[i] for i in index_list] \n",
    "filtered_text_similarity = [text_similarity[i] for i in index_list] \n",
    "filtered_suspicious_scores = [suspicious_scores[i] for i in index_list] \n",
    "filtered_avg_binary_scores = [avg_binary_scores[i] for i in index_list] \n",
    "filtered_w_density = [w_density[i] for i in index_list] \n",
    "filtered_uw_density = [uw_density[i] for i in index_list] \n",
    "filtered_w_fraudar = [w_fraudar[i] for i in index_list] \n",
    "filtered_uw_fraudar = [uw_fraudar[i] for i in index_list] \n",
    "filtered_w_outer_edge = [w_outer_edge[i] for i in index_list] \n",
    "filtered_uw_outer_edge = [uw_outer_edge[i] for i in index_list] \n",
    "filtered_w_shell_edge = [w_shell_edge[i] for i in index_list] \n",
    "filtered_uw_shell_edge = [uw_shell_edge[i] for i in index_list] \n",
    "filtered_w_outer_mod = [w_outer_mod[i] for i in index_list] \n",
    "filtered_uw_outer_mod = [uw_outer_mod[i] for i in index_list] \n",
    "filtered_w_shell_mod = [w_shell_mod[i] for i in index_list] \n",
    "filtered_uw_shell_mod = [uw_shell_mod[i] for i in index_list] \n",
    "\n",
    "print (filtered_suspicious_scores)\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import plotly.express as px\n",
    "\n",
    "filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \\\n",
    "filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, filtered_w_outer_edge, \\\n",
    "filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  filtered_w_outer_mod, \\\n",
    "filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod = \\\n",
    "zip(*sorted(zip(filtered_suspicious_scores, filtered_clusters, filtered_avg_label_scores, filtered_text_similarity, \n",
    "                filtered_w_density, filtered_uw_density, filtered_w_fraudar,  filtered_uw_fraudar, \n",
    "                filtered_w_outer_edge, filtered_uw_outer_edge, filtered_w_shell_edge, filtered_uw_shell_edge,  \n",
    "                filtered_w_outer_mod, filtered_uw_outer_mod, filtered_w_shell_mod, filtered_uw_shell_mod)))\n",
    "filtered_suspicious_scores = list(reversed(filtered_suspicious_scores))\n",
    "filtered_clusters = list(reversed(filtered_clusters))\n",
    "filtered_avg_label_scores = list(reversed(filtered_avg_label_scores))\n",
    "filtered_text_similarity = list(reversed(filtered_text_similarity))\n",
    "filtered_avg_binary_scores = list(reversed(filtered_avg_binary_scores))\n",
    "filtered_w_density = list(reversed(filtered_w_density))\n",
    "filtered_uw_density = list(reversed(filtered_uw_density))\n",
    "filtered_w_fraudar = list(reversed(filtered_w_fraudar))  \n",
    "filtered_uw_fraudar = list(reversed(filtered_uw_fraudar))\n",
    "filtered_w_outer_edge = list(reversed(filtered_w_outer_edge)) \n",
    "filtered_uw_outer_edge = list(reversed(filtered_w_density)) \n",
    "filtered_w_shell_edge = list(reversed(filtered_w_shell_edge))\n",
    "filtered_uw_shell_edge = list(reversed(filtered_uw_shell_edge))  \n",
    "filtered_w_outer_mod = list(reversed(filtered_w_outer_mod)) \n",
    "filtered_uw_outer_mod = list(reversed(filtered_uw_outer_mod)) \n",
    "filtered_w_shell_mod = list(reversed(filtered_w_shell_mod)) \n",
    "filtered_uw_shell_mod = list(reversed(filtered_uw_shell_mod))\n",
    "top_k = [10, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "rcParams['figure.figsize'] = 4,4\n",
    "for k in top_k:\n",
    "    print ('==================================k = {}========================================'.format(k) )\n",
    "    plt.hist(filtered_avg_label_scores[:k], bins=7)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(filtered_avg_binary_scores[:k], bins=3)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(filtered_text_similarity[:k], marker='o', linestyle='--')\n",
    "    plt.xlabel(' Clusters')\n",
    "    plt.ylabel('avg text similarity')\n",
    "    plt.show()\n",
    "    \n",
    "    print (\"Average of average label scores : {}\".format(sum(filtered_avg_label_scores[:k])/k))\n",
    "    print (\"Average text similarity : {}\".format(sum(filtered_text_similarity[:k])/k))\n",
    "    print ('\\n')\n",
    "    eig_ts_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_text_similarity[:k])[0]\n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_label_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print (\"Spearman Correlation | Average Label Score |          Text Similarity          | {0:.2f} \".format(eig_ts_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Label Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "    \n",
    "    eig_w_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_density[:k])[0]\n",
    "    eig_uw_den_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_density[:k])[0]\n",
    "    eig_w_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_fraudar[:k])[0]\n",
    "    eig_uw_fraud_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_fraudar[:k])[0]\n",
    "    eig_w_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_edge[:k])[0]\n",
    "    eig_uw_edge_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_edge[:k])[0]\n",
    "    eig_w_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_edge[:k])[0]\n",
    "    eig_uw_edge_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_edge[:k])[0]\n",
    "    eig_w_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_outer_mod[:k])[0]\n",
    "    eig_uw_mod_out_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_outer_mod[:k])[0]\n",
    "    eig_w_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_w_shell_mod[:k])[0]\n",
    "    eig_uw_mod_shell_pscore = spearmanr(filtered_avg_binary_scores[:k], filtered_uw_shell_mod[:k])[0]\n",
    "    \n",
    "    print ('\\n')\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "    print (\"Spearman Correlation | Average Binary Score | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))\n",
    "print (len(clusters))\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_w_outer_edge, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "ax = sns.regplot(x=filtered_text_similarity, y=filtered_w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Text Similarity', ylabel='Weighted outer edge percentage')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=filtered_avg_label_scores, y=filtered_text_similarity, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Average Label Score', ylabel='Text Similarity')\n",
    "plt.title('After filtering out clusters with only one individual')\n",
    "plt.show()\n",
    "\n",
    "    binary_dense_pred_labels = [0]*input_size\n",
    "    for ind, c in enumerate(filtered_clusters):\n",
    "        \n",
    "        cluster_idx = np.argwhere(labels == c).reshape(-1)\n",
    "#         score = sum(df_fil['label'])/len(df_fil)\n",
    "#         if score >= 0:\n",
    "        for i in cluster_idx:\n",
    "            if probs[i] > 0.0:\n",
    "                binary_dense_pred_labels[i] = 1\n",
    "    #     print (\"Number of predicted ads : {}\".format(binary_dense_pred_labels.count(1)))\n",
    "    fmeasure = f1_score(binary_true_labels,binary_dense_pred_labels,average='macro')\n",
    "    print (\"F-measure : {}\".format(fmeasure))\n",
    "    pscore = classification_report(binary_true_labels, binary_dense_pred_labels)\n",
    "    print (pscore)\n",
    "    \n",
    "    return filtered_clusters, filtered_cluster_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "eigen_ratios = post_merging_metric['eigen_ratios']\n",
    "\n",
    "eig_w_den_pscore = spearmanr(avg_label_scores[:top_k], w_density[:top_k])[0]\n",
    "eig_uw_den_pscore = spearmanr(avg_label_scores[:top_k], uw_density[:top_k])[0]\n",
    "eig_w_fraud_pscore = spearmanr(avg_label_scores[:top_k],w_fraudar[:top_k])[0]\n",
    "eig_uw_fraud_pscore = spearmanr(avg_label_scores[:top_k], uw_fraudar[:top_k])[0]\n",
    "eig_w_edge_out_pscore = spearmanr(avg_label_scores[:top_k], w_outer_edge[:top_k])[0]\n",
    "eig_uw_edge_out_pscore = spearmanr(avg_label_scores[:top_k], uw_outer_edge[:top_k])[0]\n",
    "eig_w_edge_shell_pscore = spearmanr(avg_label_scores[:top_k], w_shell_edge[:top_k])[0]\n",
    "eig_uw_edge_shell_pscore = spearmanr(avg_label_scores[:top_k], uw_shell_edge[:top_k])[0]\n",
    "eig_w_mod_out_pscore = spearmanr(avg_label_scores[:top_k], w_outer_mod[:top_k])[0]\n",
    "eig_uw_mod_out_pscore = spearmanr(avg_label_scores[:top_k], uw_outer_mod[:top_k])[0]\n",
    "eig_w_mod_shell_pscore = spearmanr(avg_label_scores[:top_k], w_shell_mod[:top_k])[0]\n",
    "eig_uw_mod_shell_pscore = spearmanr(avg_label_scores[:top_k], uw_shell_mod[:top_k])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Spearman Correlation | Eigen Ratio |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in clusters[:100]:\n",
    "    if cl < 0:\n",
    "        continue\n",
    "    df_d = df_orig[df_orig['final_label']==cl]\n",
    "    for i, row in df_d.iterrows():\n",
    "        print (row['body'])\n",
    "        print ('-------------------------------------------------------------')\n",
    "    print ('==========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choices([0,1], weights=[0.33, 0.67], k=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "binary_true_labels = df_orig['binary_label']\n",
    "binary_pred_labels = np.zeros(len(binary_true_labels))\n",
    "metric = 'max_binary'\n",
    "clusters = post_merging_metric['clusters']\n",
    "metric = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "thresholds = 3\n",
    "use_metric = False\n",
    "use_random = True\n",
    "for ind, row in df_orig.iterrows():\n",
    "    cl = row['final_label']\n",
    "    if cl == -1:\n",
    "        if use_random:\n",
    "            binary_pred_labels[ind] = random.choices([0,1], weights=[0.67, 0.33], k=1)[0]\n",
    "        else:\n",
    "            binary_pred_labels[ind] = 0\n",
    "    elif cl == -2:\n",
    "        if use_random:\n",
    "            binary_pred_labels[ind] = random.choices([0,1], weights=[0.67, 0.33], k=1)[0]\n",
    "        else:\n",
    "            binary_pred_labels[ind] = 0\n",
    "    else:\n",
    "#         binary_pred_labels[ind] = random.choices([0,1], weights=[0.67, 0.33], k=1)[0]\n",
    "        if use_metric:\n",
    "            i = clusters.index(cl)\n",
    "            score = metric[i]\n",
    "            if score > threshold:\n",
    "                binary_pred_labels[ind] = 1\n",
    "        else:\n",
    "            binary_pred_labels[ind] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_acc(tp, tn, n, p):\n",
    "    return (tp*n/p + (n))/(2*n)\n",
    "\n",
    "def get_tpr(tp, fn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def get_fpr(fp, tn):\n",
    "    return fp/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "c_mat = confusion_matrix(binary_true_labels, binary_pred_labels)\n",
    "\n",
    "tn, fp, fn, tp = c_mat[0][0], c_mat[0][1], c_mat[1][0], c_mat[1][1]\n",
    "total_n = len(np.where(binary_true_labels==0)[0])\n",
    "total_p = len(np.where(binary_true_labels==1)[0])\n",
    "weight_acc = get_weighted_acc(tp, tn, total_n, total_p)\n",
    "print (total_p, total_n)\n",
    "print (\"Weighted Accuracy : {}\".format(round(weight_acc, 2)))\n",
    "\n",
    "print (c_mat)\n",
    "\n",
    "pscore = classification_report(binary_true_labels, binary_pred_labels)\n",
    "# fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n",
    "print (pscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = list(df_orig['content_p'])\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2, 3), norm='l2', \n",
    "    smooth_idf=True, stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "\n",
    "print (bigram_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_ind = []\n",
    "for ind, cl in enumerate(clusters):\n",
    "    if cl==-1 or cl==-2:\n",
    "        continue\n",
    "    else:\n",
    "        df_f = df_orig[df_orig['final_label'] == cl]\n",
    "        if df_f.shape[0] < 20:\n",
    "            continue\n",
    "        sort_ind += list(df_f.index)\n",
    "print (sort_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_matrix = cosine_similarity(bigram_matrix)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    "right='off', left='off', labelleft='off')\n",
    "plt.imshow(sim_matrix[sort_ind, :][:,sort_ind])\n",
    "plt.savefig('../results/sim_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rcParams['figure.figsize'] = 4,4\n",
    "metric = 'max_binary'\n",
    "clusters = post_merging_metric['clusters']\n",
    "metric = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "threshold = [0, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 3.5]\n",
    "use_metric = True\n",
    "use_random = True\n",
    "fprs= []\n",
    "tprs = []\n",
    "for thresh in threshold:\n",
    "    binary_true_labels = df_orig['binary_label']\n",
    "    binary_pred_labels = np.zeros(len(binary_true_labels))\n",
    "    for ind, row in df_orig.iterrows():\n",
    "        cl = row['final_label']\n",
    "        if cl == -1:\n",
    "            binary_pred_labels[ind] = 0\n",
    "        elif cl == -2:\n",
    "            if use_random:\n",
    "                binary_pred_labels[ind] = random.choice([0,1])\n",
    "            else:\n",
    "                binary_pred_labels[ind] = 0\n",
    "        else:\n",
    "            if use_metric:\n",
    "                i = clusters.index(cl)\n",
    "                score = metric[i]\n",
    "                if score > thresh:\n",
    "                    binary_pred_labels[ind] = 1\n",
    "            else:\n",
    "                binary_pred_labels[ind] = 1\n",
    "\n",
    "    c_mat = confusion_matrix(binary_true_labels, binary_pred_labels)\n",
    "\n",
    "    tn, fp, fn, tp = c_mat[0][0], c_mat[0][1], c_mat[1][0], c_mat[1][1]\n",
    "    # total_n = len(np.where(binary_true_labels==0)[0])\n",
    "    # total_p = len(np.where(binary_true_labels==1)[0])\n",
    "    # weight_acc = get_weighted_acc(tp, tn, total_n, total_p)\n",
    "    # print (total_p, total_n)\n",
    "    # print (\"Weighted Accuracy : {}\".format(round(weight_acc, 2)))\n",
    "\n",
    "    # pscore = classification_report(binary_true_labels, binary_pred_labels)\n",
    "    fprs.append(get_fpr(fp, tn))\n",
    "    tprs.append(get_tpr(tp, fn))\n",
    "\n",
    "plt.step(fprs, tprs)\n",
    "plt.ylabel('TPR (Sensitivity)')\n",
    "plt.xlabel('FPR (1-Specificity)')\n",
    "# plt.tick_params(\n",
    "#     axis='both',          # changes apply to the x-axis\n",
    "#     which='both',      # both major and minor ticks are affected\n",
    "#     bottom=False,      # ticks along the bottom edge are off\n",
    "#     top=False,         # ticks along the top edge are off\n",
    "#     labelbottom=False,\n",
    "# right='off', left='off', labelleft='off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(df, score_type):\n",
    "    if score_type == 'max_binary':\n",
    "        score = max(df['binary_label'])\n",
    "    elif score_type == 'avg_label':\n",
    "        score = sum(df['label'])/len(df)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Supervised Setting\n",
    "\n",
    "import random\n",
    "\n",
    "binary_true_labels = df_orig['binary_label']\n",
    "\n",
    "n_sample = int(0.1 * len(df_orig))\n",
    "random_sample = random.sample(range(len(df_orig)), n_sample)\n",
    "clusters.append(-2)\n",
    "score_vals = []\n",
    "\n",
    "for cl in clusters:\n",
    "    if cl == -1:\n",
    "        score_vals.append(-1)\n",
    "    elif cl== -2:\n",
    "        score_vals.append(-1)\n",
    "    else:\n",
    "        df_f = df_orig[df_orig['final_label'] == cl]\n",
    "#         print (len(df_f))\n",
    "        for i, row in df_f.iterrows():\n",
    "            if i in random_sample:\n",
    "                df_f.drop(i, inplace=True)\n",
    "        score_vals.append(calculate_score(df_f, 'avg_label'))\n",
    "\n",
    "print (len(score_vals))\n",
    "\n",
    "\n",
    "# metric = 'max_binary_scores'\n",
    "clusters = post_merging_metric['clusters']\n",
    "metric = score_vals\n",
    "use_metric = True\n",
    "use_random = True\n",
    "threshold = [0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6]\n",
    "tprs = []\n",
    "fprs= []\n",
    "for thresh in threshold:\n",
    "    binary_pred_labels = np.zeros(len(binary_true_labels))\n",
    "    for ind, row in df_orig.iterrows():\n",
    "        cl = row['final_label']\n",
    "        if cl == -1:\n",
    "            binary_pred_labels[ind] = 0\n",
    "        elif cl == -2:\n",
    "            if use_random:\n",
    "                binary_pred_labels[ind] = random.choice([0,1])\n",
    "            else:\n",
    "                binary_pred_labels[ind] = 0\n",
    "        else:\n",
    "            if use_metric:\n",
    "                i = clusters.index(cl)\n",
    "                score = metric[i]\n",
    "                if score > thresh:\n",
    "                    binary_pred_labels[ind] = 1\n",
    "            else:\n",
    "                binary_pred_labels[ind] = 1\n",
    "\n",
    "    c_mat = confusion_matrix(binary_true_labels, binary_pred_labels)\n",
    "\n",
    "    tn, fp, fn, tp = c_mat[0][0], c_mat[0][1], c_mat[1][0], c_mat[1][1]\n",
    "\n",
    "    fprs.append(get_fpr(fp, tn))\n",
    "    tprs.append(get_tpr(tp, fn))\n",
    "\n",
    "plt.step(fprs, tprs)\n",
    "plt.ylabel('TPR (Sensitivity)')\n",
    "plt.xlabel('FPR (1-Specificity)')\n",
    "# plt.tick_params(\n",
    "#     axis='both',          # changes apply to the x-axis\n",
    "#     which='both',      # both major and minor ticks are affected\n",
    "#     bottom=False,      # ticks along the bottom edge are off\n",
    "#     top=False,         # ticks along the top edge are off\n",
    "#     labelbottom=False,\n",
    "# right='off', left='off', labelleft='off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(fprs, tprs)\n",
    "plt.ylabel('TPR (Sensitivity)')\n",
    "plt.xlabel('FPR (1-Specificity)')\n",
    "# plt.tick_params(\n",
    "#     axis='both',          # changes apply to the x-axis\n",
    "#     which='both',      # both major and minor ticks are affected\n",
    "#     bottom=False,      # ticks along the bottom edge are off\n",
    "#     top=False,         # ticks along the top edge are off\n",
    "#     labelbottom=False,\n",
    "# right='off', left='off', labelleft='off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "c_mat = confusion_matrix(binary_true_labels, binary_pred_labels)\n",
    "\n",
    "tn, fp, fn, tp = c_mat[0][0], c_mat[0][1], c_mat[1][0], c_mat[1][1]\n",
    "total_n = len(np.where(binary_true_labels==0)[0])\n",
    "total_p = len(np.where(binary_true_labels==1)[0])\n",
    "weight_acc = get_weighted_acc(tp, tn, total_n, total_p)\n",
    "print (total_p, total_n)\n",
    "print (\"Weighted Accuracy : {}\".format(round(weight_acc, 2)))\n",
    "\n",
    "pscore = classification_report(binary_true_labels, binary_pred_labels)\n",
    "print (pscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(post_merging_metric['clusters']), len(post_merging_metric['avg_label_scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "class1_clusters = []\n",
    "class2_clusters = []\n",
    "class3_clusters = []\n",
    "clusters = post_merging_metric['clusters']\n",
    "scores = post_merging_metric['avg_label_scores']\n",
    "# print (scores)\n",
    "df_plot = df_orig[df_orig['sim_check'] == False]\n",
    "for ind, cl in enumerate(clusters[:458]):\n",
    "    if cl == -1:\n",
    "        continue\n",
    "    elif scores[ind] > 3.5:\n",
    "        class1_clusters.append(cl)\n",
    "    elif scores[ind] <=3.5 and scores[ind]> 2:\n",
    "        class2_clusters.append(cl)\n",
    "    else:\n",
    "        class3_clusters.append(cl)\n",
    "\n",
    "purity = np.zeros((3,3))\n",
    "classes = [{'clusters' : class1_clusters,'purity':[]}, \n",
    "           {'clusters' : class2_clusters,'purity':[]}, \n",
    "           {'clusters' : class3_clusters,'purity':[]}]\n",
    "for ind, cls in enumerate(classes):\n",
    "    t_p = [0,0,0]\n",
    "    cls_clusters = cls['clusters']\n",
    "    for cl in cls_clusters:\n",
    "        df_f = df_plot[df_plot['final_label']==cl]\n",
    "        p = [len(df_f[df_f['label']<3]),\n",
    "                 len(df_f[df_f['label']==3]),\n",
    "                 len(df_f[df_f['label']>3])]\n",
    "        \n",
    "        t_p = [sum(x) for x in zip(t_p, p)]\n",
    "#     print (t_p)\n",
    "    purity[ind] = np.array(t_p)\n",
    "    cls['purity'] = t_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = range(3)\n",
    "width = 0.35  \n",
    "p0 = plt.bar(xaxis, purity.T[0], width, color='green', alpha=0.4)\n",
    "p1 = plt.bar(xaxis, purity.T[1], width, bottom=purity.T[0], color='grey', alpha=0.6 )\n",
    "gap_1 = [sum(x) for x in zip(purity.T[0], purity.T[1])]\n",
    "p2 = plt.bar(xaxis, purity.T[2], width, bottom=gap_1, color='red', alpha=0.6)\n",
    "# gap_2 = [sum(x) for x in zip(gap_1, purity.T[2])]\n",
    "# p3 = plt.bar(xaxis, purity.T[3], width, bottom=gap_2, color='grey', alpha=0.4 )\n",
    "# gap_3 = [sum(x) for x in zip(gap_2, purity.T[3])]\n",
    "# p4 = plt.bar(xaxis, purity.T[4], width, bottom=gap_3, color='red', alpha=0.5 )\n",
    "# gap_4 = [sum(x) for x in zip(gap_3, purity.T[4])]\n",
    "# p5 = plt.bar(xaxis, purity.T[5], width,bottom=gap_4, color='red', alpha=0.6 )\n",
    "# gap_5 = [sum(x) for x in zip(gap_4, purity.T[5])]\n",
    "# p6 = plt.bar(xaxis, purity.T[6], width,bottom=gap_5,color='red', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "true_labels_all = df_original['binary_label']\n",
    "print (true_labels_all)\n",
    "c_mat = confusion_matrix(true_labels_all, binary_all_labels)\n",
    "b_score = balanced_accuracy_score(true_labels_all, binary_all_labels)\n",
    "tn, fp, fn, tp = c_mat[0][0], c_mat[0][1], c_mat[1][0], c_mat[1][1]\n",
    "total_n = len(np.where(true_labels_all==0)[0])\n",
    "total_p = len(np.where(true_labels_all==1)[0])\n",
    "weight_acc = get_weighted_acc(tp, tn, total_n, total_p)\n",
    "print (total_n, total_p)\n",
    "pscore = classification_report(true_labels_all, binary_all_labels)\n",
    "fmeasure = f1_score(true_labels_all, binary_all_labels,average='weighted')\n",
    "clusters = list(set(df_data['cluster_label']))\n",
    "print (pscore)\n",
    "print (\"Fmeasure: {}\".format(fmeasure))\n",
    "print (\"Weighted Acc : {}\".format(weight_acc))\n",
    "class1_purity = []\n",
    "class2_purity = []\n",
    "class3_purity = []\n",
    "class1_count = 0\n",
    "class2_count = 0\n",
    "class3_count = 0\n",
    "avg_label_score = []\n",
    "for c in clusters:\n",
    "    df_fil = df_data[df_data['cluster_label']==c]\n",
    "    if len(df_fil) == 0:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = sum(df_fil['label'])/len(df_fil)\n",
    "    binary_label_purity = sum(df_fil['binary_label'])/len(df_fil)\n",
    "    if score >= 3.5:\n",
    "        class1_count += 1\n",
    "        class1_purity.append(binary_label_purity)\n",
    "    elif score < 3.5 and score >= 2:\n",
    "        class2_count += 1\n",
    "        class2_purity.append(binary_label_purity)\n",
    "    else:\n",
    "        class3_count += 1\n",
    "        class3_purity.append(binary_label_purity)\n",
    "        \n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"|           |   Total   | Filtered |  Avg Purity  |\")\n",
    "print (\"|  Class 1  |    {}    |    {}    |     {}     |\".format(class1_count, class1_count, round(sum(class1_purity)/class1_count,2)))\n",
    "print (\"|  Class 2  |    {}    |    {}    |     {}     |\".format(class2_count, class2_count, round(sum(class2_purity)/class2_count,2)))\n",
    "print (\"|  Class 3  |    {}    |    {}    |     {}     |\".format(class3_count, class3_count, round(sum(class3_purity)/class3_count, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['cluster_counts'], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_MAP = {\n",
    "    'WEIGHTED_FRAUDAR' : 'weighted_fraudar_scores',\n",
    "    'WEIGHTED_OUTER_MODULARITY' : 'weighted_outer_modularity_scores',\n",
    "    'WEIGHTED_OUTER_EDGE_PERCENTAGE' : 'weighted_outer_edge_perc_scores',\n",
    "    'WEIGHTED_DENSITY' : 'weighted_cluster_density',\n",
    "    'UNWEIGHTED_FRAUDAR' : 'unweighted_fraudar_scores',\n",
    "    'UNWEIGHTED_OUTER_MODULARITY' : 'unweighted_outer_modularity_scores',\n",
    "    'UNWEIGHTED_OUTER_EDGE_PERCENTAGE' : 'unweighted_outer_edge_perc_scores',\n",
    "    'UNWEIGHTED_DENSITY' : 'unweighted_cluster_density'\n",
    "}\n",
    "\n",
    "METRIC_CUTOFF_MAP = {\n",
    "    'WEIGHTED_FRAUDAR' : 0.2,\n",
    "    'WEIGHTED_OUTER_MODULARITY' : 0.0005,\n",
    "    'WEIGHTED_SHELL_MODULARITY' : 0.0005,\n",
    "    'WEIGHTED_OUTER_EDGE_PERCENTAGE' : 0.5,\n",
    "    'WEIGHTED_SHELL_EDGE_PERCENTAGE' : 0.2,\n",
    "    'WEIGHTED_DENSITY' : 0.05,\n",
    "    'UNWEIGHTED_FRAUDAR' : 2,\n",
    "    'UNWEIGHTED_OUTER_MODULARITY' : 0.0005,\n",
    "    'UNWEIGHTED_SHELL_MODULARITY' : 0.0005,\n",
    "    'UNWEIGHTED_OUTER_EDGE_PERCENTAGE' : 0.5,\n",
    "    'UNWEIGHTED_SHELL_EDGE_PERCENTAGE' : 0.2,\n",
    "    'UNWEIGHTED_DENSITY' : 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourth_quadrant_count(avg_label_scores, metric, threshold):\n",
    "    avg_label_scores_np = np.array(avg_label_scores)\n",
    "    metric_np = np.array(metric)\n",
    "    trafficking_indices = list(np.where(avg_label_scores_np>3)[0])\n",
    "    low_metric_threshold = list(np.where(metric_np<threshold)[0])\n",
    "    \n",
    "    quad4 = [x for x in trafficking_indices if x in low_metric_threshold]\n",
    "    \n",
    "    return quad4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 4,4\n",
    "print (\"########## AVERAGE LABEL SCORES ################\")\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['weighted_cluster_density'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['weighted_cluster_density'], METRIC_CUTOFF_MAP['WEIGHTED_DENSITY'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['WEIGHTED_DENSITY'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Weighted Cluster Density\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/weighted_density.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['unweighted_cluster_density'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['unweighted_cluster_density'], \n",
    "                                  METRIC_CUTOFF_MAP['UNWEIGHTED_DENSITY'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['UNWEIGHTED_DENSITY'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs UnWeighted Cluster Density\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/unweighted_density.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['weighted_fraudar_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['weighted_fraudar_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['WEIGHTED_FRAUDAR'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['WEIGHTED_FRAUDAR'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Weighted Fraudar Scores\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/weighted_fraudar.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['unweighted_fraudar_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['unweighted_fraudar_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['UNWEIGHTED_FRAUDAR'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['UNWEIGHTED_FRAUDAR'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs UnWeighted Fraudar Scores\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/unweighted_fraudar.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['weighted_outer_edge_perc_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['weighted_outer_edge_perc_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['WEIGHTED_OUTER_EDGE_PERCENTAGE'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['WEIGHTED_OUTER_EDGE_PERCENTAGE'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Weighted Outer Edge Perc\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/weighted_outer_edge.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['unweighted_outer_edge_perc_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['unweighted_outer_edge_perc_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['UNWEIGHTED_OUTER_EDGE_PERCENTAGE'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['UNWEIGHTED_OUTER_EDGE_PERCENTAGE'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Unweighted Outer Edge Perc\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/unweighted_outer_edge.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['unweighted_shell_edge_perc_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['unweighted_shell_edge_perc_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['UNWEIGHTED_SHELL_EDGE_PERCENTAGE'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['UNWEIGHTED_SHELL_EDGE_PERCENTAGE'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Unweighted Shell Edge Perc\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/unweighted_shell_edge.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['weighted_shell_edge_perc_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['weighted_shell_edge_perc_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['WEIGHTED_SHELL_EDGE_PERCENTAGE'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['WEIGHTED_SHELL_EDGE_PERCENTAGE'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs weighted Shell Edge Perc\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/weighted_shell_edge.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['weighted_outer_modularity_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['weighted_outer_modularity_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['WEIGHTED_OUTER_MODULARITY'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['WEIGHTED_OUTER_MODULARITY'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Weighted Outer Modularity\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/weighted_outer_modularity.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['unweighted_outer_modularity_scores'], alpha=0.2)\n",
    "\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['unweighted_outer_modularity_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['UNWEIGHTED_OUTER_MODULARITY'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['UNWEIGHTED_OUTER_MODULARITY'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Unweighted Outer Modularity\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/unweighted_outer_modularity.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['weighted_shell_modularity_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['weighted_shell_modularity_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['WEIGHTED_SHELL_MODULARITY'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['WEIGHTED_SHELL_MODULARITY'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Weighted Shell Modularity\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/weighted_shell_modularity.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['weighted_shell_modularity_scores'], alpha=0.2)\n",
    "quad4 = get_fourth_quadrant_count(post_merging_metric['avg_label_scores'], \n",
    "                                  post_merging_metric['unweighted_shell_modularity_scores'], \n",
    "                                  METRIC_CUTOFF_MAP['UNWEIGHTED_SHELL_MODULARITY'])\n",
    "plt.legend(['Number of clusters in Quad4={}'.format(len(quad4))])\n",
    "plt.axvline(x=3, color='red', linestyle='--')\n",
    "plt.axhline(y=METRIC_CUTOFF_MAP['UNWEIGHTED_SHELL_MODULARITY'], color='black', linestyle='--')\n",
    "plt.title(\"Average Label Scores vs Weighted Shell Modularity\")\n",
    "plt.savefig('../results/anomaly_metric_scatter_plots/unweighted_shell_modularity.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['cluster_counts'], alpha=0.2)\n",
    "plt.title(\"Average Label Scores vs Weighted Shell Modularity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using overall modularity of the graph\n",
    "#Increase in eigen ratio in total results (% of results with eigenratio above 0.8)\n",
    "# % of results in quadrant 4 with and without rerunning on noisy set and merging of clusters\n",
    "# total number of anomalous clusters\n",
    "# Average cluster sizes before and after rerunning\n",
    "# for merged clusters, if modularity or density increased or decreased\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(encoded_vecs, list(df_data['cluster_label']), metric='euclidean')\n",
    "\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_vals = post_merging_metric['max_binary_scores']\n",
    "labels = list(df_data['cluster_label'].copy())\n",
    "clusters = post_merging_metric['clusters']\n",
    "binary_labels = df_original['binary_label']\n",
    "print (binary_labels.shape)\n",
    "# scoring_vals, labels, clusters, binary_labels = zip(*sorted(zip(scoring_vals, labels, clusters, binary_labels)))\n",
    "# print (binary_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all = np.zeros(len(df_original))\n",
    "cnt = 0\n",
    "for i in range(len(df_original)):\n",
    "    if i in noisy_set:\n",
    "        labels_all[i] = -1\n",
    "    else:\n",
    "        labels_all[i] = labels[cnt]\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (labels_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_acc(tp, tn, n, p):\n",
    "    return (tp*n/p + (n))/(2*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "n_sample = int(0.1 * len(df_original))\n",
    "random_sample = random.sample(range(len(df_original)), n_sample)\n",
    "\n",
    "print (n_sample)\n",
    "binary_labels = np.array(binary_labels)\n",
    "true_labels_random_sample = binary_labels[random_sample]\n",
    "pred_labels_random_sample = np.zeros(n_sample)\n",
    "\n",
    "print (len(np.where(true_labels_random_sample==1)[0]))\n",
    "threshold = 0\n",
    "anomalous_clusters = []\n",
    "for ind, c in enumerate(clusters):\n",
    "    if c == -1:\n",
    "        continue\n",
    "    if scoring_vals[ind] > threshold:\n",
    "        anomalous_clusters.append(c)\n",
    "\n",
    "        \n",
    "for i, sam in enumerate(random_sample):\n",
    "    if sam in noisy_set:\n",
    "        continue\n",
    "    elif labels_all[sam] in anomalous_clusters:\n",
    "        pred_labels_random_sample[i] = 1\n",
    "\n",
    "pscore = classification_report(true_labels_random_sample, list(pred_labels_random_sample))\n",
    "fmeasure = f1_score(true_labels_random_sample,list(pred_labels_random_sample),average='weighted')\n",
    "c_mat = confusion_matrix(true_labels_random_sample, pred_labels_random_sample)\n",
    "b_score = balanced_accuracy_score(true_labels_random_sample, pred_labels_random_sample)\n",
    "tn, fp, fn, tp = c_mat[0][0], c_mat[0][1], c_mat[1][0], c_mat[1][1]\n",
    "total_n = len(np.where(true_labels_random_sample==0)[0])\n",
    "total_p = len(np.where(true_labels_random_sample==1)[0])\n",
    "weight_acc = get_weighted_acc(tp, tn, total_n, total_p)\n",
    "roc_curve(true_labels_random_sample, pred_labels_random_sample)\n",
    "print (\"weighted acc : {}\".format(round(weight_acc, 2)))\n",
    "print (\"Confusion matrix:\")\n",
    "print (c_mat)\n",
    "print (pscore)\n",
    "print (\"Weighted Accuracy : {}\".format(round(b_score, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "fraudar_threshold = 1\n",
    "density_threshold = 0.05\n",
    "modularity_threshold = 0.3\n",
    "\n",
    "def execute_filtering_by_count(metric, cluster_counts, clusters, binary_true_labels, n_top, name_filter=True):\n",
    "    labels = df_data['cluster_label']\n",
    "\n",
    "    metric,cluster_counts, clusters = zip(*sorted(zip(metric, cluster_counts, clusters)))\n",
    "    metric = list(reversed(metric))\n",
    "    cluster_counts = list(reversed(cluster_counts))\n",
    "    clusters = list(reversed(clusters))\n",
    "    \n",
    "    input_size = bigram_matrix.shape[0]\n",
    "    filtered_clusters = []\n",
    "    filtered_cluster_metric = []\n",
    "    df_orig = df_data.copy()\n",
    "#     df_orig['cluster_label'] = labels\n",
    "\n",
    "    for ind, c in enumerate(clusters):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        df_fil = df_orig[df_orig['cluster_label']==c]\n",
    "        if name_filter:\n",
    "            names = [x.lower() if type(x) == type('') else None for x in df_fil['Name'].unique()]\n",
    "            if math.nan in names:\n",
    "                names.remove(math.nan)\n",
    "            if None in names:\n",
    "                names.remove(None)\n",
    "            names = list(set(names))\n",
    "            if len(names)>1:\n",
    "    #             print (names)\n",
    "                filtered_clusters.append(c)\n",
    "                filtered_cluster_metric.append(metric[ind])\n",
    "        else:\n",
    "            filtered_clusters.append(c)\n",
    "            filtered_cluster_metric.append(metric[ind])\n",
    "        if len(filtered_clusters) == n_top:\n",
    "            break\n",
    "\n",
    "    binary_dense_pred_labels = [0]*input_size\n",
    "    for ind, c in enumerate(filtered_clusters):\n",
    "        \n",
    "        cluster_idx = np.argwhere(labels == c).reshape(-1)\n",
    "#         score = sum(df_fil['label'])/len(df_fil)\n",
    "#         if score >= 0:\n",
    "        for i in cluster_idx:\n",
    "            binary_dense_pred_labels[i] = 1\n",
    "    #     print (\"Number of predicted ads : {}\".format(binary_dense_pred_labels.count(1)))\n",
    "    fmeasure = f1_score(binary_true_labels,binary_dense_pred_labels,average='macro')\n",
    "    print (\"F-measure : {}\".format(fmeasure))\n",
    "    pscore = classification_report(binary_true_labels, binary_dense_pred_labels)\n",
    "    print (pscore)\n",
    "    \n",
    "    return filtered_clusters, filtered_cluster_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "#     max_index = 1\n",
    "#     max_diff = densities[1] - densities[0]\n",
    "name_filter = [True, False]\n",
    "\n",
    "filter_metrics = ['WEIGHTED_FRAUDAR', 'WEIGHTED_OUTER_MODULARITY', 'WEIGHTED_OUTER_EDGE_PERCENTAGE', 'WEIGHTED_DENSITY', \n",
    "                 'UNWEIGHTED_FRAUDAR', 'UNWEIGHTED_OUTER_MODULARITY', 'UNWEIGHTED_OUTER_EDGE_PERCENTAGE', 'UNWEIGHTED_DENSITY']\n",
    "for n_filter in name_filter:\n",
    "    for filter_param in filter_metrics:\n",
    "        class1_purity = []\n",
    "        class2_purity = []\n",
    "        class3_purity = []\n",
    "        class1_count = 0\n",
    "        class2_count = 0\n",
    "        class3_count = 0\n",
    "        print ('==============Anomally Metric: {}, Name Filtering: {}======================'.format(filter_param, \n",
    "                                                                                                    n_filter))\n",
    "        metric = post_merging_metric[METRIC_MAP[filter_param]]\n",
    "        metric_cutoff = METRIC_CUTOFF_MAP[filter_param]\n",
    "        cluster_counts = post_merging_metric['cluster_counts']\n",
    "        clusters = list(set(df_data['cluster_label']))\n",
    "\n",
    "        filtered_clusters, filtered_cluster_metric = execute_filtering_by_count(metric, \n",
    "                                                                       cluster_counts, clusters, \n",
    "                                                                       binary_true_labels, 100, n_filter)\n",
    "        print (\"Number of anomalous clusters : {}\".format(len(filtered_clusters)))\n",
    "        avg_label_score = []\n",
    "        for c in filtered_clusters:\n",
    "            df_fil = df_data[df_data['cluster_label']==c]\n",
    "            if len(df_fil) == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = sum(df_fil['label'])/len(df_fil)\n",
    "            avg_label_score.append(score)\n",
    "            binary_label_purity = sum(df_fil['binary_label'])/len(df_fil)\n",
    "            if score >= 3.5:\n",
    "                class1_count += 1\n",
    "                class1_purity.append(binary_label_purity)\n",
    "            elif score < 3.5 and score >= 2:\n",
    "                class2_count += 1\n",
    "                class2_purity.append(binary_label_purity)\n",
    "            else:\n",
    "                class3_count += 1\n",
    "                class3_purity.append(binary_label_purity)\n",
    "\n",
    "\n",
    "    #     mean_score = sum(avg_label_score)/len(avg_label_score)\n",
    "    #     print (\"Mean Trafficking score for ads : {}\".format(mean_score))\n",
    "\n",
    "        pre_cl = np.array(post_merging_metric['avg_label_scores'])\n",
    "#         avg_l = np.array(avg_label_score)\n",
    "        print (\"\\n\")\n",
    "        print (\"|           |   Total   | Filtered |  Avg Purity  |\")\n",
    "        print (\"|  Class 1  |    {}    |    {}    |     {}     |\".format(len(np.where(pre_cl>=3.5)[0]), class1_count, round(sum(class1_purity)/class1_count,2)))\n",
    "        print (\"|  Class 2  |    {}    |    {}    |     {}     |\".format(len(np.where((pre_cl>=2.0) & (pre_cl<3.5))[0]), class2_count, round(sum(class2_purity)/class2_count,2)))\n",
    "        print (\"|  Class 3  |    {}    |    {}    |     {}     |\".format(len(np.where(pre_cl<=2.0)[0]), class3_count, round(sum(class3_purity)/class3_count, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "#     max_index = 1\n",
    "#     max_diff = densities[1] - densities[0]\n",
    "\n",
    "settings = [{'name_filter': False, 'threshold_filter' : True},\n",
    "           {'name_filter': True, 'threshold_filter': False},\n",
    "           {'name_filter' : True, 'threshold_filter': True}]\n",
    "\n",
    "filter_metrics = ['WEIGHTED_FRAUDAR', 'WEIGHTED_OUTER_MODULARITY', 'WEIGHTED_OUTER_EDGE_PERCENTAGE', 'WEIGHTED_DENSITY', \n",
    "                 'UNWEIGHTED_FRAUDAR', 'UNWEIGHTED_OUTER_MODULARITY', 'UNWEIGHTED_OUTER_EDGE_PERCENTAGE', 'UNWEIGHTED_DENSITY']\n",
    "for setting in settings:\n",
    "    if setting['threshold_filter'] is True:\n",
    "        for filter_param in filter_metrics:\n",
    "            class1_purity = []\n",
    "            class2_purity = []\n",
    "            class3_purity = []\n",
    "            class1_count = 0\n",
    "            class2_count = 0\n",
    "            class3_count = 0\n",
    "            print ('==============Anomally Metric: {}, Name Filtering: {}======================'.format(filter_param, \n",
    "                                                                                                        setting['name_filter']))\n",
    "            metric = post_merging_metric[METRIC_MAP[filter_param]]\n",
    "            metric_cutoff = METRIC_CUTOFF_MAP[filter_param]\n",
    "            cluster_counts = post_merging_metric['cluster_counts']\n",
    "            clusters = list(set(df_data['cluster_label']))\n",
    "\n",
    "            filtered_clusters, filtered_cluster_metric = execute_filtering(metric, metric_cutoff, \n",
    "                                                                           cluster_counts, clusters, \n",
    "                                                                           binary_true_labels,setting['name_filter'], \n",
    "                                                                           setting['threshold_filter'], filter_param)\n",
    "            print (\"Number of anomalous clusters : {}\".format(len(filtered_clusters)))\n",
    "            avg_label_score = []\n",
    "            for c in filtered_clusters:\n",
    "                df_fil = df_data[df_data['cluster_label']==c]\n",
    "                if len(df_fil) == 0:\n",
    "                    score = 0\n",
    "                else:\n",
    "                    score = sum(df_fil['label'])/len(df_fil)\n",
    "                avg_label_score.append(score)\n",
    "                binary_label_purity = sum(df_fil['binary_label'])/len(df_fil)\n",
    "                if score >= 3.5:\n",
    "                    class1_count += 1\n",
    "                    class1_purity.append(binary_label_purity)\n",
    "                elif score < 3.5 and score >= 2:\n",
    "                    class2_count += 1\n",
    "                    class2_purity.append(binary_label_purity)\n",
    "                else:\n",
    "                    class3_count += 1\n",
    "                    class3_purity.append(binary_label_purity)\n",
    "\n",
    "            pre_cl = np.array(post_merging_metric['avg_label_scores'])\n",
    "\n",
    "            print (\"\\n\")\n",
    "            print (\"|           |   Total   | Filtered |  Avg Purity  |\")\n",
    "            print (\"|  Class 1  |    {}    |    {}    |     {}     |\".format(len(np.where(pre_cl>=3.5)[0]), class1_count, round(sum(class1_purity)/class1_count,2)))\n",
    "            print (\"|  Class 2  |    {}    |    {}    |     {}     |\".format(len(np.where((pre_cl>=2.0) & (pre_cl<3.5))[0]), class2_count, round(sum(class2_purity)/class2_count,2)))\n",
    "            print (\"|  Class 3  |    {}    |    {}    |     {}     |\".format(len(np.where(pre_cl<=2.0)[0]), class3_count, round(sum(class3_purity)/class3_count, 2)))\n",
    "    \n",
    "    else:\n",
    "        print (\"===================Only Name Filter===========================\")\n",
    "        filtered_clusters, filtered_cluster_metric = execute_filtering(metric, metric_cutoff, cluster_counts, clusters, \n",
    "                                                                           binary_true_labels,setting['name_filter'], setting['threshold_filter'], filter_param)\n",
    "        print (\"Number of anomalous clusters : {}\".format(len(filtered_clusters)))\n",
    "        class1_purity = []\n",
    "        class2_purity = []\n",
    "        class3_purity = []\n",
    "        class1_count = 0\n",
    "        class2_count = 0\n",
    "        class3_count = 0\n",
    "        avg_label_score = []\n",
    "        for c in filtered_clusters:\n",
    "            df_fil = df_data[df_data['cluster_label']==c]\n",
    "            if len(df_fil) == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = sum(df_fil['label'])/len(df_fil)\n",
    "            avg_label_score.append(score)\n",
    "            binary_label_purity = sum(df_fil['binary_label'])/len(df_fil)\n",
    "            if score >= 3.5:\n",
    "                class1_count += 1\n",
    "                class1_purity.append(binary_label_purity)\n",
    "            elif score < 3.5 and score >= 2:\n",
    "                class2_count += 1\n",
    "                class2_purity.append(binary_label_purity)\n",
    "            else:\n",
    "                class3_count += 1\n",
    "                class3_purity.append(binary_label_purity)\n",
    "\n",
    "    #     mean_score = sum(avg_label_score)/len(avg_label_score)\n",
    "    #     print (\"Mean Trafficking score for ads : {}\".format(mean_score))\n",
    "\n",
    "        pre_cl = np.array(post_merging_metric['avg_label_scores'])\n",
    "\n",
    "        print (\"\\n\")\n",
    "        print (\"|           |   Total   | Filtered |  Avg Purity  |\")\n",
    "        print (\"|  Class 1  |    {}    |    {}    |     {}     |\".format(len(np.where(pre_cl>=3.5)[0]), class1_count, round(sum(class1_purity)/class1_count,2)))\n",
    "        print (\"|  Class 2  |    {}    |    {}    |     {}     |\".format(len(np.where((pre_cl>=2.0) & (pre_cl<3.5))[0]), class2_count, round(sum(class2_purity)/class2_count,2)))\n",
    "        print (\"|  Class 3  |    {}    |    {}    |     {}     |\".format(len(np.where(pre_cl<=2.0)[0]), class3_count, round(sum(class3_purity)/class3_count, 2)))\n",
    "    \n",
    "    \n",
    "#     rcParams['figure.figsize'] = 5,5\n",
    "#     num_bins = 7\n",
    "#     n, bins, patches = plt.hist(avg_label_score, num_bins, facecolor='blue', alpha=0.5)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10,10\n",
    "plt.scatter(post_merging_metric['avg_label_scores'], post_merging_metric['weighted_outer_edge_perc_scores'], alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Unweighted Shell Modularity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_orig.to_csv(OUTPUT_FILE)\n",
    "avg_label_score = post_merging_metric['avg_label_scores']\n",
    "filtered_cluster_metric = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "cluster_prop = {}\n",
    "for cl in filtered_clusters:\n",
    "    print (cl)\n",
    "    if cl == -1:\n",
    "        continue\n",
    "    index = clusters.index(cl)\n",
    "    \n",
    "    cluster_prop[int(cl)] = {'avg_label_score' : avg_label_score[index], \n",
    "                             'cluster_metric' : filtered_cluster_metric[index],\n",
    "                             'word_list' : ', '.join(bigrams_dict[int(cl)]),\n",
    "                            'top_bigrams' : ', '.join(top_bigrams[int(cl)])}\n",
    "with open(OUTPUT_PROP_FILE, 'w') as f:\n",
    "    json.dump(cluster_prop, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "df_data.reset_index(drop=True, inplace=True)\n",
    "labels = post_merging_metric['cluster_label']\n",
    "clusters = list(set(df_data['cluster_label']))\n",
    "import math\n",
    "\n",
    "stop_words = set(list(stop_words) + ['height', 'weight', 'age', 'am', 'lbs', 'years', 'year'])\n",
    "content = df_data['content_p']\n",
    "print (len(content))\n",
    "content = content.replace(np.nan, '', regex=True)\n",
    "vectorizer1 = TfidfVectorizer(lowercase=True, ngram_range=(2,2))\n",
    "b_mat = vectorizer1.fit_transform(content)\n",
    "features_col = vectorizer1.get_feature_names()\n",
    "\n",
    "top_bigrams = {}\n",
    "bigrams_dict = {}\n",
    "\n",
    "for l in clusters:\n",
    "    \n",
    "    cluster_idx = np.argwhere(labels == l).reshape(-1)\n",
    "#     clusters.append(l)\n",
    "#     shell_subgraph, outer_subgraph, core_subgraph = get_all_subgraphs(bigram_matrix, cluster_idx)\n",
    "    \n",
    "    if l== -1:\n",
    "        continue\n",
    "#     cluster_ajac = (bigram_matrix[cluster_idx,:][:])\n",
    "    df_filt = df_data[df_data['cluster_label']== l]\n",
    "    local_content = list(df_filt['content_p'])\n",
    "#     print (content.shape)\n",
    "    # Create and fit the LDA model\n",
    "    count_vectorizer = TfidfVectorizer(ngram_range=(2,2), use_idf=False)\n",
    "    count_data = count_vectorizer.fit_transform(local_content).todense()\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=2)\n",
    "    local_vecs = svd.fit_transform(count_data)\n",
    "    \n",
    "    ajac_big = (b_mat[cluster_idx,:][:]).todense()\n",
    "#     top_bigram_ajac = (count_data[cluster_idx,:][:]).todense()\n",
    "    top_bigram_features = count_vectorizer.get_feature_names()\n",
    "    \n",
    "    bigram_sums = np.sum(np.asarray(count_data), axis=0)\n",
    "#     print (bigram_sums)\n",
    "    bigram_index = bigram_sums.argsort()[:-20:-1]\n",
    "#     print (bigram_index)\n",
    "    n_top_words = 20\n",
    "    number_topics = 1\n",
    "    lda = LDA(n_components=number_topics)\n",
    "    lda.fit(ajac_big)\n",
    "    bigrams_list = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "#         print(\"\\nTopic #%d:\" % topic_idx)\n",
    "#         print(\", \".join([words[i]\n",
    "#                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        word_list = [features_col[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    bigrams_dict[l] = word_list\n",
    "    \n",
    "    \n",
    "    for i in bigram_index:\n",
    "#         print (bigram_sums[i])\n",
    "        f = top_bigram_features[i]\n",
    "        if f.split()[0] in stop_words or f.split()[1] in stop_words or len(f.split()[0]) < 3 or \\\n",
    "        len(f.split()[1]) < 3 or f.split()[0] == f.split()[1]:\n",
    "            continue\n",
    "        bigrams_list.append(f)\n",
    "        if len(bigrams_list) == 5:\n",
    "            break\n",
    "        \n",
    "#     print (bigrams_list)\n",
    "    top_bigrams[l] = bigrams_list\n",
    "    print (bigrams_list)\n",
    "#     for ix, row in df_filt.iterrows():\n",
    "#         print ('---------------------------------')\n",
    "#         print (row['body'])\n",
    "#     break\n",
    "    if l % 50 == 0:\n",
    "        print (l)\n",
    "\n",
    "# original_labels = labels.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(filtered_clusters))\n",
    "print (len(clusters))\n",
    "pre_cl = np.array(post_merging_metric['avg_label_scores'])\n",
    "print (len(np.where(pre_cl>=3.5)[0]), )\n",
    "avg_l = np.array(avg_label_score)\n",
    "print (len(np.where(avg_l>=3.5)[0]))\n",
    "\n",
    "\n",
    "print (\"     {}     |    {}    |\".format(len(np.where(pre_cl>=3.5)[0]), len(np.where(avg_l>=3.5)[0])))\n",
    "print (\"     {}     |    {}    |\".format(len(np.where((pre_cl>=2.0) & (pre_cl<=3.5))[0]), len(np.where((avg_l>=2.0) & (avg_l<=3.5))[0])))\n",
    "print (\"     {}     |    {}    |\".format(len(np.where(pre_cl<=2.0)[0]), len(np.where(avg_l<2.0)[0])))\n",
    "\n",
    "# print (\"     {}     |    {}    |\".format(len(np.where(pre_cl>=3.5)[0])), len(np.where(avg_l>=3.5)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_clusters = []\n",
    "\n",
    "for ind, c in enumerate(clusters):\n",
    "    df_fil = df_data[df_data['cluster_label']==c]\n",
    "    names = [x.lower() if type(x) == type('') else None for x in df_fil['Name'].unique()]\n",
    "    if math.nan in names:\n",
    "        names.remove(math.nan)\n",
    "    if None in names:\n",
    "        names.remove(None)\n",
    "    names = list(set(names))\n",
    "    if len(names)>1:\n",
    "#         print (names)\n",
    "        filtered_clusters.append(c)\n",
    "#         filtered_cluster_metric.append(anomaly_cluster_metric[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_data['cluster_label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['cluster_label'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "eigen_ratios = post_merging_metric['eigen_ratios']\n",
    "\n",
    "eig_w_den_pscore = spearmanr(eigen_ratios, post_merging_metric['weighted_cluster_density'])[0]\n",
    "eig_uw_den_pscore = spearmanr(eigen_ratios, post_merging_metric['unweighted_cluster_density'])[0]\n",
    "eig_w_fraud_pscore = spearmanr(eigen_ratios,post_merging_metric['weighted_fraudar_scores'])[0]\n",
    "eig_uw_fraud_pscore = spearmanr(eigen_ratios, post_merging_metric['unweighted_fraudar_scores'])[0]\n",
    "eig_w_edge_out_pscore = spearmanr(eigen_ratios, post_merging_metric['weighted_outer_edge_perc_scores'])[0]\n",
    "eig_uw_edge_out_pscore = spearmanr(eigen_ratios, post_merging_metric['unweighted_outer_edge_perc_scores'])[0]\n",
    "eig_w_edge_shell_pscore = spearmanr(eigen_ratios, post_merging_metric['weighted_shell_edge_perc_scores'])[0]\n",
    "eig_uw_edge_shell_pscore = spearmanr(eigen_ratios, post_merging_metric['unweighted_shell_edge_perc_scores'])[0]\n",
    "eig_w_mod_out_pscore = spearmanr(eigen_ratios, post_merging_metric['weighted_outer_modularity_scores'])[0]\n",
    "eig_uw_mod_out_pscore = spearmanr(eigen_ratios, post_merging_metric['unweighted_outer_modularity_scores'])[0]\n",
    "eig_w_mod_shell_pscore = spearmanr(eigen_ratios, post_merging_metric['weighted_shell_modularity_scores'])[0]\n",
    "eig_uw_mod_shell_pscore = spearmanr(eigen_ratios, post_merging_metric['unweighted_shell_modularity_scores'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Spearman Correlation | Eigen Ratio |     Weighted Cluster Density      | {0:.2f} \".format(eig_w_den_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |    UnWeighted Cluster Density     | {0:.2f} \".format(eig_uw_den_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |      Weighted Fraudar Score       | {0:.2f} \".format(eig_w_fraud_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |     UnWeighted Fraudar Score      | {0:.2f} \".format(eig_uw_fraud_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |     Weighted Outer Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |    UnWeighted Outer Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |     Weighted Shell Edge Perc      | {0:.2f} \".format(eig_w_edge_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |    UnWeighted Shell Edge Perc     | {0:.2f} \".format(eig_uw_edge_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |  Weighted Outer Modularity Score  | {0:.2f} \".format(eig_w_mod_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio | UnWeighted Outer Modularity Score | {0:.2f} \".format(eig_uw_mod_out_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio |  Weighted Shell Modularity Score  | {0:.2f} \".format(eig_w_mod_shell_pscore))\n",
    "print (\"Spearman Correlation | Eigen Ratio | UnWeighted Shell Modularity Score | {0:.2f} \".format(eig_uw_mod_shell_pscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_label_scores = []\n",
    "max_label_scores = []\n",
    "sum_label_scores = []\n",
    "avg_binary_scores = []\n",
    "max_binary_scores = []\n",
    "sum_binary_scores = []\n",
    "df_data['cluster_label'] = labels\n",
    "df_data['probabilities'] = probs\n",
    "df_data['binary_label'] = binary_true_labels\n",
    "for c in unique_labels:\n",
    "    df_fil = df_data[df_data['cluster_label']==c]\n",
    "    \n",
    "    max_label_scores.append(max(df_fil['label']))\n",
    "    avg_label_scores.append(sum(df_fil['label'])/len(df_fil['label']))\n",
    "    sum_label_scores.append(sum(df_fil['label']))\n",
    "    \n",
    "    max_binary_scores.append(max(df_fil['binary_label']))\n",
    "    avg_binary_scores.append(sum(df_fil['binary_label'])/len(df_fil['binary_label']))\n",
    "    sum_binary_scores.append(sum(df_fil['binary_label']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_cluster_index = clusters.index(-1)\n",
    "avg_label_scores[noisy_cluster_index] = 0\n",
    "max_label_scores[noisy_cluster_index] = 0\n",
    "sum_label_scores[noisy_cluster_index] = 0\n",
    "\n",
    "max_binary_scores[noisy_cluster_index] = 0\n",
    "avg_binary_scores[noisy_cluster_index] = 0\n",
    "sum_binary_scores[noisy_cluster_index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = spearmanr(eigen_ratios, avg_scores)[0]\n",
    "print (score)\n",
    "\n",
    "print (\"Eigen Ratios |  Average Label Scores  | {} \".format(spearmanr(eigen_ratios, avg_label_scores)[0]))\n",
    "print (\"Eigen Ratios |    Max Label Scores    | {} \".format(spearmanr(eigen_ratios, max_label_scores)[0]))\n",
    "print (\"Eigen Ratios |    Sum Label Scores    | {} \".format(spearmanr(eigen_ratios, sum_label_scores)[0]))\n",
    "print (\"Eigen Ratios |    Max Binary Scores   | {} \".format(spearmanr(eigen_ratios, max_binary_scores)[0]))\n",
    "print (\"Eigen Ratios |  Average Binary Scores | {} \".format(spearmanr(eigen_ratios, avg_binary_scores)[0]))\n",
    "print (\"Eigen Ratios |   Sum Binary Scores    | {} \".format(spearmanr(eigen_ratios, sum_binary_scores)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Spearman Correlation | Average Label Score |       Weighted Density         | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_cluster_density'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |      Unweighted Density        | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_cluster_density'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |       Weighted Fraudar         | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_fraudar_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |      Unweighted Fraudar        | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_fraudar_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |    Weighted Outer Edge Perc    | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |   Unweighted Outer Edge Perc   | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |    Weighted Shell Edge Perc    | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |   Unweighted Shell Edge Perc   | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |    Weighted Outer Modularity   | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |   Unweighted Outer Modularity  | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |    Weighted Shell Modularity   | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Label Score |   Unweighted Shell Modularity  | {0:.2f} \".format(spearmanr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_modularity_scores'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "print (\"Pearson Correlation | Average Label Score |       Weighted Density         | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_cluster_density'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |      Unweighted Density        | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_cluster_density'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |       Weighted Fraudar         | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_fraudar_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |      Unweighted Fraudar        | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_fraudar_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |    Weighted Outer Edge Perc    | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |   Unweighted Outer Edge Perc   | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |    Weighted Shell Edge Perc    | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |   Unweighted Shell Edge Perc   | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |    Weighted Outer Modularity   | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |   Unweighted Outer Modularity  | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |    Weighted Shell Modularity   | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Label Score |   Unweighted Shell Modularity  | {0:.2f} \".format(pearsonr(post_merging_metric['avg_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_modularity_scores'])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Spearman Correlation | Average Binary Score |       Weighted Density         | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_cluster_density'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |      Unweighted Density        | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_cluster_density'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |       Weighted Fraudar         | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_fraudar_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |      Unweighted Fraudar        | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_fraudar_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |    Weighted Outer Edge Perc    | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |   Unweighted Outer Edge Perc   | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |    Weighted Shell Edge Perc    | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |   Unweighted Shell Edge Perc   | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |    Weighted Outer Modularity   | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |   Unweighted Outer Modularity  | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |    Weighted Shell Modularity   | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Average Binary Score |   Unweighted Shell Modularity  | {0:.2f} \".format(spearmanr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_modularity_scores'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "print (\"Pearson Correlation | Average Binary Score |       Weighted Density         | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_cluster_density'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |      Unweighted Density        | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_cluster_density'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |       Weighted Fraudar         | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_fraudar_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |      Unweighted Fraudar        | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_fraudar_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |    Weighted Outer Edge Perc    | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |   Unweighted Outer Edge Perc   | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |    Weighted Shell Edge Perc    | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |   Unweighted Shell Edge Perc   | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |    Weighted Outer Modularity   | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |   Unweighted Outer Modularity  | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |    Weighted Shell Modularity   | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Average Binary Score |   Unweighted Shell Modularity  | {0:.2f} \".format(pearsonr(post_merging_metric['avg_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_modularity_scores'])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Spearman Correlation | Max Label Score |       Weighted Density         | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_cluster_density'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |      Unweighted Density        | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_cluster_density'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |       Weighted Fraudar         | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_fraudar_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |      Unweighted Fraudar        | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_fraudar_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |    Weighted Outer Edge Perc    | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |   Unweighted Outer Edge Perc   | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |    Weighted Shell Edge Perc    | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |   Unweighted Shell Edge Perc   | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |    Weighted Outer Modularity   | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |   Unweighted Outer Modularity  | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |    Weighted Shell Modularity   | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Label Score |   Unweighted Shell Modularity  | {0:.2f} \".format(spearmanr(post_merging_metric['max_label_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_modularity_scores'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Spearman Correlation | Max Binary Score |       Weighted Density         | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_cluster_density'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |      Unweighted Density        | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_cluster_density'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |       Weighted Fraudar         | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_fraudar_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |      Unweighted Fraudar        | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_fraudar_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |    Weighted Outer Edge Perc    | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |   Unweighted Outer Edge Perc   | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |    Weighted Shell Edge Perc    | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |   Unweighted Shell Edge Perc   | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |    Weighted Outer Modularity   | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |   Unweighted Outer Modularity  | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |    Weighted Shell Modularity   | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_modularity_scores'])[0]))\n",
    "print (\"Spearman Correlation | Max Binary Score |   Unweighted Shell Modularity  | {0:.2f} \".format(spearmanr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_modularity_scores'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "print (\"Pearson Correlation | Max Binary Score |       Weighted Density         | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_cluster_density'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |      Unweighted Density        | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_cluster_density'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |       Weighted Fraudar         | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_fraudar_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |      Unweighted Fraudar        | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_fraudar_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |    Weighted Outer Edge Perc    | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |   Unweighted Outer Edge Perc   | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |    Weighted Shell Edge Perc    | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |   Unweighted Shell Edge Perc   | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_edge_perc_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |    Weighted Outer Modularity   | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_outer_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |   Unweighted Outer Modularity  | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_outer_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |    Weighted Shell Modularity   | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['weighted_shell_modularity_scores'])[0]))\n",
    "print (\"Pearson Correlation | Max Binary Score |   Unweighted Shell Modularity  | {0:.2f} \".format(pearsonr(post_merging_metric['max_binary_scores'], \n",
    "                                                                                                                 post_merging_metric['unweighted_shell_modularity_scores'])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(unweighted_shell_edge_perc_scores, unweighted_shell_modularity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 5,5\n",
    "num_bins = 7\n",
    "n, bins, patches = plt.hist(avg_label_scores, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.title(\"Average Label Scores Histogram\")\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(max_label_scores, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.title(\"Max Label Scores Histogram\")\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(sum_label_scores, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.title(\"Sum Label Scores Histogram\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print (\"########## AVERAGE LABEL SCORES ################\")\n",
    "plt.scatter(max_label_scores, weighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Weighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, unweighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs UnWeighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, weighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Weighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, unweighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs UnWeighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, weighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Weighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, unweighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Unweighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, weighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Weighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, unweighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Unweighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, weighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Weighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, unweighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Unweighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(max_label_scores, weighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Weighted Shell Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_label_scores, unweighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Max Label Scores vs Unweighted Shell Modularity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print (\"########## AVERAGE LABEL SCORES ################\")\n",
    "plt.scatter(sum_label_scores, weighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Weighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, unweighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs UnWeighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, weighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Weighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, unweighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs UnWeighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, weighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Weighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, unweighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Unweighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, weighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Weighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, unweighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Unweighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, weighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Weighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, unweighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Unweighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(sum_label_scores, weighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Weighted Shell Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_label_scores, unweighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Sum Label Scores vs Unweighted Shell Modularity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print (\"########## AVERAGE LABEL SCORES ################\")\n",
    "plt.scatter(avg_binary_scores, weighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Weighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, unweighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs UnWeighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, weighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Weighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, unweighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs UnWeighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, weighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Weighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, unweighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Unweighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, weighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Weighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, unweighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Unweighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, weighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Weighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, unweighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Unweighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(avg_binary_scores, weighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Weighted Shell Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(avg_binary_scores, unweighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Average Binary Scores vs Unweighted Shell Modularity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"########## AVERAGE LABEL SCORES ################\")\n",
    "plt.scatter(max_binary_scores, weighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Weighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, unweighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs UnWeighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, weighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Weighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, unweighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs UnWeighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, weighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Weighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, unweighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Unweighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, weighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Weighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, unweighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Unweighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, weighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Weighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, unweighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Unweighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, weighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Weighted Shell Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(max_binary_scores, unweighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Max Binary Scores vs Unweighted Shell Modularity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"########## AVERAGE LABEL SCORES ################\")\n",
    "plt.scatter(sum_binary_scores, weighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Weighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, unweighted_cluster_density, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs UnWeighted Cluster Density\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, weighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Weighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, unweighted_fraudar_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs UnWeighted Fraudar Scores\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, weighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Weighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, unweighted_outer_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Unweighted Outer Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, weighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Weighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, unweighted_shell_edge_perc_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Unweighted Shell Edge Perc\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, weighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Weighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, unweighted_outer_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Unweighted Outer Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, weighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Weighted Shell Modularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(sum_binary_scores, unweighted_shell_modularity_scores, alpha=0.2)\n",
    "plt.title(\"Sum Binary Scores vs Unweighted Shell Modularity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(avg_scores, weighted_fraudar_scores, alpha=0.2)\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(max_scores, weighted_fraudar_scores, alpha=0.2)\n",
    "# plt.show()\n",
    "# percentage of clusters in quadrant 4\n",
    "# new definition for edge percentage\n",
    "# look into clusters with high label but low anomaly score\n",
    "# correct modularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_ratios[clusters.index(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df_data['content_p']\n",
    "content = content.replace(np.nan, '', regex=True)\n",
    "vectorizer1 = TfidfVectorizer(lowercase=True, ngram_range=(2,2))\n",
    "b_mat = vectorizer1.fit_transform(content)\n",
    "features_col = vectorizer1.get_feature_names()\n",
    "\n",
    "\n",
    "cluster_counts = []\n",
    "\n",
    "weighted_cluster_density = []\n",
    "unweighted_cluster_density = []\n",
    "unweighted_fraudar_scores = []\n",
    "weighted_fraudar_scores = []\n",
    "unweighted_outer_edge_perc_scores = []\n",
    "weighted_outer_edge_perc_scores = []\n",
    "unweighted_shell_edge_perc_scores = []\n",
    "weighted_shell_edge_perc_scores = []\n",
    "weighted_outer_modularity_scores = []\n",
    "unweighted_outer_modularity_scores = []\n",
    "weighted_shell_modularity_scores = []\n",
    "unweighted_shell_modularity_scores = []\n",
    "bigrams_dict = {}\n",
    "total_edges_unweighted = np.count_nonzero(bigram_matrix)\n",
    "sil_scores = []\n",
    "total_edges_weighted = np.sum(bigram_matrix)\n",
    "eigen_ratios = []\n",
    "for l in clusters:\n",
    "    \n",
    "    cluster_idx = np.argwhere(labels == l).reshape(-1)\n",
    "#     clusters.append(l)\n",
    "    shell_subgraph, outer_subgraph, core_subgraph = get_all_subgraphs(bigram_matrix, cluster_idx)\n",
    "    \n",
    "    if l== -1:\n",
    "        weighted_cluster_density.append(0)\n",
    "        unweighted_cluster_density.append(0)\n",
    "        weighted_fraudar_scores.append(0)\n",
    "        unweighted_fraudar_scores.append(0)\n",
    "        weighted_outer_edge_perc_scores.append(0)\n",
    "        unweighted_outer_edge_perc_scores.append(0)\n",
    "        weighted_shell_edge_perc_scores.append(0)\n",
    "        unweighted_shell_edge_perc_scores.append(0)\n",
    "    #     fraudar_scores.append(calculate_fradaur_score(cluster_ajac))\n",
    "        unweighted_outer_modularity_scores.append(0)\n",
    "        weighted_outer_modularity_scores.append(0)\n",
    "        unweighted_shell_modularity_scores.append(0)\n",
    "        weighted_shell_modularity_scores.append(0)\n",
    "        cluster_counts.append(math.log(len(cluster_idx), 10)) \n",
    "        eigen_ratios.append(0)\n",
    "        continue\n",
    "#     cluster_ajac = (bigram_matrix[cluster_idx,:][:])\n",
    "    df_filt = df_data[df_data['cluster_label']== l]\n",
    "    local_content = list(df_filt['content_p'])\n",
    "#     print (content.shape)\n",
    "    # Create and fit the LDA model\n",
    "    count_vectorizer = TfidfVectorizer(ngram_range=(2,2), use_idf=False)\n",
    "    count_data = count_vectorizer.fit_transform(local_content)\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=2)\n",
    "    local_vecs = svd.fit_transform(count_data)\n",
    "    w = svd.singular_values_\n",
    "    eigen_rat = w[1]/w[0]\n",
    "#     eigen_rat = eigen_rat * eigen_rat\n",
    "    eigen_ratios.append(eigen_rat)\n",
    "    \n",
    "    ajac_big = (b_mat[cluster_idx,:][:]).todense()\n",
    "    bigram_sums = np.max(np.asarray(ajac_big), axis=0)\n",
    "#     print (bigram_sums)\n",
    "#     bigram_index = bigram_sums.argsort()[::-1][:50]\n",
    "#     print (bigram_index)\n",
    "    n_top_words = 20\n",
    "    number_topics = 1\n",
    "    lda = LDA(n_components=number_topics)\n",
    "    lda.fit(ajac_big)\n",
    "    bigrams_list = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "#         print(\"\\nTopic #%d:\" % topic_idx)\n",
    "#         print(\", \".join([words[i]\n",
    "#                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        word_list = [features_col[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    bigrams_dict[l] = word_list\n",
    "    \n",
    "    \n",
    "#     for i in bigram_index:\n",
    "# #         print (bigram_sums[i])\n",
    "#         bigrams_list.append(features_col[i])\n",
    "# #     print (bigrams_list)\n",
    "#     bigrams_dict[l] = bigrams_list\n",
    "    \n",
    "    weighted_cluster_density.append(calculate_weighted_density(core_subgraph))\n",
    "    unweighted_cluster_density.append(calculate_unweighted_density(core_subgraph))\n",
    "    weighted_fraudar_scores.append(calculate_weighted_fraudar_score(core_subgraph))\n",
    "    unweighted_fraudar_scores.append(calculate_unweighted_fraudar_score(core_subgraph))\n",
    "    weighted_outer_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "    unweighted_outer_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "    weighted_shell_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "    unweighted_shell_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "    unweighted_outer_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, outer_subgraph, total_edges_unweighted))\n",
    "    weighted_outer_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, outer_subgraph, total_edges_weighted))\n",
    "    unweighted_shell_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, shell_subgraph, total_edges_unweighted))\n",
    "    weighted_shell_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, shell_subgraph, total_edges_weighted))\n",
    "    cluster_counts.append(math.log(len(cluster_idx), 10)) \n",
    "#     break\n",
    "#     vert_col = len(nonzero_col) - len(nonzero_col.count(0))\n",
    "#     print (vert_row, vert_col)\n",
    "#     print (edge_weight)\n",
    "    \n",
    "#     cluster_ajac_density = nx.density(bipartite.from_biadjacency_matrix(cluster_ajac))\n",
    "    if l % 50 == 0:\n",
    "        print (l)\n",
    "original_labels = labels.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [x for x in range(len(clusters))]\n",
    "plt.scatter(x_axis, eigen_ratios, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_np = np.array(eigen_ratios)\n",
    "print(np.where(eigen_np>0.8)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_matrix = np.asarray(bigram_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noise = df_data[df_data['cluster_label'].isin(clusters_nonhomogenous)]\n",
    "df_noise.to_csv('../data/noisy_rerun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "avg_sorted_scores, weighted_cluster_density_sorted, weighted_fraudar_scores_sorted, weighted_edge_perc_scores_sorted, weighted_modularity_scores_sorted = \\\n",
    "    zip(*sorted(zip(avg_scores, weighted_cluster_density, weighted_fraudar_scores, weighted_edge_perc_scores, weighted_modularity_scores)))\n",
    "plt.plot(weighted_cluster_density_sorted, label = 'density')\n",
    "plt.plot(weighted_fraudar_scores_sorted, label= 'fraudar')\n",
    "plt.plot(weighted_edge_perc_scores_sorted, label= 'edge percentage')\n",
    "plt.plot(weighted_modularity_scores_sorted, label= 'modularity')\n",
    "plt.title(\"WEIGHTED METRICS sorted by label score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "avg_sorted_scores, unweighted_cluster_density_sorted, unweighted_fraudar_scores_sorted, unweighted_edge_perc_scores_sorted, unweighted_modularity_scores_sorted = \\\n",
    "    zip(*sorted(zip(avg_scores, unweighted_cluster_density, unweighted_fraudar_scores, unweighted_edge_perc_scores, unweighted_modularity_scores)))\n",
    "plt.plot(unweighted_cluster_density_sorted, label = 'density')\n",
    "plt.plot(unweighted_fraudar_scores_sorted, label= 'fraudar')\n",
    "plt.plot(unweighted_edge_perc_scores_sorted, label= 'edge percentage')\n",
    "plt.plot(weighted_modularity_scores_sorted, label= 'modularity')\n",
    "plt.title(\"Unweighted Metrics sorted by label score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "cluster_counts_sorted, weighted_cluster_density_sorted, weighted_fraudar_scores_sorted, weighted_edge_perc_scores_sorted, weighted_modularity_scores_sorted = \\\n",
    "    zip(*sorted(zip(cluster_counts, weighted_cluster_density, weighted_fraudar_scores, weighted_edge_perc_scores, weighted_modularity_scores)))\n",
    "plt.plot(weighted_cluster_density_sorted, label = 'density', linestyle='None', marker='o')\n",
    "plt.plot(weighted_fraudar_scores_sorted, label= 'fraudar', linestyle='None', marker='o')\n",
    "plt.plot(weighted_edge_perc_scores_sorted, label= 'edge percentage', linestyle='None', marker='o')\n",
    "plt.plot(weighted_modularity_scores_sorted, label= 'modularity', linestyle='None', marker='o')\n",
    "plt.title(\"WEIGHTED METRICS vs cluster counts\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "cluster_counts_sorted, unweighted_cluster_density_sorted, unweighted_fraudar_scores_sorted, unweighted_edge_perc_scores_sorted, unweighted_modularity_scores_sorted = \\\n",
    "    zip(*sorted(zip(cluster_counts_sorted, unweighted_cluster_density, unweighted_fraudar_scores, unweighted_edge_perc_scores, unweighted_modularity_scores)))\n",
    "plt.plot(unweighted_cluster_density_sorted, label = 'density', linestyle='None', marker='o')\n",
    "plt.plot(unweighted_fraudar_scores_sorted, label= 'fraudar', linestyle='None', marker='o')\n",
    "plt.plot(unweighted_edge_perc_scores_sorted, label= 'edge percentage', linestyle='None', marker='o')\n",
    "plt.plot(unweighted_modularity_scores_sorted, label= 'modularity', linestyle='None', marker='o')\n",
    "plt.title(\"Unweighted Metrics vs cluster counts\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels = labels.copy()\n",
    "noise_labels = np.where(original_labels==-1)[0]\n",
    "print (len(noise_labels))\n",
    "total_index = list(range(len(encoded_vecs)))\n",
    "# print(list(set(total_index)- set(noise_labels)))\n",
    "rem_index = list(set(total_index)- set(noise_labels))\n",
    "# print (rem_index)\n",
    "original_labels = list(original_labels)\n",
    "non_noisy_labels = [original_labels[i] for i in rem_index]\n",
    "non_noisy_vecs = encoded_vecs[rem_index,:][:]\n",
    "# original_labels = original_labels.reshape(-1, 1)\n",
    "# print (original_labels.shape)\n",
    "# non_noisy_labels = original_labels[:, rem_index]\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(non_noisy_vecs, non_noisy_labels, metric='euclidean')\n",
    "# sample_scores = silhouette_samples_block(non_noisy_vecs, non_noisy_labels, metric='euclidean')\n",
    "# print (sample_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cluster_density, fraudar_scores, cluster_counts, clusters = zip(*sorted(zip(cluster_density, fraudar_scores, cluster_counts, clusters)))\n",
    "avg_scores, modularity_scores, fraudar_scores, cluster_density,cluster_counts, clusters = zip(*sorted(zip(avg_scores, modularity_scores,fraudar_scores, cluster_density, cluster_counts, clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rcParams['figure.figsize'] = 20,10\n",
    "# plt.plot(cluster_density)\n",
    "# plt.title(\"CLUSTER DENSITIES\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(fraudar_scores)\n",
    "# plt.title(\"FRAUDAR\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(modularity_scores)\n",
    "# plt.title(\"MODULARITY\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(avg_scores)\n",
    "# plt.title(\"AVERAGE SCORE\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(filtered_clusters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 5,5\n",
    "num_bins = 7\n",
    "n, bins, patches = plt.hist(avg_scores, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cluster_metric, avg_label_score, filtered_clusters = zip(*sorted(zip(filtered_cluster_metric, avg_label_score, filtered_clusters)))\n",
    "filtered_cluster_metric = list(reversed(filtered_cluster_metric))\n",
    "avg_label_score = list(reversed(avg_label_score))\n",
    "filtered_clusters = list(reversed(filtered_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = [0] * 3\n",
    "\n",
    "for index, cl in enumerate(filtered_clusters):\n",
    "    if cl == -1:\n",
    "        continue\n",
    "    df_filt = df_data[df_data['cluster_label']==cl]\n",
    "    if avg_label_score[index] < 2:\n",
    "        total_count[0] +=1\n",
    "    elif avg_label_score[index] >=2 and avg_label_score[index] <= 3.5:\n",
    "        total_count[1] +=1\n",
    "    else:\n",
    "        total_count[2] +=1\n",
    "print (total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations(words_hashmap, phrase):\n",
    "    print (phrase)\n",
    "    phrase_list = []\n",
    "    first_second = phrase.split()\n",
    "    first = first_second[0]\n",
    "    last = first_second[-1]\n",
    "    if last in words_hashmap:\n",
    "        for bigram in words_hashmap[last]:\n",
    "            if bigram in phrase:\n",
    "                continue\n",
    "            bigram_words = bigram.split()\n",
    "            new_phrase = phrase + ' ' + bigram_words[1]\n",
    "            if len(new_phrase.split()) == 10:\n",
    "                return [new_phrase]\n",
    "            phrase_list = get_combinations(words_hashmap, new_phrase)\n",
    "            phrase_list.append(new_phrase)\n",
    "    return []\n",
    "            \n",
    "def get_word_hashmap(words_list):\n",
    "    words_hashmap = {}\n",
    "    total_list = words_list.copy()\n",
    "    print (\"Creating hashmap\")\n",
    "    for words in words_list:\n",
    "#         print(words.split())\n",
    "        first, second = words.split()\n",
    "        if first in words_hashmap:\n",
    "            words_hashmap[first].append(words)\n",
    "        else:\n",
    "            words_hashmap[first] = [words]\n",
    "    return words_hashmap\n",
    "    print (\"Word hashmap creation done\")\n",
    "    for words in word_list:\n",
    "        print (words)\n",
    "        first_second = words.split()\n",
    "        first = first_second[0]\n",
    "        second = first_second[1]\n",
    "        if second in words_hashmap:\n",
    "            for bigram in words_hashmap[second]:\n",
    "                total_list.append(first + ' ' + bigram)\n",
    "    total_list = list(set(total_list))\n",
    "    \n",
    "    return total_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_array = ['grey'] * len(true_labels)\n",
    "anomaly_indices = []\n",
    "rgba_colors = np.zeros((len(true_labels),4))\n",
    "for ind, cl in enumerate(filtered_clusters):\n",
    "    if cl == -1:\n",
    "        continue\n",
    "    cluster_idx = np.argwhere(labels == cl).reshape(-1)\n",
    "    anomaly_indices += list(cluster_idx)\n",
    "rgba_colors[:, 0] = 0\n",
    "rgba_colors[:, 3] = 0.01\n",
    "print (anomaly_indices)\n",
    "for ind in anomaly_indices:\n",
    "    rgba_colors[ind, 0] = 1\n",
    "    rgba_colors[ind, 3] = 1.0\n",
    "    \n",
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.scatter(X_embedded.T[0], X_embedded.T[1], color=rgba_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(encoded_vecs, original_labels, metric='euclidean')\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    word_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(\"\\nTopic #%d:\" % topic_idx)\n",
    "#         print(\", \".join([words[i]\n",
    "#                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        word_list = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "# Tweak the two parameters below\n",
    "number_topics = 1\n",
    "number_words = 30\n",
    "\n",
    "\n",
    "# model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "count = 0\n",
    "trafficking_ads = []\n",
    "eigen_ratios = []\n",
    "# filtered_cluster_metric, avg_label_score, filtered_clusters = zip(*sorted(zip(filtered_cluster_metric, avg_label_score, filtered_clusters)))\n",
    "colors_highlight = ['1m','2m','3m','4m','5m','6m']\n",
    "cluster_word_list = []\n",
    "low_eigen_clusters = []\n",
    "for index, cl in enumerate(filtered_clusters):\n",
    "    if cl == -1:\n",
    "        cluster_word_list.append('')\n",
    "        continue\n",
    "    count += 1\n",
    "#     if count == 10:\n",
    "#         break\n",
    "    cluster_idx = np.argwhere(original_labels == cl).reshape(-1)\n",
    "   \n",
    "    ajac_big = (b_mat[cluster_idx,:][:]).todense()\n",
    "    agg_scores = np.max(np.asarray(ajac_big), axis=0)\n",
    "#     if avg_label_score[index] < 3.5:\n",
    "#         continue\n",
    "    df_filt = df_data[df_data['cluster_label']==cl]\n",
    "    df_filt = df_filt.sort_values(by=['probabilities'], ascending=False)\n",
    "    content = []\n",
    "#     for ind, row in df_filt.iterrows():\n",
    "#         c = ''\n",
    "#         if row['title'] and type(row['title']) == type(' '):\n",
    "#             c = row['title']\n",
    "#         c += row['body']\n",
    "#         c = re.sub(r'\\d+', '', c)\n",
    "#         if type(row['Name']) == type(''):\n",
    "#             name = row['Name'].split(';')\n",
    "#             for n in name:\n",
    "#                 name_regex = re.compile(re.escape(n), re.IGNORECASE)\n",
    "#                 c = name_regex.sub('', c)\n",
    "#             c = re.sub(r'[^\\x00-\\x7F]+',' ', c)\n",
    "#             cleanr = re.compile('<.*?>')\n",
    "#             c = re.sub(cleanr, '', c)\n",
    "#         content.append(c)\n",
    "    \n",
    "    content = list(df_filt['content_p'])\n",
    "#     print (content.shape)\n",
    "    # Create and fit the LDA model\n",
    "    count_vectorizer = TfidfVectorizer(ngram_range=(2,2), use_idf=False)\n",
    "    count_data = count_vectorizer.fit_transform(content)\n",
    "    lda = LDA(n_components=number_topics)\n",
    "    lda.fit(count_data)\n",
    "    col_names = count_vectorizer.get_feature_names()\n",
    "    # Print the topics found by the LDA model\n",
    "#     print(\"Topics found via LDA:\")\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=10)\n",
    "    local_vecs = svd.fit_transform(count_data)\n",
    "    w = svd.singular_values_\n",
    "    count_data = count_data.todense()\n",
    "    word_list = print_topics(lda, count_vectorizer, number_words)\n",
    "    agg_scores_local = np.max(np.asarray(count_data), axis=0)\n",
    "    tf_idf = []\n",
    "    tf_local = []\n",
    "#     print (agg_scores)\n",
    "    for word in word_list:\n",
    "        big_index = features_col.index(word)\n",
    "        tf_idf.append(agg_scores[big_index])\n",
    "        tf_local.append(agg_scores_local[col_names.index(word)])\n",
    "#     hashmap = get_word_hashmap(word_list)\n",
    "#     total_list = []\n",
    "#     for word in word_list:\n",
    "#         total_list += get_combinations(hashmap, word)\n",
    "#     print (\", \".join(word for word in total_list))\n",
    "    tf_idf, tf_local, word_list = zip(*sorted(zip(tf_idf, tf_local, word_list)))\n",
    "    tf_idf = list(reversed(tf_idf))\n",
    "    tf_local = list(reversed(tf_local))\n",
    "    word_list = list(reversed(word_list))\n",
    "\n",
    "#     print (tf_idf)\n",
    "#     print (\"-------------------------Local Scores------------------------\")\n",
    "#     print (tf_local)\n",
    "#     print (word_list)\n",
    "    cluster_word_list.append(word_list)\n",
    "#     bigram_list = bigrams_dict[cl]\n",
    "#     print (\", \".join(word for word in word_list))\n",
    "#     print (bigrams_dict[cl])\n",
    "#     print (\", \".join(word for word in bigram_list))\n",
    "#     common_bigrams = list(set(word_list) & set(bigram_list))\n",
    "#     print (\"--------------------------------------------------\")\n",
    "#     print (\", \".join(word for word in common_bigrams))\n",
    "#     ads = []\n",
    "#     print (\"\\n\\n\")\n",
    "    eig_ratio = (w[0]+1)/(w[1]+1)\n",
    "    eig_ratio = eig_ratio * eig_ratio\n",
    "    eigen_ratios.append(eig_ratio)\n",
    "#     if eig_ratio > 0.0:\n",
    "#         print ('=========Cluster = {}, Average Score = {}, Cluster metric = {}, Eigenvalues ratio = {}==========\\n'.format(\n",
    "#             cl, avg_label_score[index], filtered_cluster_metric[index], eig_ratio*eig_ratio))\n",
    "#         print (bigrams_dict[cl])\n",
    "#         for ind, row in df_filt.iterrows():\n",
    "#             body = row['body']\n",
    "#             title = row['title']\n",
    "#             for i, bigram in enumerate(word_list):\n",
    "#     #             print (bigram)\n",
    "#                 try:\n",
    "#                     start_ind = body.lower().index(bigram)\n",
    "#                     end_ind = start_ind + len(bigram)\n",
    "#     #                 print (start_ind)\n",
    "#                     body = body[:start_ind] + '\\033[4' + colors_highlight[i%7] + body[start_ind:end_ind] + '\\033[m' + body[end_ind:]\n",
    "#                     start_ind = title.lower().index(bigram)\n",
    "#                     end_ind = start_ind + len(bigram)\n",
    "#     #                 print (start_ind)\n",
    "#                     title = title[:start_ind] + '\\033[4' + colors_highlight[i%7] + title[start_ind:end_ind] + '\\033[m' + title[end_ind:]\n",
    "#                 except:\n",
    "#                     pass\n",
    "#             print (\"----------------------------------------------------------------------------------------------------------------\")\n",
    "#             print (\"{}, {}, {}\".format(title, body, row['Name']))\n",
    "#             ad = [row['id'], row['title'], row['body'], row['label'], count]\n",
    "#             trafficking_ads.append(ad)\n",
    "#     #     break\n",
    "#         time.sleep(5)\n",
    "#         print ('\\n\\n')\n",
    "plt.plot(eigen_ratios)\n",
    "plt.show()\n",
    "# with open('../data/svd_results/trafficking_ads_check_highlight.csv', 'w') as f:\n",
    "#     csv_writer = csv.writer(f)\n",
    "#     csv_writer.writerows(trafficking_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(filtered_cluster_metric, eigen_ratios)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_ratios, filtered_clusters = zip(*sorted(zip(eigen_ratios, filtered_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = 0\n",
    "\n",
    "for ind, cl in enumerate(filtered_clusters):\n",
    "    if eigen_ratios[ind] < 1.5:\n",
    "        counts += 1\n",
    "        print ('=======================================================================================')\n",
    "        df_filt = df_data[df_data['cluster_label']==cl]\n",
    "        for i, row in df_filt.iterrows():\n",
    "            print ('{}'.format(row['body']))\n",
    "            print ('---------------------------------------------------------------------------------')\n",
    "print (counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(filtered_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = list(df_results['content_p'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2,2), norm='l2', \n",
    "     stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "features_col = vectorizer.get_feature_names()\n",
    "print(bigram_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cluster_labels_results = df_results['cluster_label']\n",
    "cluster_vectors = np.zeros((len(filtered_clusters), bigram_matrix.shape[1]))\n",
    "for ind, cl in enumerate(filtered_clusters):\n",
    "    cluster_idx = np.argwhere(cluster_labels_results == cl).reshape(-1)\n",
    "    tf_cluster_mat = bigram_matrix[cluster_idx,:][:].todense()\n",
    "    tf_cluster_mat_flat = np.mean(tf_cluster_mat, axis=0)\n",
    "    print (tf_cluster_mat_flat.shape)\n",
    "    cluster_vectors[ind] = tf_cluster_mat_flat\n",
    "    \n",
    "pairwise_sim_mat = cosine_similarity(cluster_vectors, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(pairwise_sim_mat, 0.0)\n",
    "np.where(pairwise_sim_mat>0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results['cluster_label']==261]['body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
