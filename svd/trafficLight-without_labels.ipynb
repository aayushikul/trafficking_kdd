{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "FILE = '../data/canada/annoncexxx_17000_sample.csv'\n",
    "df_orig = pd.read_csv(FILE)\n",
    "content = []\n",
    "# content = df_data['body']\n",
    "# content = content.replace(np.nan, '', regex=True)\n",
    "for index, row in df_orig.iterrows():\n",
    "    c = ''\n",
    "    # print (row['body'])\n",
    "    # print (row['body'])\n",
    "    # if type(row['title']) == type(' '):\n",
    "    # \tc = c + row['title']\n",
    "    if type(row['description']) == type(' '):\n",
    "        c = c + row['description']\n",
    "    # c = row['title'] + ' ' + row['body']\n",
    "    c = re.sub(r'\\d+', '', c)\n",
    "#     if type(row['Name']) == type(''):\n",
    "#         name = row['Name'].split(';')\n",
    "#         for n in name:\n",
    "#             name_regex = re.compile(re.escape(n), re.IGNORECASE)\n",
    "#             c = name_regex.sub('', c)\n",
    "    c = re.sub(r'[^\\x00-\\x7F]+',' ', c)\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    c = re.sub(cleanr, '', c)\n",
    "#     df_data.at[index, 'body'] = c\n",
    "    content.append(c)\n",
    "df_orig['content_p'] = content\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2,3), norm='l2', \n",
    "     stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "features_col = vectorizer.get_feature_names()\n",
    "print(bigram_matrix.shape)\n",
    "# print (bigram_matrix[0])\n",
    "\n",
    "# svd = TruncatedSVD(n_components=3)\n",
    "# svd.fit_transform(bigram_matrix)\n",
    "# np.save('../data/modalities_data/tf_idf_bigrams.npy', bigram_matrix)\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=20)\n",
    "encoded_vecs = svd.fit_transform(bigram_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, algorithm='best', alpha=1.0)\n",
    "clusterer.fit(encoded_vecs)\n",
    "#     print (clusterer.labels_)\n",
    "labels = clusterer.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "unique_labels = set(labels)\n",
    "probs = clusterer.probabilities_\n",
    "#     colors = [plt.cm.Spectral(each)\n",
    "#               for each in np.linspace(0, 1, len(set(true_labels))]\n",
    "print (\"Number of labels : \" + str(len(list(set(clusterer.labels_)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig['cluster_label'] = labels\n",
    "df_orig['sim_check'] = False\n",
    "df_orig['sim_index'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairwise_similarities(mat):\n",
    "    mat = np.asarray(mat.todense())\n",
    "    sim_scores = np.zeros((mat.shape[0], mat.shape[0]))\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(i+1, mat.shape[0]):\n",
    "#             print (\"i : {}, j : {}\".format(i,j))\n",
    "            if i == j:\n",
    "                continue\n",
    "#             print (len(mat[i]))\n",
    "            sim_scores[i][j] = (mat[i]==mat[j]).all()\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "filtered_df = pd.DataFrame(columns=df_orig.columns)\n",
    "# unique_labels = list(unique_labels) + [-1]\n",
    "blacklisted_global_indices = []\n",
    "for l in unique_labels:\n",
    "    df_fil = df_orig[df_orig['cluster_label']==l].copy()\n",
    "    if l == -1:\n",
    "        filtered_df = pd.concat((filtered_df, df_fil), axis=0)\n",
    "        continue\n",
    "    indices = list(df_fil.index)\n",
    "    df_fil.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    ads = list(df_fil['content_p'])\n",
    "#     print (ads)\n",
    "    count_vectorizer = CountVectorizer(lowercase=True, ngram_range=(2,3), stop_words=stop_words)\n",
    "    ads_vectors = count_vectorizer.fit_transform(ads)\n",
    "#     sim_scores = calculate_pairwise_similarities(ads_vectors)\n",
    "    sim_scores = cosine_similarity(ads_vectors, dense_output=True)\n",
    "    sim_scores *= np.tri(*sim_scores.shape)\n",
    "    np.fill_diagonal(sim_scores, 0.0)\n",
    "\n",
    "    indices_similar = np.where(sim_scores>0.998)\n",
    "    cluster_tuples = zip(indices_similar[0], indices_similar[1])\n",
    "#     print (cluster_tuples)\n",
    "#     indices_similar = np.where(sim_scores == 1)\n",
    "#     x = indices_similar[0]\n",
    "#     y = indices_similar[1]\n",
    "#     print (sim_scores)\n",
    "#     print (y)\n",
    "    blacklisted_indices = []\n",
    "    \n",
    "    for tup in cluster_tuples:\n",
    "#         print (tup)\n",
    "        if tup[0] > tup[1]:\n",
    "            blacklisted_indices.append(indices[tup[1]])\n",
    "            blacklisted_global_indices.append(indices[tup[1]])\n",
    "#     print (df_orig.loc[blacklisted_global_indices, 'body'])\n",
    "#     break\n",
    "#     blacklisted_indices = list(set(blacklisted_indices))\n",
    "    print (indices, set(blacklisted_indices))\n",
    "    remain_ind = [x for x in indices if x not in blacklisted_indices]\n",
    "    df_orig.loc[blacklisted_indices, 'sim_index'] = remain_ind[0] if len(remain_ind) >0 else -1\n",
    "#     if len(df_fil) < 10:\n",
    "#         print (df_fil['body'])\n",
    "#         print (sim_scores)\n",
    "#         print (blacklisted_indices)\n",
    "#     df_fil.drop(blacklisted_indices, inplace=True)\n",
    "#     filtered_df = pd.concat((filtered_df, df_fil), axis=0)\n",
    "blacklisted_global_indices = list(set(blacklisted_global_indices))\n",
    "df_orig.loc[blacklisted_global_indices, 'sim_check'] = True\n",
    "\n",
    "#     break\n",
    "print (df_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_orig.at[312, 'description'])\n",
    "print (df_orig.at[406, 'description'])\n",
    "print (df_orig.at[610, 'description'])\n",
    "print (df_orig.at[1070, 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(set(blacklisted_global_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_df = df_orig[df_orig['sim_check'] == False].copy()\n",
    "print (filtered_df.shape)\n",
    "content = []\n",
    "# content = df_data['body']\n",
    "# content = content.replace(np.nan, '', regex=True)\n",
    "for index, row in filtered_df.iterrows():\n",
    "    c = ''\n",
    "    # print (row['body'])\n",
    "    # print (row['body'])\n",
    "    # if type(row['title']) == type(' '):\n",
    "    # \tc = c + row['title']\n",
    "    if type(row['description']) == type(' '):\n",
    "        c = c + row['description']\n",
    "    # c = row['title'] + ' ' + row['body']\n",
    "    c = re.sub(r'\\d+', '', c)\n",
    "#     if type(row['Name']) == type(''):\n",
    "#         name = row['Name'].split(';')\n",
    "#         for n in name:\n",
    "#             name_regex = re.compile(re.escape(n), re.IGNORECASE)\n",
    "#             c = name_regex.sub('', c)\n",
    "    c = re.sub(r'[^\\x00-\\x7F]+',' ', c)\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    c = re.sub(cleanr, '', c)\n",
    "#     df_data.at[index, 'body'] = c\n",
    "    content.append(c)\n",
    "filtered_df['content_p'] = content\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2,4), norm='l2', \n",
    "     stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "bigram_matrix = vectorizer.fit_transform(content)\n",
    "features_col = vectorizer.get_feature_names()\n",
    "print(bigram_matrix.shape)\n",
    "# print (bigram_matrix[0])\n",
    "\n",
    "# svd = TruncatedSVD(n_components=3)\n",
    "# svd.fit_transform(bigram_matrix)\n",
    "# np.save('../data/modalities_data/tf_idf_bigrams.npy', bigram_matrix)\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bigram_matrix.sum(axis=1)\n",
    "print (s.shape)\n",
    "print (len(np.argwhere(s==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=64)\n",
    "encoded_vecs = svd.fit_transform(bigram_matrix)\n",
    "# print (a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (encoded_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "# rcParams['figure.figsize'] = 10,100\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.imshow(encoded_vecs)\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "origin = np.zeros(64)\n",
    "dist = np.zeros(len(encoded_vecs), dtype=float)\n",
    "for i in range(len(encoded_vecs)):\n",
    "    dist[i] = distance.euclidean(origin, encoded_vecs[i])\n",
    "    \n",
    "noisy_set = np.argwhere(dist < 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bigram_matrix))\n",
    "print (bigram_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bigram_matrix.sum(axis=1)\n",
    "print (s.shape)\n",
    "print (len(np.argwhere(s==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['noise'] = False\n",
    "filtered_df['index1'] = filtered_df.index\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "print (max(filtered_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_matrix = bigram_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "# spatial.distance.euclidean(origin, encoded_vecs[0])\n",
    "noisy_list = []\n",
    "# filtered_df.reset_index(drop=True, inplace=True)\n",
    "noisy_list=list(map(lambda x : x[0], noisy_set))\n",
    "print(len(noisy_list))\n",
    "filtered_df.loc[noisy_list, 'noise'] = True\n",
    "\n",
    "# noisy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.set_index('index1', inplace=True)\n",
    "df_orig = df_orig.join(filtered_df['noise'], how='outer')\n",
    "filtered_df['index1'] = filtered_df.index\n",
    "filtered_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonoise = filtered_df[filtered_df['noise'] == False].copy()\n",
    "df_nonoise.reset_index(drop=True, inplace=True)\n",
    "bigram_matrix = np.delete(bigram_matrix, noisy_list, axis=0)\n",
    "encoded_vecs = np.delete(encoded_vecs, noisy_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (encoded_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_set = noisy_set.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "X_embedded = umap.UMAP().fit_transform(encoded_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (encoded_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 100,100\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.imshow(encoded_vecs)\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################### HBDSCAN ############################\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, algorithm='best', alpha=1.0)\n",
    "clusterer.fit(encoded_vecs)\n",
    "#     print (clusterer.labels_)\n",
    "labels = clusterer.labels_\n",
    "labels_clustering = labels\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "unique_labels = set(labels)\n",
    "probs = clusterer.probabilities_\n",
    "#     colors = [plt.cm.Spectral(each)\n",
    "#               for each in np.linspace(0, 1, len(set(true_labels))]\n",
    "print (\"Number of labels : \" + str(len(list(set(clusterer.labels_)))))\n",
    "#     palette = sns.color_palette()\n",
    "#     cluster_colors = [sns.desaturate(palette[col], sat)\n",
    "#                       if col >= 0 else (0.5, 0.5, 0.5) for col, sat in\n",
    "#                       zip(clusterer.labels_, clusterer.probabilities_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_true_labels = np.asarray(binary_true_labels)\n",
    "# # noise_labels = np.where(labels==-1)\n",
    "# no_noise_labels = np.where(labels!=-1)\n",
    "# tr_labels = np.where(binary_true_labels==1)\n",
    "# print (binary_true_labels.shape)\n",
    "# print (no_noise_labels[0].shape)\n",
    "# print (tr_labels)\n",
    "# # lst3 = [value for value in tr_labels if value in noise_labels] \n",
    "# common_tp = np.intersect1d(no_noise_labels[0], tr_labels[0])\n",
    "# print (common_tp.shape)\n",
    "# total_p = len(common_tp)/len((tr_labels[0]))\n",
    "# print (total_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "noise_labels = np.where(labels==-1)\n",
    "# print (len(noise_labels[0]))\n",
    "noise_vecs = encoded_vecs[noise_labels[0],:][:]\n",
    "noise_dist = np.zeros(len(noise_vecs), dtype=float)\n",
    "for i in range(len(noise_dist)):\n",
    "    noise_dist[i] = distance.euclidean(origin, encoded_vecs[i])\n",
    "# print (noise_dist)\n",
    "noise_dist = np.sort(noise_dist)\n",
    "plt.plot(noise_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly_indices = []\n",
    "# rgba_colors = np.zeros((len(true_labels),4))\n",
    "# for ind, cl in enumerate(labels):\n",
    "#     if cl == -1:\n",
    "#         continue\n",
    "#     cluster_idx = np.argwhere(labels == cl).reshape(-1)\n",
    "#     anomaly_indices += list(cluster_idx)\n",
    "# rgba_colors[:, 0] = 0\n",
    "# rgba_colors[:, 3] = 0.01\n",
    "# # print (anomaly_indices)\n",
    "# for ind in anomaly_indices:\n",
    "#     rgba_colors[ind, 0] = 1\n",
    "#     rgba_colors[ind, 3] = 0.1\n",
    "    \n",
    "# rcParams['figure.figsize'] = 20,10\n",
    "# plt.scatter(X_embedded.T[0], X_embedded.T[1], color=rgba_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rcParams['figure.figsize'] = 10,10\n",
    "plt.scatter(X_embedded.T[0], X_embedded.T[1], alpha=0.2)\n",
    "# plt.title(\"Clustering accuracy={}, fmeasure_synth={}, number_of_labels={}\".format(clustering_acc, fmeasure, \n",
    "#                                                                                   len(unique_labels)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10,2\n",
    "for i in range(10):\n",
    "    ax1 = plt.subplot(1,5,1)\n",
    "    ax2 = plt.subplot(1,5,2)\n",
    "    ax3 = plt.subplot(1,5,3)\n",
    "    ax4 = plt.subplot(1,5,4)\n",
    "    ax5 = plt.subplot(1,5,5)\n",
    "    \n",
    "    ax1.scatter(encoded_vecs.T[i], encoded_vecs.T[i+1], alpha=0.1)\n",
    "    ax1.title.set_text(\"{},{}\".format(i, i+1))\n",
    "    \n",
    "    ax2.scatter(encoded_vecs.T[i], encoded_vecs.T[i+2], alpha=0.1)\n",
    "    ax2.title.set_text(\"{},{}\".format(i, i+2))\n",
    "    \n",
    "    ax3.scatter(encoded_vecs.T[i], encoded_vecs.T[i+3], alpha=0.1)\n",
    "    ax3.title.set_text(\"{},{}\".format(i, i+3))\n",
    "    \n",
    "    ax4.scatter(encoded_vecs.T[i], encoded_vecs.T[i+4], alpha=0.1)\n",
    "    ax4.title.set_text(\"{},{}\".format(i, i+4))\n",
    "    \n",
    "    ax5.scatter(encoded_vecs.T[i], encoded_vecs.T[i+5], alpha=0.1)\n",
    "    ax5.title.set_text(\"{},{}\".format(i, i+5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "#     plt.savefig('../results/svd_components_{}.png'.format(i))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonoise['label'] = -1 \n",
    "val_counts = df_nonoise['phone'].value_counts()\n",
    "unique_phone_numbers = val_counts.index.tolist()\n",
    "print (len(unique_phone_numbers))\n",
    "color_arr = []\n",
    "for ind, u_p in enumerate(unique_phone_numbers):\n",
    "    if val_counts[ind] > 1:\n",
    "        df_nonoise.at[df_nonoise['phone'] == u_p, 'label'] = ind\n",
    "#     print (df_data['label'].nunique())\n",
    "print (df_nonoise['label'].nunique())\n",
    "\n",
    "color_arr = []\n",
    "df_labels = df_nonoise['label']\n",
    "for l in df_labels:\n",
    "    if l == -1:\n",
    "        color_arr.append('grey')\n",
    "    else:\n",
    "        color_arr.append('red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "plt.scatter(X_embedded.T[0], X_embedded.T[1], c=color_arr, alpha=0.4)\n",
    "# plt.title(\"Clustering accuracy={}, fmeasure_synth={}, number_of_labels={}\".format(clustering_acc, fmeasure, \n",
    "#                                                                                   len(unique_labels)))\n",
    "# plt.savefig('../results/embedding_canada_cluster_by_phone.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10,2\n",
    "\n",
    "\n",
    "a = [0,1,2,3,4,5]\n",
    "# b = [0,1,2,3,4,5]\n",
    "\n",
    "from itertools import combinations \n",
    "tuples = list(combinations(a, 2))\n",
    "count = 0\n",
    "# for i in range(5):\n",
    "#     if i%5 == 0:\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         continue\n",
    "#     ax1 = plt.subplot(1,5,1)\n",
    "#     ax2 = plt.subplot(1,5,2)\n",
    "#     ax3 = plt.subplot(1,5,3)\n",
    "#     ax4 = plt.subplot(1,5,4)\n",
    "#     ax5 = plt.subplot(1,5,5)\n",
    "    \n",
    "#     ax1.scatter(encoded_vecs.T[tuples[i][0]], encoded_vecs.T[tuples[i][1]], c=color_array, alpha=0.2)\n",
    "#     ax1.title.set_text(\"{},{}\".format(tuples[i][0], tuples[i][1]))\n",
    "    \n",
    "#     ax2.scatter(encoded_vecs.T[tuples[i+1][0]], encoded_vecs.T[tuples[i+1][1]], c=color_array, alpha=0.2)\n",
    "#     ax2.title.set_text(\"{},{}\".format(tuples[i+1][0], tuples[i+1][1]))\n",
    "    \n",
    "#     ax3.scatter(encoded_vecs.T[tuples[i+2][0]], encoded_vecs.T[tuples[i+2][1]], c=color_array, alpha=0.2)\n",
    "#     ax3.title.set_text(\"{},{}\".format(tuples[i+2][0], tuples[i+2][1]))\n",
    "    \n",
    "#     ax4.scatter(encoded_vecs.T[tuples[i+3][0]], encoded_vecs.T[tuples[i+3][1]], c=color_array, alpha=0.2)\n",
    "#     ax4.title.set_text(\"{},{}\".format(tuples[i+3][0], tuples[i+3][1]))\n",
    "    \n",
    "#     ax5.scatter(encoded_vecs.T[tuples[i+4][0]], encoded_vecs.T[tuples[i+4][1]], c=color_array, alpha=0.2)\n",
    "#     ax5.title.set_text(\"{},{}\".format(tuples[i+4][0], tuples[i+4][1]))\n",
    "    \n",
    "    \n",
    "#     ax2.scatter(encoded_vecs.T[0], encoded_vecs.T[2], c=color_array, alpha=0.2)\n",
    "#     ax2.title.set_text(\"{},{}\".format(tup[i][0], tup[i][1]))\n",
    "#     plt.tight_layout()\n",
    "#     # plt.savefig('../results/svd_components_0.png')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    ax1 = plt.subplot(1,5,1)\n",
    "    ax2 = plt.subplot(1,5,2)\n",
    "    ax3 = plt.subplot(1,5,3)\n",
    "    ax4 = plt.subplot(1,5,4)\n",
    "    ax5 = plt.subplot(1,5,5)\n",
    "    \n",
    "    ax1.scatter(encoded_vecs.T[i], encoded_vecs.T[i+1], c=color_arr, alpha=0.2)\n",
    "    ax1.title.set_text(\"{},{}\".format(i, i+1))\n",
    "    \n",
    "    ax2.scatter(encoded_vecs.T[i], encoded_vecs.T[i+2], c=color_arr, alpha=0.2)\n",
    "    ax2.title.set_text(\"{},{}\".format(i, i+2))\n",
    "    \n",
    "    ax3.scatter(encoded_vecs.T[i], encoded_vecs.T[i+3], c=color_arr, alpha=0.2)\n",
    "    ax3.title.set_text(\"{},{}\".format(i, i+3))\n",
    "    \n",
    "    ax4.scatter(encoded_vecs.T[i], encoded_vecs.T[i+4], c=color_arr, alpha=0.2)\n",
    "    ax4.title.set_text(\"{},{}\".format(i, i+4))\n",
    "    \n",
    "    ax5.scatter(encoded_vecs.T[i], encoded_vecs.T[i+5], c=color_arr, alpha=0.2)\n",
    "    ax5.title.set_text(\"{},{}\".format(i, i+5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "#     plt.savefig('../results/svd_components_{}.png'.format(i))\n",
    "    plt.show()\n",
    "#2,7\n",
    "#1,6\n",
    "#0,5\n",
    "#4,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 8,2\n",
    "ax1 = plt.subplot(1,4,1)\n",
    "ax2 = plt.subplot(1,4,2)\n",
    "ax3 = plt.subplot(1,4,3)\n",
    "ax4 = plt.subplot(1,4,4)\n",
    "\n",
    "ax1.scatter(encoded_vecs.T[2], encoded_vecs.T[7], c=color_arr, alpha=0.2)\n",
    "ax1.text(0.5, 0.5, \"{},{}\".format(2, 7))\n",
    "# ax1.title.set_text(\"{},{}\".format(i, i+1))\n",
    "\n",
    "ax2.scatter(encoded_vecs.T[1], encoded_vecs.T[6], c=color_arr, alpha=0.2)\n",
    "ax2.text(0.5, 0.5,\"{},{}\".format(1, 6))\n",
    "\n",
    "ax3.scatter(encoded_vecs.T[0], encoded_vecs.T[5], c=color_arr, alpha=0.2)\n",
    "ax3.text(0.5, 0.5,\"{},{}\".format(0, 5))\n",
    "\n",
    "ax4.scatter(encoded_vecs.T[4], encoded_vecs.T[9], c=color_arr, alpha=0.2)\n",
    "ax4.text(0.5, 0.5,\"{},{}\".format(4, 9))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/svd_components_to_use{}.png'.format(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bigram_matrix.sum(axis=1)\n",
    "print (s.shape)\n",
    "print (len(np.argwhere(s==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subgraphs(mat_data, cl_ind):\n",
    "    filtered_sub = mat_data[cl_ind,:][:]\n",
    "    bigrams_count = np.count_nonzero(filtered_sub, axis=0)\n",
    "#     print (\"Bigrams count matrix shape : {}\".format(bigrams_count.shape))\n",
    "    zero_count_index = np.where(bigrams_count==0)[1]\n",
    "#     print (zero_count_index)\n",
    "    one_count_index = list(np.where(bigrams_count==1)[1])\n",
    "    core_bigrams_index = list(np.where(bigrams_count>1)[1])\n",
    "    outer_bigrams_index = list(one_count_index + core_bigrams_index)\n",
    "    \n",
    "    bigram_induced_graph = mat_data[:][:,core_bigrams_index]\n",
    "    ads_count = np.count_nonzero(bigram_induced_graph, axis=1)\n",
    "    ads_in_shell_index = np.where(ads_count>1)[0]\n",
    "    \n",
    "    not_core_ads = list(set(list(ads_in_shell_index)) - set(cl_ind))\n",
    "    not_core_bigrams = list(set(list(one_count_index)) - set(core_bigrams_index))\n",
    "    mat_copy = np.asarray(mat_data)\n",
    "    for i in not_core_ads:\n",
    "        for j in not_core_bigrams:\n",
    "            mat_copy[i][j] = 0\n",
    "    shell_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "    mat_copy[not_core_ads, :] = 0\n",
    "    outer_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "    mat_copy[:,one_count_index] = 0\n",
    "    core_subgraph = mat_copy[ads_in_shell_index,:][:,outer_bigrams_index]\n",
    "    shell_subgraph = np.asarray(shell_subgraph)\n",
    "    outer_subgraph = np.asarray(outer_subgraph)\n",
    "    core_subgraph = np.asarray(core_subgraph)\n",
    "\n",
    "    return shell_subgraph, outer_subgraph, core_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_counts = []\n",
    "df_nonoise['cluster_label'] = labels\n",
    "df_nonoise['probabilities'] = probs\n",
    "for c in unique_labels:\n",
    "    df_fil = df_nonoise[df_nonoise['cluster_label']==c]\n",
    "    nun_phone = df_fil['phone'].nunique()\n",
    "#     score = sum(df_fil['label'])/len(df_fil['label'])\n",
    "    phone_counts.append(nun_phone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_unweighted_density(core_mat):\n",
    "    edge_weight = np.count_nonzero(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "#     print (ads_core_num)\n",
    "#     print (bigrams_core_num)\n",
    "    return edge_weight/(ads_core_num * bigrams_core_num + 1)\n",
    "\n",
    "def calculate_weighted_density(core_mat):\n",
    "    edge_weight = np.sum(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "\n",
    "    return edge_weight/(ads_core_num * bigrams_core_num + 1)\n",
    "\n",
    "def calculate_unweighted_fraudar_score(core_mat):\n",
    "    edge_weight = np.count_nonzero(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return edge_weight/(ads_core_num + bigrams_core_num + 1)\n",
    "\n",
    "def calculate_weighted_fraudar_score(core_mat):\n",
    "    edge_weight = np.sum(core_mat)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return edge_weight/(ads_core_num + bigrams_core_num + 1)\n",
    "\n",
    "def calculate_unweighted_edge_per_score(core_mat, outer_mat):\n",
    "    core_edges = np.count_nonzero(core_mat)\n",
    "    outer_edges = np.count_nonzero(outer_mat)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return ((core_edges + 1)/(outer_edges +1))*(math.log(bigrams_core_num+1))\n",
    "\n",
    "def calculate_weighted_edge_per_score(core_mat, outer_mat):\n",
    "    core_edges = np.sum(core_mat)\n",
    "    outer_edges = np.sum(outer_mat)\n",
    "    \n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    \n",
    "    return ((core_edges + 1)/(outer_edges +1))*(math.log(bigrams_core_num+1))\n",
    "\n",
    "\n",
    "def calculate_custom_score(core_mat, outer_mat):\n",
    "    bigram_degrees = np.count_nonzero(np.asarray(core_mat), axis=0)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_core_num = len(np.where(bigrams_count>0)[0])\n",
    "    outer_edges = np.count_nonzero(outer_mat)\n",
    "    bigram_degrees = bigram_degrees/bigrams_core_num\n",
    "    \n",
    "    ads_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_num = len(np.where(ads_count>0)[0])\n",
    "    \n",
    "    return (np.sum(bigram_degrees)/(outer_edges+1))*(math.log(bigrams_core_num+1))*(math.log(ads_core_num+1))\n",
    "# #     print (mat.shape)\n",
    "#     edges_nonzero = np.count_nonzero(mat, axis=0)\n",
    "#     unique, counts = np.unique(edges_nonzero, return_counts=True)\n",
    "#     degree_counts = dict(zip(unique, counts))\n",
    "#     numerator = 0.0\n",
    "#     denominator = 0.0\n",
    "#     half = max(mat.shape[0]/2, 2)\n",
    "#     for k, v in degree_counts.items():\n",
    "#         if k == 0:\n",
    "#             continue\n",
    "#         elif k <= half:\n",
    "#             denominator += k*v\n",
    "#         else:\n",
    "#             denominator += k*v\n",
    "#             numerator += k*v\n",
    "#     if denominator == 0.0:\n",
    "#         return 0.0\n",
    "#     else:\n",
    "#         return numerator/denominator\n",
    "\n",
    "# def calculate_weighted_edge_per_score(mat):\n",
    "#     return 0.0\n",
    "\n",
    "#Should be shell_mat instead of outer_mat, change once you figure out how to get shell subgraph.\n",
    "def calculate_unweighted_modularity_score(core_mat, outer_mat, total_edges):\n",
    "#     total_edges = math.log(total_edges)\n",
    "    ad_degrees = np.count_nonzero(np.asarray(outer_mat), axis=1)\n",
    "#     print (ad_degrees)\n",
    "    bigram_degrees = np.count_nonzero(np.asarray(outer_mat), axis=0)\n",
    "#     print (bigram_degrees)\n",
    "    ads_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_index = np.where(ads_count>0)[0]\n",
    "    bigrams_core_index = np.where(bigrams_count>0)[0]\n",
    "#     ads_list_index = [x for x in range(core_mat.shape[0])]\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "    summation = 0.0\n",
    "    for i in ads_core_index:\n",
    "        for j in bigrams_core_index:\n",
    "            if core_mat[i][j] == 0:\n",
    "                adj = 0\n",
    "            else:\n",
    "                adj = 1\n",
    "            if adj == 1:\n",
    "                summation += (adj - (ad_degrees[i] * bigram_degrees[j])/total_edges)\n",
    "\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "#     summation = 0.0\n",
    "#     for k in range(nonzero_edges.shape[0]):\n",
    "#         ad_index = nonzero_edges[k][0]\n",
    "#         big_index = nonzero_edges[k][1]\n",
    "# #         print (ad_index)\n",
    "# #         print (big_index)\n",
    "# #         if core_mat[ad_index][big_index] != 0:\n",
    "# #             adj = 1\n",
    "# #         else:\n",
    "# #             adj = 0\n",
    "        \n",
    "#         summation += (1 - (ad_degrees[ad_index] * bigram_degrees[big_index])/(2*total_edges))\n",
    "    \n",
    "    return (summation/total_edges)*(math.log(len(ads_core_index)+1))\n",
    "\n",
    "def calculate_weighted_modularity_score(core_mat, outer_mat, total_edges):\n",
    "    ad_degrees = np.sum(np.asarray(outer_mat), axis=1)\n",
    "    bigram_degrees = np.sum(np.asarray(outer_mat), axis=0)\n",
    "    \n",
    "    ads_count = np.count_nonzero(core_mat, axis=1)\n",
    "    bigrams_count = np.count_nonzero(core_mat, axis=0)\n",
    "    ads_core_index = np.where(ads_count>0)[0]\n",
    "    bigrams_core_index = np.where(bigrams_count>0)[0]\n",
    "\n",
    "    \n",
    "\n",
    "#     ads_list_index = [x for x in range(core_mat.shape[0])]\n",
    "#     nonzero_edges = np.transpose(np.nonzero(core_mat))\n",
    "    summation = 0.0\n",
    "    for i in ads_core_index:\n",
    "        for j in bigrams_core_index:\n",
    "            if core_mat[i][j] != 0:\n",
    "                summation += (core_mat[i][j] - (ad_degrees[i] * bigram_degrees[j])/total_edges)\n",
    "#     summation = 0.0\n",
    "#     for k in range(nonzero_edges.shape[0]):\n",
    "#         ad_index = nonzero_edges[k][0]\n",
    "#         big_index = nonzero_edges[k][1]\n",
    "#         summation += (outer_mat[ad_index][big_index] - (ad_degrees[ad_index] * bigram_degrees[big_index])/(2*total_edges))\n",
    "    \n",
    "    return (summation/total_edges)*(math.log(len(ads_core_index)+1))\n",
    "\n",
    "def calculate_pairwise_modularity(mat):\n",
    "    mat = np.asarray(mat.todense())\n",
    "    sim_scores = np.zeros((mat.shape[0], mat.shape[0]))\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(i+1, mat.shape[0]):\n",
    "#             print (\"i : {}, j : {}\".format(i,j))\n",
    "            if i == j:\n",
    "                continue\n",
    "#             print (len(mat[i]))\n",
    "            sim_scores[i][j] = calculate_modularity_score(np.vstack((mat[i], mat[j])))\n",
    "    \n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_all_metrics(bigram_matrix, unique_labels, labels, df_data):\n",
    "    eigen_ratios = []\n",
    "    weighted_cluster_density = []\n",
    "    unweighted_cluster_density = []\n",
    "    unweighted_fraudar_scores = []\n",
    "    weighted_fraudar_scores = []\n",
    "    unweighted_outer_edge_perc_scores = []\n",
    "    weighted_outer_edge_perc_scores = []\n",
    "    unweighted_shell_edge_perc_scores = []\n",
    "    weighted_shell_edge_perc_scores = []\n",
    "    weighted_outer_modularity_scores = []\n",
    "    unweighted_outer_modularity_scores = []\n",
    "    weighted_shell_modularity_scores = []\n",
    "    unweighted_shell_modularity_scores = []\n",
    "    phone_counts = []\n",
    "    pairwise_similarity = []\n",
    "    custom_score = []\n",
    "    clusters = []\n",
    "    cluster_counts = []\n",
    "    \n",
    "    total_edges_unweighted = np.count_nonzero(bigram_matrix)\n",
    "    total_edges_weighted = np.sum(bigram_matrix)\n",
    "    for l in unique_labels:\n",
    "#         s = bigram_matrix.sum(axis=1)\n",
    "        if l== -1:\n",
    "            weighted_cluster_density.append(0)\n",
    "            unweighted_cluster_density.append(0)\n",
    "            weighted_fraudar_scores.append(0)\n",
    "            unweighted_fraudar_scores.append(0)\n",
    "            weighted_outer_edge_perc_scores.append(0)\n",
    "            unweighted_outer_edge_perc_scores.append(0)\n",
    "            weighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_outer_modularity_scores.append(0)\n",
    "            weighted_outer_modularity_scores.append(0)\n",
    "            unweighted_shell_modularity_scores.append(0)\n",
    "            weighted_shell_modularity_scores.append(0)\n",
    "            pairwise_similarity.append(0)\n",
    "            custom_score.append(0)\n",
    "\n",
    "            cluster_counts.append(len(cluster_idx))\n",
    "            \n",
    "            \n",
    "            eigen_ratios.append(0)\n",
    "            clusters.append(l)\n",
    "            phone_counts.append(0)\n",
    "            \n",
    "            continue\n",
    "#         print (s.shape)\n",
    "#         print (\"bigram matrix sum : {}\".format(bigram_matrix.sum()))\n",
    "#         print (\"Zero elems: {}\".format(len(np.argwhere(s==0))))\n",
    "        cluster_idx = np.argwhere(labels == l).reshape(-1)\n",
    "#         print (l, len(cluster_idx))\n",
    "        \n",
    "        print (cluster_idx)\n",
    "        shell_subgraph, outer_subgraph, core_subgraph = get_all_subgraphs(bigram_matrix.copy(), cluster_idx)\n",
    "#         print (l, len(cluster_idx), core_subgraph.sum(), outer_subgraph.sum(), shell_subgraph.sum())\n",
    "        \n",
    "\n",
    "        df_filt = df_data[df_data['cluster_label']== l]\n",
    "        if len(df_filt) == 0 or core_subgraph.shape[0] == 0 or shell_subgraph.shape[0] == 0 or outer_subgraph.shape[0] == 0:\n",
    "            weighted_cluster_density.append(0)\n",
    "            unweighted_cluster_density.append(0)\n",
    "            weighted_fraudar_scores.append(0)\n",
    "            unweighted_fraudar_scores.append(0)\n",
    "            weighted_outer_edge_perc_scores.append(0)\n",
    "            unweighted_outer_edge_perc_scores.append(0)\n",
    "            weighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_shell_edge_perc_scores.append(0)\n",
    "            unweighted_outer_modularity_scores.append(0)\n",
    "            weighted_outer_modularity_scores.append(0)\n",
    "            unweighted_shell_modularity_scores.append(0)\n",
    "            weighted_shell_modularity_scores.append(0)\n",
    "            pairwise_similarity.append(0)\n",
    "            custom_score.append(0)\n",
    "            cluster_counts.append(len(cluster_idx))\n",
    "            eigen_ratios.append(0)\n",
    "            clusters.append(l)\n",
    "            phone_counts.append(0)\n",
    "            continue\n",
    "#         elif core_subgraph.shape[0] == 0 or shell_subgraph.shape[0] == 0 or outer_subgraph.shape[0] == 0:\n",
    "#             continue\n",
    "        print (l, df_filt.shape)\n",
    "        \n",
    "        local_content = list(df_filt['content_p'])\n",
    "        count_vectorizer = TfidfVectorizer(ngram_range=(2,2), use_idf=False)\n",
    "        count_data = count_vectorizer.fit_transform(local_content)\n",
    "\n",
    "        svd = TruncatedSVD(n_components=2)\n",
    "        local_vecs = svd.fit_transform(count_data)\n",
    "        w = svd.singular_values_\n",
    "        eigen_rat = w[1]/w[0]\n",
    "        eigen_ratios.append(eigen_rat)\n",
    "        \n",
    "        print (outer_subgraph.shape)\n",
    "        print (core_subgraph.shape)\n",
    "        pairwise_sim_mat = cosine_similarity(outer_subgraph, dense_output=True)\n",
    "        pairwise_sim_mat = np.tril(pairwise_sim_mat, -1)\n",
    "#         print (sum(pairwise_sim_mat).shape)\n",
    "        print (pairwise_sim_mat.sum())\n",
    "        an_score = calculate_weighted_edge_per_score(core_subgraph, outer_subgraph)\n",
    "        phone = [x if type(x) == type('') else None for x in df_filt['phone'].unique()]\n",
    "        if math.nan in phone:\n",
    "            phone.remove(math.nan)\n",
    "        if None in phone:\n",
    "            phone.remove(None)\n",
    "        phone = list(set(phone))\n",
    "        c_score = an_score * max(0, len(phone)-1)\n",
    "        print (\"Scores : {}, {}, {}\".format(an_score, len(phone), c_score))\n",
    "        weighted_cluster_density.append(calculate_weighted_density(core_subgraph))\n",
    "        unweighted_cluster_density.append(calculate_unweighted_density(core_subgraph))\n",
    "        weighted_fraudar_scores.append(calculate_weighted_fraudar_score(core_subgraph))\n",
    "        unweighted_fraudar_scores.append(calculate_unweighted_fraudar_score(core_subgraph))\n",
    "        weighted_outer_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "        unweighted_outer_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "        weighted_shell_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "        unweighted_shell_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "        unweighted_outer_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, outer_subgraph, total_edges_unweighted))\n",
    "        weighted_outer_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, outer_subgraph, total_edges_weighted))\n",
    "        unweighted_shell_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, shell_subgraph, total_edges_unweighted))\n",
    "        weighted_shell_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, shell_subgraph, total_edges_weighted))\n",
    "        custom_score.append(c_score)\n",
    "        phone_counts.append(len(phone))\n",
    "        pairwise_similarity.append(pairwise_sim_mat.sum()/len(cluster_idx))\n",
    "        cluster_counts.append(len(cluster_idx))\n",
    "        clusters.append(l)\n",
    "        \n",
    "        count_data = []\n",
    "        local_content = []\n",
    "        shell_subgraph = []\n",
    "        core_subgraph = []\n",
    "        outer_subgraph = []\n",
    "        if l % 50 == 0:\n",
    "            print (l)\n",
    "            gc.collect()\n",
    "#     original_labels = labels.copy()\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['weighted_cluster_density'] = weighted_cluster_density\n",
    "    metrics['unweighted_cluster_density'] = unweighted_cluster_density\n",
    "    metrics['weighted_fraudar_scores'] = weighted_fraudar_scores\n",
    "    metrics['unweighted_fraudar_scores'] = unweighted_fraudar_scores\n",
    "    metrics['weighted_outer_edge_perc_scores'] = weighted_outer_edge_perc_scores\n",
    "    metrics['unweighted_outer_edge_perc_scores'] = unweighted_outer_edge_perc_scores\n",
    "    metrics['weighted_shell_edge_perc_scores'] = weighted_shell_edge_perc_scores\n",
    "    metrics['unweighted_shell_edge_perc_scores'] = unweighted_shell_edge_perc_scores\n",
    "    metrics['unweighted_outer_modularity_scores'] = unweighted_outer_modularity_scores\n",
    "    metrics['weighted_outer_modularity_scores'] = weighted_outer_modularity_scores\n",
    "    metrics['unweighted_shell_modularity_scores'] = unweighted_shell_modularity_scores\n",
    "    metrics['weighted_shell_modularity_scores'] = weighted_shell_modularity_scores\n",
    "    metrics['pairwise_similarity'] = pairwise_similarity\n",
    "    metrics['custom_score'] = custom_score\n",
    "    metrics['phone_counts'] = phone_counts\n",
    "    metrics['eigen_ratios'] = eigen_ratios\n",
    "    metrics['clusters'] = clusters\n",
    "    metrics['labels'] = labels.copy()\n",
    "    metrics['cluster_counts'] = cluster_counts\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# def get_all_metrics(bigram_matrix, unique_labels, labels, df_data):\n",
    "#     eigen_ratios = []\n",
    "#     weighted_cluster_density = []\n",
    "#     unweighted_cluster_density = []\n",
    "#     unweighted_fraudar_scores = []\n",
    "#     weighted_fraudar_scores = []\n",
    "#     unweighted_outer_edge_perc_scores = []\n",
    "#     weighted_outer_edge_perc_scores = []\n",
    "#     unweighted_shell_edge_perc_scores = []\n",
    "#     weighted_shell_edge_perc_scores = []\n",
    "#     weighted_outer_modularity_scores = []\n",
    "#     unweighted_outer_modularity_scores = []\n",
    "#     weighted_shell_modularity_scores = []\n",
    "#     unweighted_shell_modularity_scores = []\n",
    "#     clusters = []\n",
    "#     cluster_counts = []\n",
    "    \n",
    "#     total_edges_unweighted = np.count_nonzero(bigram_matrix)\n",
    "#     total_edges_weighted = np.sum(bigram_matrix)\n",
    "#     for l in unique_labels:\n",
    "# #         s = bigram_matrix.sum(axis=1)\n",
    "#         if l== -1:\n",
    "#             weighted_cluster_density.append(0)\n",
    "#             unweighted_cluster_density.append(0)\n",
    "#             weighted_fraudar_scores.append(0)\n",
    "#             unweighted_fraudar_scores.append(0)\n",
    "#             weighted_outer_edge_perc_scores.append(0)\n",
    "#             unweighted_outer_edge_perc_scores.append(0)\n",
    "#             weighted_shell_edge_perc_scores.append(0)\n",
    "#             unweighted_shell_edge_perc_scores.append(0)\n",
    "#             unweighted_outer_modularity_scores.append(0)\n",
    "#             weighted_outer_modularity_scores.append(0)\n",
    "#             unweighted_shell_modularity_scores.append(0)\n",
    "#             weighted_shell_modularity_scores.append(0)\n",
    "#             cluster_counts.append(math.log(len(cluster_idx), 10)) \n",
    "#             eigen_ratios.append(0)\n",
    "#             clusters.append(l)\n",
    "\n",
    "#             continue\n",
    "# #         print (s.shape)\n",
    "# #         print (\"bigram matrix sum : {}\".format(bigram_matrix.sum()))\n",
    "# #         print (\"Zero elems: {}\".format(len(np.argwhere(s==0))))\n",
    "#         cluster_idx = np.argwhere(labels == l).reshape(-1)\n",
    "# #         print (l, len(cluster_idx))\n",
    "#         clusters.append(l)\n",
    "#         shell_subgraph, outer_subgraph, core_subgraph = get_all_subgraphs(bigram_matrix.copy(), cluster_idx)\n",
    "# #         print (l, len(cluster_idx), core_subgraph.sum(), outer_subgraph.sum(), shell_subgraph.sum())\n",
    "        \n",
    "\n",
    "#         df_filt = df_data[df_data['cluster_label']== l]\n",
    "        \n",
    "#         local_content = list(df_filt['content_p'])\n",
    "#         try:\n",
    "#             count_vectorizer = TfidfVectorizer(ngram_range=(2,2), use_idf=False)\n",
    "#             count_data = count_vectorizer.fit_transform(local_content)\n",
    "#             if count_data.shape[1] < 5:\n",
    "#                 eigen_ratios.append(1)\n",
    "#             else:\n",
    "#                 svd = TruncatedSVD(n_components=2)\n",
    "#                 local_vecs = svd.fit_transform(count_data)\n",
    "#                 w = svd.singular_values_\n",
    "#                 eigen_rat = w[1]/w[0]\n",
    "#                 eigen_ratios.append(eigen_rat)\n",
    "#         except:\n",
    "#             eigen_ratios.append(1)\n",
    "\n",
    "#         weighted_cluster_density.append(calculate_weighted_density(core_subgraph))\n",
    "#         unweighted_cluster_density.append(calculate_unweighted_density(core_subgraph))\n",
    "#         weighted_fraudar_scores.append(calculate_weighted_fraudar_score(core_subgraph))\n",
    "#         unweighted_fraudar_scores.append(calculate_unweighted_fraudar_score(core_subgraph))\n",
    "#         weighted_outer_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "#         unweighted_outer_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, outer_subgraph))\n",
    "#         weighted_shell_edge_perc_scores.append(calculate_weighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "#         unweighted_shell_edge_perc_scores.append(calculate_unweighted_edge_per_score(core_subgraph, shell_subgraph))\n",
    "#         unweighted_outer_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, outer_subgraph, total_edges_unweighted))\n",
    "#         weighted_outer_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, outer_subgraph, total_edges_weighted))\n",
    "#         unweighted_shell_modularity_scores.append(calculate_unweighted_modularity_score(core_subgraph, shell_subgraph, total_edges_unweighted))\n",
    "#         weighted_shell_modularity_scores.append(calculate_weighted_modularity_score(core_subgraph, shell_subgraph, total_edges_weighted))\n",
    "#         cluster_counts.append(math.log(len(cluster_idx)+1, 10)) \n",
    "        \n",
    "#         count_data = []\n",
    "#         local_content = []\n",
    "#         shell_subgraph = []\n",
    "#         core_subgraph = []\n",
    "#         outer_subgraph = []\n",
    "#         if l % 50 == 0:\n",
    "#             print (l)\n",
    "#             gc.collect()\n",
    "# #     original_labels = labels.copy()\n",
    "\n",
    "#     metrics = {}\n",
    "#     metrics['weighted_cluster_density'] = weighted_cluster_density\n",
    "#     metrics['unweighted_cluster_density'] = unweighted_cluster_density\n",
    "#     metrics['weighted_fraudar_scores'] = weighted_fraudar_scores\n",
    "#     metrics['unweighted_fraudar_scores'] = unweighted_fraudar_scores\n",
    "#     metrics['weighted_outer_edge_perc_scores'] = weighted_outer_edge_perc_scores\n",
    "#     metrics['unweighted_outer_edge_perc_scores'] = unweighted_outer_edge_perc_scores\n",
    "#     metrics['weighted_shell_edge_perc_scores'] = weighted_shell_edge_perc_scores\n",
    "#     metrics['unweighted_shell_edge_perc_scores'] = unweighted_shell_edge_perc_scores\n",
    "#     metrics['unweighted_outer_modularity_scores'] = unweighted_outer_modularity_scores\n",
    "#     metrics['weighted_outer_modularity_scores'] = weighted_outer_modularity_scores\n",
    "#     metrics['unweighted_shell_modularity_scores'] = unweighted_shell_modularity_scores\n",
    "#     metrics['weighted_shell_modularity_scores'] = weighted_shell_modularity_scores\n",
    "#     metrics['eigen_ratios'] = eigen_ratios\n",
    "#     metrics['clusters'] = clusters\n",
    "#     metrics['labels'] = labels.copy()\n",
    "#     metrics['cluster_counts'] = cluster_counts\n",
    "    \n",
    "#     return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_merging_metrics = get_all_metrics(bigram_matrix, unique_labels, labels, df_nonoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 5,5\n",
    "clusters = pre_merging_metrics['clusters']\n",
    "pre_merging_metrics['eigen_ratios'][pre_merging_metrics['clusters'].index(-1)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_ratios = pre_merging_metrics['eigen_ratios']\n",
    "eigen_np = np.array(eigen_ratios)\n",
    "clusters_nonhomogenous_index = np.where(eigen_np > 0.8)[0]\n",
    "clusters_nonhomogenous = [clusters[i] for i in clusters_nonhomogenous_index]\n",
    "# print (clusters_nonhomogenous)\n",
    "non_noisy_clusters = [x for x in clusters if x not in clusters_nonhomogenous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_cluster_idx = []\n",
    "original_labels = pre_merging_metrics['labels'].copy()\n",
    "for l in clusters_nonhomogenous:\n",
    "    cluster_idx = np.argwhere(original_labels == l).reshape(-1)\n",
    "    rerun_cluster_idx += list(cluster_idx)\n",
    "#     print (total_cluster_idx)\n",
    "print (len(rerun_cluster_idx))\n",
    "rerun_bigram_data = bigram_matrix[rerun_cluster_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=64)\n",
    "encoded_vecs_rerun = svd.fit_transform(rerun_bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.fit(encoded_vecs_rerun)\n",
    "#     print (clusterer.labels_)\n",
    "rerun_clustering_labels = clusterer.labels_\n",
    "rerun_labels_clustering = labels\n",
    "rerun_n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_rerun_ = list(labels).count(-1)\n",
    "rerun_unique_labels = set(labels)\n",
    "rerun_probs = clusterer.probabilities_\n",
    "#     colors = [plt.cm.Spectral(each)\n",
    "#               for each in np.linspace(0, 1, len(set(true_labels))]\n",
    "print (\"Number of labels : \" + str(len(list(set(clusterer.labels_)))))\n",
    "#     palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_clusters = max(pre_merging_metrics['clusters'])\n",
    "# labels = pre_merging_metrics['labels']\n",
    "rerun_clusters = []\n",
    "for ind, ad in enumerate(rerun_cluster_idx):\n",
    "    if rerun_clustering_labels[ind] != -1:\n",
    "        original_labels[ad] = rerun_clustering_labels[ind] + max_clusters + 1\n",
    "        rerun_clusters.append(rerun_clustering_labels[ind] + max_clusters + 1)\n",
    "        probs[ad] = rerun_probs[ind]\n",
    "    else:\n",
    "        original_labels[ad] = -1\n",
    "        probs[ad] = rerun_probs[ind]\n",
    "        rerun_clusters.append(-1)\n",
    "rerun_clusters = list(set(rerun_clusters))\n",
    "clusters = rerun_clusters + non_noisy_clusters\n",
    "# print (clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_splitting_metrics = get_all_metrics(bigram_matrix, rerun_clusters, original_labels, df_nonoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_ratios = []\n",
    "weighted_cluster_density = []\n",
    "unweighted_cluster_density = []\n",
    "unweighted_fraudar_scores = []\n",
    "weighted_fraudar_scores = []\n",
    "unweighted_outer_edge_perc_scores = []\n",
    "weighted_outer_edge_perc_scores = []\n",
    "unweighted_shell_edge_perc_scores = []\n",
    "weighted_shell_edge_perc_scores = []\n",
    "weighted_outer_modularity_scores = []\n",
    "unweighted_outer_modularity_scores = []\n",
    "weighted_shell_modularity_scores = []\n",
    "unweighted_shell_modularity_scores = []\n",
    "pairwise_similarity = []\n",
    "custom_score = []\n",
    "phone_counts = []\n",
    "cluster_counts = []\n",
    "print (len(rerun_clusters))\n",
    "print (len(post_splitting_metrics['eigen_ratios']))\n",
    "for i in clusters:\n",
    "    if i in rerun_clusters:\n",
    "        ind = rerun_clusters.index(i)\n",
    "        eigen_ratios.append(post_splitting_metrics['eigen_ratios'][ind])\n",
    "        weighted_cluster_density.append(post_splitting_metrics['weighted_cluster_density'][ind])\n",
    "        unweighted_cluster_density.append(post_splitting_metrics['unweighted_cluster_density'][ind])\n",
    "        unweighted_fraudar_scores.append(post_splitting_metrics['unweighted_fraudar_scores'][ind])\n",
    "        weighted_fraudar_scores.append(post_splitting_metrics['weighted_fraudar_scores'][ind])\n",
    "        unweighted_outer_edge_perc_scores.append(post_splitting_metrics['unweighted_outer_edge_perc_scores'][ind])\n",
    "        weighted_outer_edge_perc_scores.append(post_splitting_metrics['weighted_outer_edge_perc_scores'][ind])\n",
    "        unweighted_shell_edge_perc_scores.append(post_splitting_metrics['unweighted_shell_edge_perc_scores'][ind])\n",
    "        weighted_shell_edge_perc_scores.append(post_splitting_metrics['weighted_shell_edge_perc_scores'][ind])\n",
    "        weighted_outer_modularity_scores.append(post_splitting_metrics['weighted_outer_modularity_scores'][ind])\n",
    "        unweighted_outer_modularity_scores.append(post_splitting_metrics['unweighted_outer_modularity_scores'][ind])\n",
    "        weighted_shell_modularity_scores.append(post_splitting_metrics['weighted_shell_modularity_scores'][ind])\n",
    "        unweighted_shell_modularity_scores.append(post_splitting_metrics['unweighted_shell_modularity_scores'][ind])\n",
    "        pairwise_similarity.append(post_splitting_metrics['pairwise_similarity'][ind])\n",
    "        custom_score.append(post_splitting_metrics['custom_score'][ind])\n",
    "        phone_counts.append(post_splitting_metrics['phone_counts'][ind])\n",
    "        cluster_counts.append(post_splitting_metrics['cluster_counts'][ind])\n",
    "        \n",
    "    elif i in non_noisy_clusters:\n",
    "        ind = pre_merging_metrics['clusters'].index(i)\n",
    "        eigen_ratios.append(pre_merging_metrics['eigen_ratios'][ind])\n",
    "        weighted_cluster_density.append(pre_merging_metrics['weighted_cluster_density'][ind])\n",
    "        unweighted_cluster_density.append(pre_merging_metrics['unweighted_cluster_density'][ind])\n",
    "        unweighted_fraudar_scores.append(pre_merging_metrics['unweighted_fraudar_scores'][ind])\n",
    "        weighted_fraudar_scores.append(pre_merging_metrics['weighted_fraudar_scores'][ind])\n",
    "        unweighted_outer_edge_perc_scores.append(pre_merging_metrics['unweighted_outer_edge_perc_scores'][ind])\n",
    "        weighted_outer_edge_perc_scores.append(pre_merging_metrics['weighted_outer_edge_perc_scores'][ind])\n",
    "        unweighted_shell_edge_perc_scores.append(pre_merging_metrics['unweighted_shell_edge_perc_scores'][ind])\n",
    "        weighted_shell_edge_perc_scores.append(pre_merging_metrics['weighted_shell_edge_perc_scores'][ind])\n",
    "        weighted_outer_modularity_scores.append(pre_merging_metrics['weighted_outer_modularity_scores'][ind])\n",
    "        unweighted_outer_modularity_scores.append(pre_merging_metrics['unweighted_outer_modularity_scores'][ind])\n",
    "        weighted_shell_modularity_scores.append(pre_merging_metrics['weighted_shell_modularity_scores'][ind])\n",
    "        unweighted_shell_modularity_scores.append(pre_merging_metrics['unweighted_shell_modularity_scores'][ind])\n",
    "        pairwise_similarity.append(pre_merging_metrics['pairwise_similarity'][ind])\n",
    "        custom_score.append(pre_merging_metrics['custom_score'][ind])\n",
    "        phone_counts.append(pre_merging_metrics['phone_counts'][ind])\n",
    "        cluster_counts.append(pre_merging_metrics['cluster_counts'][i])\n",
    "\n",
    "metrics = {}\n",
    "metrics['weighted_cluster_density'] = weighted_cluster_density\n",
    "metrics['unweighted_cluster_density'] = unweighted_cluster_density\n",
    "metrics['weighted_fraudar_scores'] = weighted_fraudar_scores\n",
    "metrics['unweighted_fraudar_scores'] = unweighted_fraudar_scores\n",
    "metrics['weighted_outer_edge_perc_scores'] = weighted_outer_edge_perc_scores\n",
    "metrics['unweighted_outer_edge_perc_scores'] = unweighted_outer_edge_perc_scores\n",
    "metrics['weighted_shell_edge_perc_scores'] = weighted_shell_edge_perc_scores\n",
    "metrics['unweighted_shell_edge_perc_scores'] = unweighted_shell_edge_perc_scores\n",
    "metrics['unweighted_outer_modularity_scores'] = unweighted_outer_modularity_scores\n",
    "metrics['weighted_outer_modularity_scores'] = weighted_outer_modularity_scores\n",
    "metrics['unweighted_shell_modularity_scores'] = unweighted_shell_modularity_scores\n",
    "metrics['weighted_shell_modularity_scores'] = weighted_shell_modularity_scores\n",
    "metrics['pairwise_similarity'] = pairwise_similarity\n",
    "metrics['custom_score'] = custom_score\n",
    "metrics['phone_counts'] = phone_counts\n",
    "metrics['eigen_ratios'] = eigen_ratios\n",
    "metrics['clusters'] = clusters\n",
    "metrics['cluster_label'] = labels.copy()\n",
    "metrics['cluster_counts'] = cluster_counts \n",
    "\n",
    "post_noisy_split_metrics = metrics\n",
    "print (len(post_noisy_split_metrics['eigen_ratios']))\n",
    "print (len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_labels_results = df_nonoise['cluster_label']\n",
    "cluster_vectors = np.zeros((len(clusters), bigram_matrix.shape[1]))\n",
    "for ind, cl in enumerate(clusters):\n",
    "    cluster_idx = np.argwhere(cluster_labels_results == cl).reshape(-1)\n",
    "    tf_cluster_mat = bigram_matrix[cluster_idx,:][:]\n",
    "    tf_cluster_mat_flat = np.mean(tf_cluster_mat, axis=0)\n",
    "    print (tf_cluster_mat_flat.shape)\n",
    "    cluster_vectors[ind] = tf_cluster_mat_flat\n",
    "\n",
    "where_are_nans = np.isnan(cluster_vectors)\n",
    "cluster_vectors[where_are_nans] = 0\n",
    "pairwise_sim_mat = cosine_similarity(cluster_vectors, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_sim_mat *= np.tri(*pairwise_sim_mat.shape)\n",
    "np.fill_diagonal(pairwise_sim_mat, 0.0)\n",
    "\n",
    "sim_clusters = np.where(pairwise_sim_mat>0.8)\n",
    "cluster_tuples = zip(sim_clusters[0], sim_clusters[1])\n",
    "\n",
    "clusters_to_be_merged = []\n",
    "\n",
    "for tup in cluster_tuples:\n",
    "    clusters_to_be_merged.append((clusters[tup[0]], clusters[tup[1]]))\n",
    "print (clusters_to_be_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_clusters = np.array(cluster_labels_results)\n",
    "print (bigram_matrix.shape)\n",
    "for cl in clusters_to_be_merged:\n",
    "    np_clusters = np.array(cluster_labels_results)\n",
    "#     adj_matrix_copy = bigram_matrix.copy()\n",
    "    replace_index = np.where(np_clusters==cl[1])\n",
    "    for i in replace_index:\n",
    "        np_clusters[i] = cl[0]\n",
    "    pre_cl_ind1 = np.argwhere(pre_clusters==cl[0]).reshape(-1)\n",
    "    pre_cl_ind2 = np.argwhere(pre_clusters==cl[1]).reshape(-1)\n",
    "    print (pre_cl_ind1)\n",
    "    pre_shell1, pre_outer1, pre_core1 = get_all_subgraphs(bigram_matrix.copy(), pre_cl_ind1)\n",
    "    pre_shell2, pre_outer2, pre_core2 = get_all_subgraphs(bigram_matrix.copy(), pre_cl_ind2)\n",
    "    total_edges = np.sum(bigram_matrix)\n",
    "    mod1 = calculate_weighted_edge_per_score(pre_core1, pre_outer1)\n",
    "    mod2 = calculate_weighted_edge_per_score(pre_core2, pre_outer2)\n",
    "    print (np.sum(pre_shell1))\n",
    "    post_cl_ind = np.argwhere(np_clusters==cl[0]).reshape(-1)\n",
    "    post_shell, post_outer, post_core = get_all_subgraphs(bigram_matrix.copy(), post_cl_ind)\n",
    "    post_mod = calculate_weighted_edge_per_score(post_core, post_outer)\n",
    "    \n",
    "    print (\"Cluster1: {}, Cluster2: {}, Pre mod1: {}, Pre Mod2: {}, Post Mod: {}\".format(cl[0], cl[1], mod1, mod2, post_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre_clusters = np.array(cluster_labels_results)\n",
    "# print (bigram_matrix.shape)\n",
    "# for cl in clusters_to_be_merged:\n",
    "#     np_clusters = np.array(cluster_labels_results)\n",
    "# #     adj_matrix_copy = bigram_matrix.copy()\n",
    "#     replace_index = np.where(np_clusters==cl[1])\n",
    "#     for i in replace_index:\n",
    "#         np_clusters[i] = cl[0]\n",
    "#     pre_cl_ind1 = np.argwhere(pre_clusters==cl[0]).reshape(-1)\n",
    "#     pre_cl_ind2 = np.argwhere(pre_clusters==cl[1]).reshape(-1)\n",
    "# #     print (pre_cl_ind1)\n",
    "#     pre_shell1, pre_outer1, pre_core1 = get_all_subgraphs(bigram_matrix.copy(), pre_cl_ind1)\n",
    "#     pre_shell2, pre_outer2, pre_core2 = get_all_subgraphs(bigram_matrix.copy(), pre_cl_ind2)\n",
    "#     total_edges = np.sum(bigram_matrix)\n",
    "#     mod1 = calculate_weighted_edge_per_score(pre_core1, pre_outer1)\n",
    "#     mod2 = calculate_weighted_edge_per_score(pre_core2, pre_outer2)\n",
    "# #     print (np.sum(pre_shell1))\n",
    "#     post_cl_ind = np.argwhere(np_clusters==cl[0]).reshape(-1)\n",
    "#     post_shell, post_outer, post_core = get_all_subgraphs(bigram_matrix.copy(), post_cl_ind)\n",
    "#     post_mod = calculate_weighted_edge_per_score(post_core, post_outer)\n",
    "    \n",
    "# #     print (\"Cluster1: {}, Cluster2: {}, Pre mod1: {}, Pre Mod2: {}, Post Mod: {}\".format(cl[0], cl[1], mod1, mod2, post_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_ratios = post_noisy_split_metrics['eigen_ratios']\n",
    "weighted_cluster_density = post_noisy_split_metrics['weighted_cluster_density']\n",
    "unweighted_cluster_density = post_noisy_split_metrics['unweighted_cluster_density']\n",
    "unweighted_fraudar_scores = post_noisy_split_metrics['unweighted_fraudar_scores']\n",
    "weighted_fraudar_scores = post_noisy_split_metrics['weighted_fraudar_scores']\n",
    "unweighted_outer_edge_perc_scores = post_noisy_split_metrics['unweighted_outer_edge_perc_scores']\n",
    "weighted_outer_edge_perc_scores = post_noisy_split_metrics['weighted_outer_edge_perc_scores']\n",
    "unweighted_shell_edge_perc_scores = post_noisy_split_metrics['unweighted_shell_edge_perc_scores']\n",
    "weighted_shell_edge_perc_scores = post_noisy_split_metrics['weighted_shell_edge_perc_scores']\n",
    "weighted_outer_modularity_scores = post_noisy_split_metrics['weighted_outer_modularity_scores']\n",
    "unweighted_outer_modularity_scores = post_noisy_split_metrics['unweighted_outer_modularity_scores']\n",
    "weighted_shell_modularity_scores = post_noisy_split_metrics['weighted_shell_modularity_scores']\n",
    "unweighted_shell_modularity_scores = post_noisy_split_metrics['unweighted_shell_modularity_scores']\n",
    "pairwise_similarity = post_noisy_split_metrics['pairwise_similarity']\n",
    "custom_score = post_noisy_split_metrics['custom_score']\n",
    "phone_counts = post_noisy_split_metrics['phone_counts']\n",
    "cluster_counts = post_noisy_split_metrics['cluster_counts']\n",
    "labels = df_nonoise['cluster_label']\n",
    "df_a = df_nonoise.copy()\n",
    "# labels = df_data['cluster_label']\n",
    "to_calculate_clusters = []\n",
    "for tup in clusters_to_be_merged:\n",
    "    if tup[1] in clusters:\n",
    "        df_nonoise['cluster_label'].replace(tup[1], tup[0], inplace=True)\n",
    "    # Handle case when 3 clusters are similar to each other eg. (a,b) (c,b)\n",
    "#         print (df_a['cluster_label'].unique())\n",
    "\n",
    "        ind = clusters.index(tup[1])\n",
    "        del eigen_ratios[ind]\n",
    "        del weighted_cluster_density[ind]\n",
    "        del unweighted_cluster_density[ind]\n",
    "        del unweighted_fraudar_scores[ind]\n",
    "        del weighted_fraudar_scores[ind]\n",
    "        del unweighted_outer_edge_perc_scores[ind]\n",
    "        del weighted_outer_edge_perc_scores[ind]\n",
    "        del unweighted_shell_edge_perc_scores[ind]\n",
    "        del weighted_shell_edge_perc_scores[ind]\n",
    "        del weighted_outer_modularity_scores[ind]\n",
    "        del unweighted_outer_modularity_scores[ind]\n",
    "        del weighted_shell_modularity_scores[ind]\n",
    "        del unweighted_shell_modularity_scores[ind]\n",
    "        del pairwise_similarity[ind]\n",
    "        del custom_score[ind]\n",
    "        del phone_counts[ind]\n",
    "        del cluster_counts[ind]\n",
    "        del clusters[ind]\n",
    "        if tup[1] in to_calculate_clusters:\n",
    "            to_calculate_clusters.remove(tup[1])\n",
    "        to_calculate_clusters.append(tup[0])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "metrics = get_all_metrics(bigram_matrix, to_calculate_clusters, df_nonoise['cluster_label'], df_nonoise)\n",
    "for i, cl in enumerate(to_calculate_clusters):\n",
    "    ind = clusters.index(cl)\n",
    "    eigen_ratios[ind] = metrics['eigen_ratios'][i]\n",
    "    weighted_cluster_density[ind] = post_noisy_split_metrics['weighted_cluster_density'][i]\n",
    "    unweighted_cluster_density[ind] = post_noisy_split_metrics['unweighted_cluster_density'][i]\n",
    "    unweighted_fraudar_scores[ind] = post_noisy_split_metrics['unweighted_fraudar_scores'][i]\n",
    "    weighted_fraudar_scores[ind] = post_noisy_split_metrics['weighted_fraudar_scores'][i]\n",
    "    unweighted_outer_edge_perc_scores[ind] = post_noisy_split_metrics['unweighted_outer_edge_perc_scores'][i]\n",
    "    weighted_outer_edge_perc_scores[ind] = post_noisy_split_metrics['weighted_outer_edge_perc_scores'][i]\n",
    "    unweighted_shell_edge_perc_scores[ind] = post_noisy_split_metrics['unweighted_shell_edge_perc_scores'][i]\n",
    "    weighted_shell_edge_perc_scores[ind] = post_noisy_split_metrics['weighted_shell_edge_perc_scores'][i]\n",
    "    weighted_outer_modularity_scores[ind] = post_noisy_split_metrics['weighted_outer_modularity_scores'][i]\n",
    "    unweighted_outer_modularity_scores[ind] = post_noisy_split_metrics['unweighted_outer_modularity_scores'][i]\n",
    "    weighted_shell_modularity_scores[ind] = post_noisy_split_metrics['weighted_shell_modularity_scores'][i]\n",
    "    unweighted_shell_modularity_scores[ind] = post_noisy_split_metrics['unweighted_shell_modularity_scores'][i]\n",
    "    pairwise_similarity[ind] = post_noisy_split_metrics['pairwise_similarity'][i]\n",
    "    custom_score[ind] = post_noisy_split_metrics['custom_score'][i]\n",
    "    phone_counts[ind] = post_noisy_split_metrics['phone_counts'][i]\n",
    "    cluster_counts[ind] = post_noisy_split_metrics['cluster_counts'][i]\n",
    "\n",
    "post_merging_metric = {}\n",
    "post_merging_metric['weighted_cluster_density'] = weighted_cluster_density\n",
    "post_merging_metric['unweighted_cluster_density'] = unweighted_cluster_density\n",
    "post_merging_metric['weighted_fraudar_scores'] = weighted_fraudar_scores\n",
    "post_merging_metric['unweighted_fraudar_scores'] = unweighted_fraudar_scores\n",
    "post_merging_metric['weighted_outer_edge_perc_scores'] = weighted_outer_edge_perc_scores\n",
    "post_merging_metric['unweighted_outer_edge_perc_scores'] = unweighted_outer_edge_perc_scores\n",
    "post_merging_metric['weighted_shell_edge_perc_scores'] = weighted_shell_edge_perc_scores\n",
    "post_merging_metric['unweighted_shell_edge_perc_scores'] = unweighted_shell_edge_perc_scores\n",
    "post_merging_metric['unweighted_outer_modularity_scores'] = unweighted_outer_modularity_scores\n",
    "post_merging_metric['weighted_outer_modularity_scores'] = weighted_outer_modularity_scores\n",
    "post_merging_metric['unweighted_shell_modularity_scores'] = unweighted_shell_modularity_scores\n",
    "post_merging_metric['weighted_shell_modularity_scores'] = weighted_shell_modularity_scores\n",
    "post_merging_metric['pairwise_similarity'] = pairwise_similarity\n",
    "post_merging_metric['custom_score'] = custom_score\n",
    "post_merging_metric['phone_counts'] = phone_counts\n",
    "post_merging_metric['eigen_ratios'] = eigen_ratios\n",
    "post_merging_metric['clusters'] = clusters\n",
    "post_merging_metric['cluster_label'] = df_nonoise['cluster_label'].copy()\n",
    "post_merging_metric['cluster_counts'] = cluster_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(post_merging_metric['clusters']), df_nonoise['cluster_label'].nunique(), len(post_merging_metric['eigen_ratios']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonoise.set_index('index1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_a = df_orig.copy()\n",
    "df_nonoise['final_label'] = df_nonoise['cluster_label']\n",
    "df_orig = df_orig.join(df_nonoise['final_label'], how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig['final_label'].isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_orig[df_orig['noise'] == True].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in df_orig.iterrows():\n",
    "    if row['noise'] == True:\n",
    "        df_orig.at[ind, 'final_label'] = -2\n",
    "    elif row['sim_check'] == True:\n",
    "        df_orig.at[ind, 'final_label'] = df_orig.at[row['sim_index'], 'final_label']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cl in clusters:\n",
    "#     df_filt = df_orig[df_orig['final_label'] == cl]\n",
    "#     names = [x.lower() if type(x) == type('') else None for x in df_filt['Name'].unique()]\n",
    "#     if math.nan in names:\n",
    "#         names.remove(math.nan)\n",
    "#     if None in names:\n",
    "#         names.remove(None)\n",
    "#     names = list(set(names))\n",
    "#     print (cl, len(names))\n",
    "# # print (post_merging_metric['cluster_counts'][post_merging_metric['custom_score'].index(max(post_merging_metric['custom_score']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_scores = post_merging_metric['custom_score']\n",
    "text_similarity = post_merging_metric['pairwise_similarity']\n",
    "clusters = post_merging_metric['clusters']\n",
    "phone_counts = post_merging_metric['phone_counts']\n",
    "w_density = post_merging_metric['weighted_cluster_density']\n",
    "uw_density = post_merging_metric['unweighted_cluster_density']\n",
    "w_fraudar = post_merging_metric['weighted_fraudar_scores']\n",
    "uw_fraudar = post_merging_metric['unweighted_fraudar_scores']\n",
    "w_outer_edge = post_merging_metric['weighted_outer_edge_perc_scores']\n",
    "uw_outer_edge = post_merging_metric['unweighted_outer_edge_perc_scores']\n",
    "w_shell_edge = post_merging_metric['weighted_shell_edge_perc_scores']\n",
    "uw_shell_edge = post_merging_metric['unweighted_shell_edge_perc_scores']\n",
    "w_outer_mod = post_merging_metric['weighted_outer_modularity_scores']\n",
    "uw_outer_mod = post_merging_metric['unweighted_outer_modularity_scores']\n",
    "w_shell_mod = post_merging_metric['weighted_shell_modularity_scores']\n",
    "uw_shell_mod = post_merging_metric['unweighted_shell_modularity_scores']\n",
    "cluster_counts = post_merging_metric['cluster_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=phone_counts, y=suspicious_scores, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Unique phone numbers per cluster', ylabel='anomaly score')\n",
    "plt.show()\n",
    "\n",
    "ax = sns.regplot(x=phone_counts, y=w_outer_edge, x_ci=68, truncate=False, lowess=True, scatter_kws = {'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Unique phone numbers per cluster', ylabel='Weighted outer edge percentage')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "ax = sns.regplot(x=text_similarity, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Text Similarity', ylabel='Weighted outer edge percentage')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.regplot(x=phone_counts, y=text_similarity, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "ax.set(xlabel='Unique phone numbers per cluster', ylabel='Text Similarity')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = sns.scatterplot(x=cluster_counts, y=phone_counts, ci=68, alpha=0.4)\n",
    "ax.set(xlabel='Number of ads', ylabel='Unique phone numbers per cluster')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# ax = sns.regplot(x=avg_label_scores, y=w_outer_edge, ci=68, truncate=False, lowess=True, scatter_kws = {'color': 'b', 'alpha': 0.2}, line_kws = {'color': 'red'})\n",
    "# ax.set(xlabel='Average Label Score', ylabel='Weighted outer edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.scatter(avg_label_scores, w_outer_edge, alpha=0.2)\n",
    "# plt.title('Avg label score vs weighted outer edge percentage')\n",
    "# plt.xlabel('avg label score')\n",
    "# plt.ylabel('weighted outer edge percentage')\n",
    "# # w = np.linalg.lstsq(avg_label_scores, w_outer_edge)[0]\n",
    "# # yh = np.dot(avg_label_scores,w)\n",
    "# # plt.plot(avg_label_scores, yh, 'r-')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(avg_label_scores, w_shell_edge, alpha=0.2)\n",
    "# plt.title('Avg label score vs weighted shell edge percentage')\n",
    "# plt.xlabel('avg label score')\n",
    "# plt.ylabel('weighted shell edge percentage')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(text_similarity, w_outer_edge, alpha=0.2)\n",
    "# plt.title('Avg text similarity vs weighted outer edge percentage')\n",
    "# plt.xlabel('avg text similarity')\n",
    "# plt.ylabel('weighted outer edge percentage')\n",
    "\n",
    "# # plt.scatter(avg_label_scores, w_shell_edge, alpha=0.2)\n",
    "# # plt.title('Avg label score vs weighted shell edge percentage')\n",
    "# # plt.xlabel('avg label score')\n",
    "# # plt.ylabel('weighted shell edge percentage')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [i for i, e in enumerate(cluster_counts) if e> 100]\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filt = df_orig[df_orig['cluster_label'] == 69]\n",
    "\n",
    "for ind, row in df_filt.iterrows():\n",
    "    print ('------------------------------------------------------------')\n",
    "    print (row['description'], row['phone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
